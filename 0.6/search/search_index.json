{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mirascope","text":"<p>Mirascope is an LLM toolkit for lightning-fast, high-quality development. Building with Mirascope feels like writing the Python code you\u2019re already used to writing.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mirascope\n</code></pre> <p>You can also install additional optional dependencies if you\u2019re using those features:</p> <pre><code>pip install mirascope[anthropic]  # AnthropicCall, ...\npip install mirascope[gemini]     # GeminiCall, ...\npip install mirascope[wandb]      # WandbOpenAICall, ...\n</code></pre>"},{"location":"#examples","title":"Examples","text":""},{"location":"#colocation","title":"Colocation","text":"<p>Colocation is the core of our philosophy. Everything that can impact the quality of a call to an LLM \u2014 from the prompt to the model to the temperature \u2014 must live together so that we can properly version and test the quality of our calls over time. This is useful since we have all of the information including metadata that we could want for analysis, which is particularly important during rapid development.</p> <pre><code>import os\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n@tags([\"version:0003\"])\nclass Editor(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are a top class manga editor.\n\n    USER:\n    I'm working on a new storyline. What do you think?\n    {storyline}\n    \"\"\"\n\n    storyline: str\n\n    call_params = OpenAICallParams(model=\"gpt-4\", temperature=0.4)\n\nstoryline = \"...\"\neditor = Editor(storyline=storyline)\n\nprint(editor.messages())\n# &gt; [{'role': 'system', 'content': 'You are a top class manga editor.'}, {'role': 'user', 'content': \"I'm working on a new storyline. What do you think?\\n...\"}]\n\ncritique = editor.call()\nprint(critique.content)\n# &gt; I think the beginning starts off great, but...\n\nprint(editor.dump() | critique.dump())\n# {\n#     \"tags\": [\"version:0003\"],\n#     \"template\": \"SYSTEM:\\nYou are a top class manga editor.\\n\\nUSER:\\nI'm working on a new storyline. What do you think?\\n{storyline}\",\n#     \"inputs\": {\"storyline\": \"...\"},\n#     \"start_time\": 1710452778501.079,\n#     \"end_time\": 1710452779736.8418,\n#     \"output\": {\n#         \"id\": \"chatcmpl-92nBykcXyTpxwAbTEM5BOKp99fVmv\",\n#         \"choices\": [\n#             {\n#                 \"finish_reason\": \"stop\",\n#                 \"index\": 0,\n#                 \"logprobs\": None,\n#                 \"message\": {\n#                     \"content\": \"I think the beginning starts off great, but...\",\n#                     \"role\": \"assistant\",\n#                     \"function_call\": None,\n#                     \"tool_calls\": None,\n#                 },\n#             }\n#         ],\n#         \"created\": 1710452778,\n#         \"model\": \"gpt-4-0613\",\n#         \"object\": \"chat.completion\",\n#         \"system_fingerprint\": None,\n#         \"usage\": {\"completion_tokens\": 25, \"prompt_tokens\": 33, \"total_tokens\": 58},\n#     },\n# }\n</code></pre>"},{"location":"#chat-history","title":"Chat History","text":"<p>Our template parser makes inserting chat history beyond easy:</p> <pre><code>from openai.types.chat import ChatCompletionMessageParam\n\nfrom mirascope.openai import OpenAICall\n\nclass Librarian(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    MESSAGES: {history}\n    USER: {question}\n    \"\"\"\n\n    question: str\n    history: list[ChatCompletionMessageParam] = []\n\nlibrarian = Librarian(question=\"\", history=[])\nwhile True:\n    librarian.question = input(\"(User): \")\n    response = librarian.call()\n    librarian.history.append({\"role\": \"user\", \"content\": librarian.question})\n    librarian.history.append({\"role\": \"assistant\", \"content\": response.content})\n    print(f\"(Assistant): {response.content}\")\n\n#&gt; (User): What fantasy book should I read?\n#&gt; (Assistant): Have you read the Name of the Wind?\n#&gt; (User): I have! What do you like about it?\n#&gt; (Assistant): I love the intricate world-building...\n</code></pre>"},{"location":"#tools-function-calling","title":"Tools (Function Calling)","text":"<p>We\u2019ve made implementing and using tools (function calling) intuitive:</p> <pre><code>from typing import Literal\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n):\n    \"\"\"Get the current weather in a given location.\"\"\"\n    if \"tokyo\" in location.lower():\n        print(f\"It is 10 degrees {unit} in Tokyo, Japan\")\n    elif \"san francisco\" in location.lower():\n        print(f\"It is 72 degrees {unit} in San Francisco, CA\")\n    elif \"paris\" in location.lower():\n        print(f\"It is 22 degress {unit} in Paris, France\")\n    else:\n        print(\"I'm not sure what the weather is like in {location}\")\n\nclass Forecast(OpenAICall):\n    prompt_template = \"What's the weather in Tokyo?\"\n\n    call_params = OpenAICallParams(model=\"gpt-4\", tools=[get_current_weather])\n\ntool = Forecast().call().tool\nif tool:\n    tool.fn(**tool.args)\n      #&gt; It is 10 degrees fahrenheit in Tokyo, Japan\n</code></pre>"},{"location":"#chaining","title":"Chaining","text":"<p>Chaining multiple calls together for Chain of Thought (CoT) is as simple as writing a function:</p> <pre><code>import os\nfrom functools import cached_property\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\nclass ChefSelector(OpenAICall):\n    prompt_template = \"Name a chef who is really good at cooking {food_type} food\"\n\n    food_type: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\nclass RecipeRecommender(ChefSelector):\n    prompt_template = \"\"\"\n    SYSTEM:\n    Imagine that you are chef {chef}.\n    Your task is to recommend recipes that you, {chef}, would be excited to serve.\n\n    USER:\n    Recommend a {food_type} recipe using {ingredient}.\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-4\")\n\n    @cached_property  # !!! so multiple access doesn't make multiple calls\n    def chef(self) -&gt; str:\n        \"\"\"Uses `ChefSelector` to select the chef based on the food type.\"\"\"\n        return ChefSelector(food_type=self.food_type).call().content\n\nresponse = RecipeRecommender(food_type=\"japanese\", ingredient=\"apples\").call()\nprint(response.content)\n# &gt; Certainly! Here's a recipe for a delicious and refreshing Japanese Apple Salad: ...\n</code></pre>"},{"location":"#extracting-structured-information","title":"Extracting Structured Information","text":"<p>Convenience built on top of tools that makes extracting structured information reliable:</p> <pre><code>from typing import Literal, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(TaskDetails)\n#&gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre>"},{"location":"#fastapi-integration","title":"FastAPI Integration","text":"<p>Since we\u2019ve built our <code>BasePrompt</code> on top of Pydantic, we integrate with tools like FastAPI out-of-the-box:</p> <pre><code>import os\nfrom typing import Type\n\nfrom fastapi import FastAPI\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\napp = FastAPI()\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a {genre} book.\"\n\n@app.post(\"/\")\ndef root(book_recommender: BookRecommender) -&gt; Book:\n    \"\"\"Generates a book based on provided `genre`.\"\"\"\n    return book_recommender.extract()\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Extracting structured information using LLMs</li> <li> Additional template parsing for more complex messages<ul> <li> Chat History</li> <li> Additional Metadata</li> <li> Vision</li> </ul> </li> <li> RAG</li> <li> Agents</li> <li> Support for more LLM providers:<ul> <li> Anthropic Function Calling</li> <li> Mistral</li> <li> HuggingFace</li> </ul> </li> <li> Integrations<ul> <li> Weights &amp; Biases</li> <li> LangChain / LangSmith</li> <li> \u2026 tell us what you\u2019d like integrated!</li> </ul> </li> <li> Evaluating prompts and their quality by version</li> </ul>"},{"location":"#versioning","title":"Versioning","text":"<p>Mirascope uses\u00a0Semantic Versioning.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the\u00a0MIT License.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use poetry as our package and dependency manager.</p> <p>To create a virtual environment for development, run the following in your shell:</p> <pre><code>pip install poetry\npoetry shell\npoetry install --with dev\n</code></pre> <p>Simply use <code>exit</code> to deactivate the environment. The next time you call <code>poetry shell</code> the environment will already be setup and ready to go.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout dev\ngit pull upstream dev\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run mypy .\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR. Make sure to fill in a detailed title and description. Submit your PR for review.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p> <p>Furthermore, we have a full coverage requirement, so all incoming code must have 100% coverage. This policy ensures that every line of code is run in our tests. However, while achieving full coverage is essential, it is not sufficient on its own. Coverage metrics ensure code execution but do not guarantee correctness under all conditions. Make sure to stress test beyond coverage to reduce bugs.</p> <p>We use a Codecov dashboard to monitor and track our coverage.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>mypy</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>mypy</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>mypy</code> errors.</p>"},{"location":"HELP/","title":"How to help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li> <p>Follow us on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow William Bakst on Twitter/X</p> <ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with William Bakst on LinkedIn</p> <ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how Mirascope has helped you and how you are using it.</p> </li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues / Discussions.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"api/","title":"mirascope api","text":"<p>mirascope package.</p>"},{"location":"api/enums/","title":"enums","text":"<p>Enum Classes for mirascope.</p>"},{"location":"api/enums/#mirascope.enums.MessageRole","title":"<code>MessageRole</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Roles that the <code>BasePrompt</code> messages parser can parse from the template.</p> <p>SYSTEM: A system message. USER: A user message. ASSISTANT: A message response from the assistant or chat client. MODEL: A message response from the assistant or chat client. Model is used by     Google's Gemini instead of assistant, which doesn't have system messages. TOOL: A message representing the output of calling a tool.</p> Source code in <code>mirascope/enums.py</code> <pre><code>class MessageRole(_Enum):\n    \"\"\"Roles that the `BasePrompt` messages parser can parse from the template.\n\n    SYSTEM: A system message.\n    USER: A user message.\n    ASSISTANT: A message response from the assistant or chat client.\n    MODEL: A message response from the assistant or chat client. Model is used by\n        Google's Gemini instead of assistant, which doesn't have system messages.\n    TOOL: A message representing the output of calling a tool.\n    \"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    MODEL = \"model\"\n    TOOL = \"tool\"\n</code></pre>"},{"location":"api/enums/#mirascope.enums.MirascopeCommand","title":"<code>MirascopeCommand</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>CLI commands to be executed.</p> <ul> <li>ADD: save a modified prompt to the <code>versions/</code> folder.</li> <li>USE: load a specific version of the prompt from <code>versions/</code> as the current prompt.</li> <li>STATUS: display if any changes have been made to prompts, and if a prompt is     specified, displays changes for only said prompt.</li> <li>INIT: initializes the necessary folders for prompt versioning with CLI.</li> </ul> Source code in <code>mirascope/enums.py</code> <pre><code>class MirascopeCommand(_Enum):\n    \"\"\"CLI commands to be executed.\n\n    - ADD: save a modified prompt to the `versions/` folder.\n    - USE: load a specific version of the prompt from `versions/` as the current prompt.\n    - STATUS: display if any changes have been made to prompts, and if a prompt is\n        specified, displays changes for only said prompt.\n    - INIT: initializes the necessary folders for prompt versioning with CLI.\n    \"\"\"\n\n    ADD = \"add\"\n    USE = \"use\"\n    STATUS = \"status\"\n    INIT = \"init\"\n</code></pre>"},{"location":"api/anthropic/","title":"anthropic","text":"<p>A module for interacting with Anthropic models.</p>"},{"location":"api/anthropic/calls/","title":"anthropic.calls","text":"<p>A module for calling Anthropic's Claude API.</p>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall","title":"<code>AnthropicCall</code>","text":"<p>             Bases: <code>BaseCall[AnthropicCallResponse, AnthropicCallResponseChunk, Any]</code></p> <p>A base class for calling Anthropic's Claude models.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; There are many great books to read, it ultimately depends...\n</code></pre> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>class AnthropicCall(BaseCall[AnthropicCallResponse, AnthropicCallResponseChunk, Any]):\n    \"\"\"A base class for calling Anthropic's Claude models.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend a {genre} book.\"\n\n        genre: str\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; There are many great books to read, it ultimately depends...\n    ```\n    \"\"\"\n\n    call_params: ClassVar[AnthropicCallParams] = AnthropicCallParams()\n\n    def messages(self) -&gt; list[MessageParam]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        return self._parse_messages(\n            [MessageRole.SYSTEM, MessageRole.USER, MessageRole.ASSISTANT]\n        )  # type: ignore\n\n    def call(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n        \"\"\"Makes a call to the model using this `AnthropicCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `AnthropicCallResponse` instance.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        start_time = datetime.datetime.now().timestamp() * 1000\n        message = client.messages.create(\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n        return AnthropicCallResponse(\n            response=message,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `AnthropicCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `AnthropicCallResponse` instance.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        start_time = datetime.datetime.now().timestamp() * 1000\n        message = await client.messages.create(\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n        return AnthropicCallResponse(\n            response=message,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    def stream(\n        self, **kwargs: Any\n    ) -&gt; Generator[AnthropicCallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `AnthropicCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            An `AnthropicCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        with client.messages.stream(messages=messages, **kwargs) as stream:\n            for chunk in stream:\n                yield AnthropicCallResponseChunk(chunk=chunk, tool_types=tool_types)\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[AnthropicCallResponseChunk, None]:\n        \"\"\"Streams the response for an asynchronous call using this `AnthropicCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            An `AnthropicCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        async with client.messages.stream(messages=messages, **kwargs) as stream:\n            async for chunk in stream:\n                yield AnthropicCallResponseChunk(chunk=chunk, tool_types=tool_types)\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _setup_anthropic_kwargs(\n        self,\n        kwargs: dict[str, Any],\n    ) -&gt; tuple[\n        list[MessageParam],\n        dict[str, Any],\n        Optional[list[Type[AnthropicTool]]],\n    ]:\n        \"\"\"Overrides the `BaseCall._setup` for Anthropic specific setup.\"\"\"\n        kwargs, tool_types = self._setup(kwargs, AnthropicTool)\n        messages = self.messages()\n        system_message = \"\"\n        if messages[0][\"role\"] == \"system\":\n            system_message += messages.pop(0)[\"content\"]\n        if tool_types:\n            tool_schemas = kwargs.pop(\"tools\")\n            system_message += self._write_tools_system_message(tool_schemas)\n            kwargs[\"stop_sequences\"] = [\"&lt;/function_calls&gt;\"]\n        if system_message:\n            kwargs[\"system\"] = system_message\n        return messages, kwargs, tool_types\n\n    def _write_tools_system_message(self, tool_schemas: list[str]) -&gt; str:\n        \"\"\"Returns the Anthropic Tools System Message from their guide.\"\"\"\n        return dedent(\n            \"\"\"\n        In this environment you have access to a set of tools you can use to answer the user's question.\n\n        You may call them like this:\n        &lt;function_calls&gt;\n        &lt;invoke&gt;\n        &lt;tool_name&gt;$TOOL_NAME&lt;/tool_name&gt;\n        &lt;parameters&gt;\n        &lt;$PARAMETER_NAME&gt;$PARAMETER_VALUE&lt;/$PARAMETER_NAME&gt;\n        ...\n        &lt;/parameters&gt;\n        &lt;/invoke&gt;\n        ...\n        &lt;/function_calls&gt;\n\n        Make sure to include all parameters in the tool schema when requested.\n        If you want to call multiple tools, you should put all of the tools inside of the &lt;function_calls&gt; tag as multiple &lt;invoke&gt; elements.\n\n        To output nested structured data, encode it as valid XML with tags and values. For example:\n\n        List:\n        &lt;listTypeParameter&gt;\n            &lt;item&gt;1&lt;/item&gt;\n            &lt;item&gt;2&lt;/item&gt;\n            &lt;item&gt;3&lt;/item&gt;\n        &lt;/listTypeParameter&gt;\n\n        Dictionary:\n        &lt;dictTypeParameter&gt;\n            &lt;entry&gt;\n                &lt;key&gt;key1&lt;/key&gt;\n                &lt;value&gt;value1&lt;/value&gt;\n            &lt;/entry&gt;\n            &lt;entry&gt;\n                &lt;key&gt;key2&lt;/key&gt;\n                &lt;value&gt;value2&lt;/value&gt;\n            &lt;/entry&gt;\n        &lt;/dictTypeParameter&gt;\n\n        Here are the tools available:\n        &lt;tools&gt;\n        {tool_schemas}\n        &lt;/tools&gt;\n            \"\"\".format(tool_schemas=tool_schemas)\n        )\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>AnthropicCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnthropicCallResponse</code> <p>A <code>AnthropicCallResponse</code> instance.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n    \"\"\"Makes a call to the model using this `AnthropicCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `AnthropicCallResponse` instance.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    start_time = datetime.datetime.now().timestamp() * 1000\n    message = client.messages.create(\n        messages=messages,\n        stream=False,\n        **kwargs,\n    )\n    return AnthropicCallResponse(\n        response=message,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>AnthropicCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnthropicCallResponse</code> <p>A <code>AnthropicCallResponse</code> instance.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `AnthropicCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `AnthropicCallResponse` instance.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    start_time = datetime.datetime.now().timestamp() * 1000\n    message = await client.messages.create(\n        messages=messages,\n        stream=False,\n        **kwargs,\n    )\n    return AnthropicCallResponse(\n        response=message,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>def messages(self) -&gt; list[MessageParam]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    return self._parse_messages(\n        [MessageRole.SYSTEM, MessageRole.USER, MessageRole.ASSISTANT]\n    )  # type: ignore\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>AnthropicCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AnthropicCallResponseChunk</code> <p>An <code>AnthropicCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>def stream(\n    self, **kwargs: Any\n) -&gt; Generator[AnthropicCallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `AnthropicCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        An `AnthropicCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    with client.messages.stream(messages=messages, **kwargs) as stream:\n        for chunk in stream:\n            yield AnthropicCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response for an asynchronous call using this <code>AnthropicCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[AnthropicCallResponseChunk, None]</code> <p>An <code>AnthropicCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[AnthropicCallResponseChunk, None]:\n    \"\"\"Streams the response for an asynchronous call using this `AnthropicCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        An `AnthropicCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    async with client.messages.stream(messages=messages, **kwargs) as stream:\n        async for chunk in stream:\n            yield AnthropicCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/anthropic/tools/","title":"anthropic.tools","text":"<p>Classes for using tools with Anthropic's Claude API.</p>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool","title":"<code>AnthropicTool</code>","text":"<p>             Bases: <code>BaseTool[Element]</code></p> <p>A base class for easy use of tools with the Anthropic Claude client.</p> <p><code>AnthropicTool</code> internally handles the logic that allows you to use tools with simple calls such as <code>AnthropicCallResponse.tool</code> or <code>AnthropicTool.fn</code>, as seen in the example below.</p> <p>Example:</p> <pre><code>from mirascope import AnthropicCall, AnthropicCallParams\n\n\ndef animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n    \"\"\"Tells you your most likely favorite animal from personality traits.\n\n    Args:\n        fav_food: your favorite food.\n        fav_color: your favorite color.\n\n    Returns:\n        The animal most likely to be your favorite based on traits.\n    \"\"\"\n    return \"Your favorite animal is the best one, a frog.\"\n\n\nclass AnimalMatcher(AnthropicCall):\n    prompt_template = \"\"\"\n    Tell me my favorite animal if my favorite food is {food} and my\n    favorite color is {color}.\n    \"\"\"\n\n    food: str\n    color: str\n\n    call_params = AnthropicCallParams(tools=[animal_matcher])\n\n\nresponse = AnimalMatcher(food=\"pizza\", color=\"red\").call\ntool = response.tool\nprint(tool.fn(**tool.args))\n#&gt; Your favorite animal is the best one, a frog.\n</code></pre> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>class AnthropicTool(BaseTool[ET.Element]):\n    '''A base class for easy use of tools with the Anthropic Claude client.\n\n    `AnthropicTool` internally handles the logic that allows you to use tools with\n    simple calls such as `AnthropicCallResponse.tool` or `AnthropicTool.fn`, as seen in\n    the example below.\n\n    Example:\n\n    ```python\n    from mirascope import AnthropicCall, AnthropicCallParams\n\n\n    def animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n        \"\"\"Tells you your most likely favorite animal from personality traits.\n\n        Args:\n            fav_food: your favorite food.\n            fav_color: your favorite color.\n\n        Returns:\n            The animal most likely to be your favorite based on traits.\n        \"\"\"\n        return \"Your favorite animal is the best one, a frog.\"\n\n\n    class AnimalMatcher(AnthropicCall):\n        prompt_template = \"\"\"\n        Tell me my favorite animal if my favorite food is {food} and my\n        favorite color is {color}.\n        \"\"\"\n\n        food: str\n        color: str\n\n        call_params = AnthropicCallParams(tools=[animal_matcher])\n\n\n    response = AnimalMatcher(food=\"pizza\", color=\"red\").call\n    tool = response.tool\n    print(tool.fn(**tool.args))\n    #&gt; Your favorite animal is the best one, a frog.\n    ```\n    '''\n\n    @classmethod\n    def tool_schema(cls) -&gt; str:\n        \"\"\"Constructs XML tool schema string for use with Anthropic's Claude API.\"\"\"\n        json_schema = super().tool_schema()\n        tool_schema = (\n            dedent(\n                \"\"\"\n                &lt;tool_description&gt;\n                &lt;tool_name&gt;{name}&lt;/tool_name&gt;\n                &lt;description&gt;\n                {description}\n                &lt;/description&gt;\n                \"\"\"\n            )\n            .strip()\n            .format(name=cls.__name__, description=json_schema[\"description\"])\n        )\n\n        tool_schema += \"\\n\"\n        if \"parameters\" in json_schema:\n            tool_schema += _process_schema(json_schema[\"parameters\"])\n        tool_schema += \"&lt;/tool_description&gt;\"\n        return tool_schema\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ET.Element) -&gt; AnthropicTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given the `&lt;invoke&gt;...&lt;/invoke&gt;` block in a `Message` from an Anthropic call\n        response, this method parses out the XML defining the tool call and creates an\n        `AnthropicTool` instance from it.\n\n        Args:\n            tool_call: The XML `str` from which to extract the tool.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n\n        def _parse_xml_element(element: ET.Element) -&gt; Union[str, dict, list]:\n            \"\"\"Recursively parse an XML element into a Python data structure.\"\"\"\n            children = list(element)\n            if not children:\n                if element.text:\n                    text = element.text.strip().replace(\"\\\\n\", \"\").replace(\"\\\\\", \"\")\n                    # Attempt to load JSON-like strings as Python data structures\n                    try:\n                        return loads(text)\n                    except JSONDecodeError:\n                        return text\n                else:\n                    return \"\"\n\n            if len(children) &gt; 1 and all(\n                child.tag == children[0].tag for child in children\n            ):\n                if children[0].tag == \"entry\":\n                    dict_result = {}\n                    for child in children:\n                        key, value = child.find(\"key\"), child.find(\"value\")\n                        if key is not None and value is not None:\n                            dict_result[key.text] = _parse_xml_element(value)\n                    return dict_result\n                else:\n                    return [_parse_xml_element(child) for child in children]\n\n            result: dict[str, Any] = {}\n            for child in children:\n                child_value = _parse_xml_element(child)\n                result[child.tag] = child_value\n            return result\n\n        parameters = cast(dict[str, Any], _parse_xml_element(tool_call))\n        parameters[\"tool_call\"] = tool_call\n        return cls.model_validate(parameters)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[AnthropicTool]:\n        \"\"\"Constructs a `AnthropicTool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, AnthropicTool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[AnthropicTool]:\n        \"\"\"Constructs a `AnthropicTool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, AnthropicTool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[AnthropicTool]:\n        \"\"\"Constructs a `AnthropicTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>AnthropicTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[AnthropicTool]:\n    \"\"\"Constructs a `AnthropicTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>AnthropicTool</code> type from a function.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[AnthropicTool]:\n    \"\"\"Constructs a `AnthropicTool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>AnthropicTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[AnthropicTool]:\n    \"\"\"Constructs a `AnthropicTool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given the <code>&lt;invoke&gt;...&lt;/invoke&gt;</code> block in a <code>Message</code> from an Anthropic call response, this method parses out the XML defining the tool call and creates an <code>AnthropicTool</code> instance from it.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>Element</code> <p>The XML <code>str</code> from which to extract the tool.</p> required <p>Returns:</p> Type Description <code>AnthropicTool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ET.Element) -&gt; AnthropicTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given the `&lt;invoke&gt;...&lt;/invoke&gt;` block in a `Message` from an Anthropic call\n    response, this method parses out the XML defining the tool call and creates an\n    `AnthropicTool` instance from it.\n\n    Args:\n        tool_call: The XML `str` from which to extract the tool.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n\n    def _parse_xml_element(element: ET.Element) -&gt; Union[str, dict, list]:\n        \"\"\"Recursively parse an XML element into a Python data structure.\"\"\"\n        children = list(element)\n        if not children:\n            if element.text:\n                text = element.text.strip().replace(\"\\\\n\", \"\").replace(\"\\\\\", \"\")\n                # Attempt to load JSON-like strings as Python data structures\n                try:\n                    return loads(text)\n                except JSONDecodeError:\n                    return text\n            else:\n                return \"\"\n\n        if len(children) &gt; 1 and all(\n            child.tag == children[0].tag for child in children\n        ):\n            if children[0].tag == \"entry\":\n                dict_result = {}\n                for child in children:\n                    key, value = child.find(\"key\"), child.find(\"value\")\n                    if key is not None and value is not None:\n                        dict_result[key.text] = _parse_xml_element(value)\n                return dict_result\n            else:\n                return [_parse_xml_element(child) for child in children]\n\n        result: dict[str, Any] = {}\n        for child in children:\n            child_value = _parse_xml_element(child)\n            result[child.tag] = child_value\n        return result\n\n    parameters = cast(dict[str, Any], _parse_xml_element(tool_call))\n    parameters[\"tool_call\"] = tool_call\n    return cls.model_validate(parameters)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs XML tool schema string for use with Anthropic's Claude API.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; str:\n    \"\"\"Constructs XML tool schema string for use with Anthropic's Claude API.\"\"\"\n    json_schema = super().tool_schema()\n    tool_schema = (\n        dedent(\n            \"\"\"\n            &lt;tool_description&gt;\n            &lt;tool_name&gt;{name}&lt;/tool_name&gt;\n            &lt;description&gt;\n            {description}\n            &lt;/description&gt;\n            \"\"\"\n        )\n        .strip()\n        .format(name=cls.__name__, description=json_schema[\"description\"])\n    )\n\n    tool_schema += \"\\n\"\n    if \"parameters\" in json_schema:\n        tool_schema += _process_schema(json_schema[\"parameters\"])\n    tool_schema += \"&lt;/tool_description&gt;\"\n    return tool_schema\n</code></pre>"},{"location":"api/anthropic/types/","title":"anthropic.types","text":"<p>Type classes for interacting with Anthropics's Claude API.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallParams","title":"<code>AnthropicCallParams</code>","text":"<p>             Bases: <code>BaseCallParams[AnthropicTool]</code></p> <p>The parameters to use when calling d Claud API with a prompt.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall, AnthropicCallParams\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend some books.\"\n\n    call_params = AnthropicCallParams(\n        model=\"anthropic-3-opus-20240229\",\n    )\n</code></pre> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>class AnthropicCallParams(BaseCallParams[AnthropicTool]):\n    \"\"\"The parameters to use when calling d Claud API with a prompt.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall, AnthropicCallParams\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend some books.\"\n\n        call_params = AnthropicCallParams(\n            model=\"anthropic-3-opus-20240229\",\n        )\n    ```\n    \"\"\"\n\n    max_tokens: int = 1000\n    model: str = \"claude-3-sonnet-20240229\"\n    metadata: Optional[Metadata] = None\n    stop_sequences: Optional[list[str]] = None\n    system: Optional[str] = None\n    temperature: Optional[float] = None\n    top_k: Optional[int] = None\n    top_p: Optional[float] = None\n    extra_headers: Optional[Headers] = None\n    extra_query: Optional[Query] = None\n    extra_body: Optional[Body] = None\n    timeout: Optional[Union[float, Timeout]] = 600\n\n    wrapper: Optional[Callable[[Anthropic], Anthropic]] = None\n    wrapper_async: Optional[Callable[[AsyncAnthropic], AsyncAnthropic]] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def kwargs(\n        self,\n        tool_type: Optional[Type[AnthropicTool]] = None,\n        exclude: Optional[set[str]] = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the keyword argument call parameters.\"\"\"\n        extra_exclude = {\"wrapper\", \"wrapper_async\"}\n        exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n        return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallParams.kwargs","title":"<code>kwargs(tool_type=None, exclude=None)</code>","text":"<p>Returns the keyword argument call parameters.</p> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>def kwargs(\n    self,\n    tool_type: Optional[Type[AnthropicTool]] = None,\n    exclude: Optional[set[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the keyword argument call parameters.\"\"\"\n    extra_exclude = {\"wrapper\", \"wrapper_async\"}\n    exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n    return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse","title":"<code>AnthropicCallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[Message, AnthropicTool]</code></p> <p>Convenience wrapper around the Anthropic Claude API.</p> <p>When using Mirascope's convenience wrappers to interact with Anthropic models via <code>AnthropicCall</code>, responses using <code>Anthropic.call()</code> will return an <code>AnthropicCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend some books.\"\n\n\nprint(BookRecommender().call())\n</code></pre> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>class AnthropicCallResponse(BaseCallResponse[Message, AnthropicTool]):\n    \"\"\"Convenience wrapper around the Anthropic Claude API.\n\n    When using Mirascope's convenience wrappers to interact with Anthropic models via\n    `AnthropicCall`, responses using `Anthropic.call()` will return an\n    `AnthropicCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend some books.\"\n\n\n    print(BookRecommender().call())\n    ```\n    \"\"\"\n\n    @property\n    def tools(self) -&gt; Optional[list[AnthropicTool]]:\n        \"\"\"Returns the tools for the 0th choice message.\"\"\"\n        if (\n            not self.tool_types\n            or self.response.stop_reason != \"stop_sequence\"\n            or self.response.stop_sequence != \"&lt;/function_calls&gt;\"\n        ):\n            return None\n\n        try:\n            root_node = ET.fromstring(\n                f\"&lt;wrapper&gt;{self.response.content}&lt;/function_calls&gt;&lt;/wrapper&gt;\"\n            )\n        except ET.ParseError as e:\n            raise ValueError(\"Unable to parse tools from response\") from e\n\n        # There must be a &lt;function_calls&gt; tag since we successfully parsed the\n        # XML with the manually added &lt;/function_calls&gt; tag.\n        tool_calls_node = cast(ET.Element, root_node.find(\"function_calls\"))\n\n        extracted_tools = []\n        for tool_call_node in tool_calls_node:\n            for tool_type in self.tool_types:\n                tool_name_node = tool_call_node.find(\"tool_name\")\n                if (\n                    tool_name_node is not None\n                    and tool_name_node.text == tool_type.__name__\n                    and (parameters := tool_call_node.find(\"parameters\"))\n                ):\n                    tool = tool_type.from_tool_call(parameters)\n                    extracted_tools.append(tool)\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[AnthropicTool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string content of the 0th message.\"\"\"\n        return self.response.content[0].text\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": self.response.model_dump(),\n        }\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string content of the 0th message.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.tool","title":"<code>tool: Optional[AnthropicTool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.tools","title":"<code>tools: Optional[list[AnthropicTool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": self.response.model_dump(),\n    }\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponseChunk","title":"<code>AnthropicCallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[MessageStreamEvent, AnthropicTool]</code></p> <p>Convenience wrapper around the Anthropic API streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Anthropic models via <code>AnthropicCall</code>, responses using <code>AnthropicCall.stream()</code> will yield <code>AnthropicCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend some books.\"\n\n\nfor chunk in BookRecommender().stream():\n    print(chunk, end=\"\")\n</code></pre> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>class AnthropicCallResponseChunk(\n    BaseCallResponseChunk[MessageStreamEvent, AnthropicTool]\n):\n    \"\"\"Convenience wrapper around the Anthropic API streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Anthropic models via\n    `AnthropicCall`, responses using `AnthropicCall.stream()` will yield\n    `AnthropicCallResponseChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend some books.\"\n\n\n    for chunk in BookRecommender().stream():\n        print(chunk, end=\"\")\n    ```\n    \"\"\"\n\n    @property\n    def type(\n        self,\n    ) -&gt; Literal[\n        \"message_start\",\n        \"message_delta\",\n        \"message_stop\",\n        \"content_block_start\",\n        \"content_block_delta\",\n        \"content_block_stop\",\n    ]:\n        \"\"\"Returns the type of the chunk.\"\"\"\n        return self.chunk.type\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string content of the 0th message.\"\"\"\n        if isinstance(self.chunk, ContentBlockStartEvent):\n            return self.chunk.content_block.text\n        if isinstance(self.chunk, ContentBlockDeltaEvent):\n            return self.chunk.delta.text\n        return \"\"\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string content of the 0th message.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponseChunk.type","title":"<code>type: Literal['message_start', 'message_delta', 'message_stop', 'content_block_start', 'content_block_delta', 'content_block_stop']</code>  <code>property</code>","text":"<p>Returns the type of the chunk.</p>"},{"location":"api/base/","title":"base","text":"<p>Base modules for the Mirascope library.</p>"},{"location":"api/base/calls/","title":"base.calls","text":"<p>A base abstract interface for calling LLMs.</p>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall","title":"<code>BaseCall</code>","text":"<p>             Bases: <code>BasePrompt</code>, <code>Generic[BaseCallResponseT, BaseCallResponseChunkT, BaseToolT]</code>, <code>ABC</code></p> <p>The base class abstract interface for calling LLMs.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>class BaseCall(\n    BasePrompt, Generic[BaseCallResponseT, BaseCallResponseChunkT, BaseToolT], ABC\n):\n    \"\"\"The base class abstract interface for calling LLMs.\"\"\"\n\n    api_key: ClassVar[Optional[str]] = None\n    base_url: ClassVar[Optional[str]] = None\n    call_params: ClassVar[BaseCallParams] = BaseCallParams[BaseToolT](\n        model=\"gpt-3.5-turbo-0125\"\n    )\n\n    @abstractmethod\n    def call(self, **kwargs: Any) -&gt; BaseCallResponseT:\n        \"\"\"A call to an LLM.\n\n        An implementation of this function must return a response that extends\n        `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n        different model providers.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def call_async(self, **kwargs: Any) -&gt; BaseCallResponseT:\n        \"\"\"An asynchronous call to an LLM.\n\n        An implementation of this function must return a response that extends\n        `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n        different model providers.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def stream(self, **kwargs: Any) -&gt; Generator[BaseCallResponseChunkT, None, None]:\n        \"\"\"A call to an LLM that streams the response in chunks.\n\n        An implementation of this function must yield response chunks that extend\n        `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n        e.g. different model providers.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[BaseCallResponseChunkT, None]:\n        \"\"\"A asynchronous call to an LLM that streams the response in chunks.\n\n        An implementation of this function must yield response chunks that extend\n        `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n        e.g. different model providers.\"\"\"\n        yield ...  # type: ignore # pragma: no cover\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _setup(\n        self,\n        kwargs: dict[str, Any],\n        base_tool_type: Optional[Type[BaseToolT]] = None,\n    ) -&gt; tuple[dict[str, Any], Optional[list[Type[BaseToolT]]]]:\n        \"\"\"Returns the call params kwargs and tool types.\n\n        The tools in the call params first get converted into BaseToolT types. We then\n        need both the converted tools for the response (so it can construct actual tool\n        instances if present in the response) as well as the actual schemas injected\n        through kwargs. This function handles that setup.\n        \"\"\"\n        call_params = self.call_params.model_copy(update=kwargs)\n        kwargs = call_params.kwargs(tool_type=base_tool_type)\n        tool_types = None\n        if \"tools\" in kwargs and base_tool_type is not None:\n            tool_types = kwargs.pop(\"tools\")\n            kwargs[\"tools\"] = [tool_type.tool_schema() for tool_type in tool_types]\n        return kwargs, tool_types\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.call","title":"<code>call(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>A call to an LLM.</p> <p>An implementation of this function must return a response that extends <code>BaseCallResponse</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\ndef call(self, **kwargs: Any) -&gt; BaseCallResponseT:\n    \"\"\"A call to an LLM.\n\n    An implementation of this function must return a response that extends\n    `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n    different model providers.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>An asynchronous call to an LLM.</p> <p>An implementation of this function must return a response that extends <code>BaseCallResponse</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\nasync def call_async(self, **kwargs: Any) -&gt; BaseCallResponseT:\n    \"\"\"An asynchronous call to an LLM.\n\n    An implementation of this function must return a response that extends\n    `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n    different model providers.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.stream","title":"<code>stream(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>A call to an LLM that streams the response in chunks.</p> <p>An implementation of this function must yield response chunks that extend <code>BaseCallResponseChunk</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\ndef stream(self, **kwargs: Any) -&gt; Generator[BaseCallResponseChunkT, None, None]:\n    \"\"\"A call to an LLM that streams the response in chunks.\n\n    An implementation of this function must yield response chunks that extend\n    `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n    e.g. different model providers.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>A asynchronous call to an LLM that streams the response in chunks.</p> <p>An implementation of this function must yield response chunks that extend <code>BaseCallResponseChunk</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\nasync def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[BaseCallResponseChunkT, None]:\n    \"\"\"A asynchronous call to an LLM that streams the response in chunks.\n\n    An implementation of this function must yield response chunks that extend\n    `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n    e.g. different model providers.\"\"\"\n    yield ...  # type: ignore # pragma: no cover\n</code></pre>"},{"location":"api/base/extractors/","title":"base.extractors","text":"<p>A base abstract interface for extracting structured information using LLMs.</p>"},{"location":"api/base/extractors/#mirascope.base.extractors.BaseExtractor","title":"<code>BaseExtractor</code>","text":"<p>             Bases: <code>BasePrompt</code>, <code>Generic[BaseCallT, BaseToolT, ExtractedTypeT]</code>, <code>ABC</code></p> <p>The base abstract interface for extracting structured information using LLMs.</p> Source code in <code>mirascope/base/extractors.py</code> <pre><code>class BaseExtractor(BasePrompt, Generic[BaseCallT, BaseToolT, ExtractedTypeT], ABC):\n    \"\"\"The base abstract interface for extracting structured information using LLMs.\"\"\"\n\n    extract_schema: ExtractionType\n\n    api_key: ClassVar[Optional[str]] = None\n    base_url: ClassVar[Optional[str]] = None\n    call_params: ClassVar[BaseCallParams] = BaseCallParams[BaseToolT](\n        model=\"gpt-3.5-turbo-0125\"\n    )\n\n    @abstractmethod\n    def extract(self, retries: int = 0) -&gt; ExtractedTypeT:\n        \"\"\"Extracts the `extraction_schema` from an LLM call.\"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def extract_async(self, retries: int = 0) -&gt; ExtractedTypeT:\n        \"\"\"Asynchronously extracts the `extraction_schema` from an LLM call.\"\"\"\n        ...  # pragma: no cover\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _extract(\n        self,\n        call_type: Type[BaseCallT],\n        tool_type: Type[BaseToolT],\n        retries: int,\n        **kwargs: Any,\n    ) -&gt; ExtractedTypeT:\n        \"\"\"Extracts `extract_schema` from the call response.\n\n        The `extract_schema` is converted into a tool, complete with a description of\n        the tool, all of the fields, and their types. This allows us to take advantage\n        of tools/function calling functionality to extract information from a prompt\n        according to the context provided by the `BaseModel` schema.\n\n        Args:\n            tool_type: The type of tool to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            retries: The number of call attempts to make on `ValidationError` before\n                giving up and throwing the error to the user.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            An instance of the `extract_schema` with it's fields populated.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs, return_tool = self._setup(tool_type, kwargs)\n\n        class TempCall(call_type):  # type: ignore\n            prompt_template = self.prompt_template\n\n            call_params = call_type.call_params\n\n            model_config = ConfigDict(extra=\"allow\")\n\n        response = TempCall(**self.model_dump(exclude={\"extract_schema\"})).call(\n            **kwargs\n        )\n        try:\n            tool = response.tool\n            if tool is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            if return_tool:\n                return tool\n            if _is_base_type(self.extract_schema):\n                return tool.value\n            model = self.extract_schema(**response.tool.model_dump())\n            model._response = response  # type: ignore\n            return model  # type: ignore\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return self._extract(call_type, tool_type, retries - 1, **kwargs)\n            raise  # re-raise if we have no retries left\n\n    async def _extract_async(\n        self,\n        call_type: Type[BaseCallT],\n        tool_type: Type[BaseToolT],\n        retries: int,\n        **kwargs: Any,\n    ) -&gt; ExtractedTypeT:\n        \"\"\"Extracts `extract_schema` from the asynchronous call response.\n\n        The `extract_schema` is converted into a tool, complete with a description of\n        the tool, all of the fields, and their types. This allows us to take advantage\n        of tools/function calling functionality to extract information from a prompt\n        according to the context provided by the `BaseModel` schema.\n\n        Args:\n            tool_type: The type of tool to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            retries: The number of call attempts to make on `ValidationError` before\n                giving up and throwing the error to the user.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            An instance of the `extract_schema` with it's fields populated.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs, return_tool = self._setup(tool_type, kwargs)\n\n        class TempCall(call_type):  # type: ignore\n            prompt_template = self.prompt_template\n\n            call_params = call_type.call_params\n\n            model_config = ConfigDict(extra=\"allow\")\n\n        response = await TempCall(\n            **self.model_dump(exclude={\"extract_schema\"})\n        ).call_async(**kwargs)\n        try:\n            tool = response.tool\n            if tool is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            if return_tool:\n                return tool\n            if _is_base_type(self.extract_schema):\n                return tool.value\n            model = self.extract_schema(**response.tool.model_dump())\n            model._response = response  # type: ignore\n            return model  # type: ignore\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return await self._extract_async(\n                    call_type, tool_type, retries - 1, **kwargs\n                )\n            raise  # re-raise if we have no retries left\n\n    def _setup(\n        self, tool_type: Type[BaseToolT], kwargs: dict[str, Any]\n    ) -&gt; tuple[dict[str, Any], bool]:\n        \"\"\"Returns the call params kwargs and whether to return the tool directly.\"\"\"\n        call_params = self.call_params.model_copy(update=kwargs)\n        kwargs = call_params.kwargs(tool_type=tool_type)\n        if _is_base_type(self.extract_schema):\n            tool = tool_type.from_base_type(self.extract_schema)  # type: ignore\n            return_tool = False\n        elif not isclass(self.extract_schema):\n            tool = tool_type.from_fn(self.extract_schema)\n            return_tool = True\n        elif not issubclass(self.extract_schema, tool_type):\n            tool = tool_type.from_model(self.extract_schema)\n            return_tool = False\n        else:\n            tool = self.extract_schema\n            return_tool = True\n        kwargs[\"tools\"] = [tool]\n        return kwargs, return_tool\n</code></pre>"},{"location":"api/base/extractors/#mirascope.base.extractors.BaseExtractor.extract","title":"<code>extract(retries=0)</code>  <code>abstractmethod</code>","text":"<p>Extracts the <code>extraction_schema</code> from an LLM call.</p> Source code in <code>mirascope/base/extractors.py</code> <pre><code>@abstractmethod\ndef extract(self, retries: int = 0) -&gt; ExtractedTypeT:\n    \"\"\"Extracts the `extraction_schema` from an LLM call.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/extractors/#mirascope.base.extractors.BaseExtractor.extract_async","title":"<code>extract_async(retries=0)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Asynchronously extracts the <code>extraction_schema</code> from an LLM call.</p> Source code in <code>mirascope/base/extractors.py</code> <pre><code>@abstractmethod\nasync def extract_async(self, retries: int = 0) -&gt; ExtractedTypeT:\n    \"\"\"Asynchronously extracts the `extraction_schema` from an LLM call.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/prompts/","title":"base.prompts","text":"<p>A base class for writing prompts.</p>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The base class for working with prompts.</p> <p>This class is implemented as the base for all prompting needs across various model providers.</p> <p>Example:</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"A prompt for recommending a book.\"\"\"\n\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    USER: Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nprint(prompt.messages())\n#&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\nprint(prompt)\n#&gt; Please recommend a fantasy book.\n</code></pre> Source code in <code>mirascope/base/prompts.py</code> <pre><code>class BasePrompt(BaseModel):\n    '''The base class for working with prompts.\n\n    This class is implemented as the base for all prompting needs across various model\n    providers.\n\n    Example:\n\n    ```python\n    from mirascope import BasePrompt\n\n\n    class BookRecommendationPrompt(BasePrompt):\n        \"\"\"A prompt for recommending a book.\"\"\"\n\n        prompt_template = \"\"\"\n        SYSTEM: You are the world's greatest librarian.\n        USER: Please recommend a {genre} book.\n        \"\"\"\n\n        genre: str\n\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n    print(prompt.messages())\n    #&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\n    print(prompt)\n    #&gt; Please recommend a fantasy book.\n    ```\n    '''\n\n    tags: ClassVar[list[str]] = []\n    prompt_template: ClassVar[str] = \"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the formatted template.\"\"\"\n        return self._format_template(self.prompt_template)\n\n    def messages(self) -&gt; Union[list[Message], Any]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        message_type_by_role = {\n            MessageRole.SYSTEM: SystemMessage,\n            MessageRole.USER: UserMessage,\n            MessageRole.ASSISTANT: AssistantMessage,\n            MessageRole.MODEL: ModelMessage,\n            MessageRole.TOOL: ToolMessage,\n        }\n        return [\n            message_type_by_role[MessageRole(message[\"role\"])](\n                role=message[\"role\"], content=message[\"content\"]\n            )\n            for message in self._parse_messages(list(message_type_by_role.keys()))\n        ]\n\n    def dump(\n        self,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n        return {\n            \"tags\": self.tags,\n            \"template\": dedent(self.prompt_template).strip(\"\\n\"),\n            \"inputs\": self.model_dump(),\n        }\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _format_template(self, template: str):\n        \"\"\"Formats the given `template` with attributes matching template variables.\"\"\"\n        dedented_template = dedent(template).strip()\n        template_vars = [\n            var\n            for _, var, _, _ in Formatter().parse(dedented_template)\n            if var is not None\n        ]\n        return dedented_template.format(\n            **{var: getattr(self, var) for var in template_vars}\n        )\n\n    def _parse_messages(self, roles: list[str]) -&gt; list[Message]:\n        \"\"\"Returns messages parsed from the `template` ClassVar.\n\n        Raises:\n            ValueError: if the template contains an unknown role.\n        \"\"\"\n        messages = []\n        re_roles = \"|\".join(\n            [role.upper() for role in roles] + [\"MESSAGES\"] + [\"[A-Z]+\"]\n        )\n        for match in re.finditer(\n            rf\"({re_roles}):((.|\\n)+?)(?=({re_roles}):|\\Z)\",\n            self.prompt_template,\n        ):\n            role = match.group(1).lower()\n            if role == \"messages\":\n                template_var = [\n                    var\n                    for _, var, _, _ in Formatter().parse(match.group(2))\n                    if var is not None\n                ][0]\n                if not hasattr(self, template_var) or not isinstance(\n                    getattr(self, template_var), list\n                ):\n                    raise ValueError(\n                        f\"MESSAGES keyword used with attribute `{template_var}`, which \"\n                        \"is not a `list` of messages.\"\n                    )\n                messages += getattr(self, template_var)\n            else:\n                if role not in roles:\n                    raise ValueError(f\"Invalid role: {role}\")\n                content = self._format_template(match.group(2))\n                messages.append({\"role\": role, \"content\": content})\n        if len(messages) == 0:\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": self._format_template(self.prompt_template),\n                }\n            )\n        return messages\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the formatted template.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the formatted template.\"\"\"\n    return self._format_template(self.prompt_template)\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt.dump","title":"<code>dump()</code>","text":"<p>Dumps the contents of the prompt into a dictionary.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def dump(\n    self,\n) -&gt; dict[str, Any]:\n    \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n    return {\n        \"tags\": self.tags,\n        \"template\": dedent(self.prompt_template).strip(\"\\n\"),\n        \"inputs\": self.model_dump(),\n    }\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def messages(self) -&gt; Union[list[Message], Any]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    message_type_by_role = {\n        MessageRole.SYSTEM: SystemMessage,\n        MessageRole.USER: UserMessage,\n        MessageRole.ASSISTANT: AssistantMessage,\n        MessageRole.MODEL: ModelMessage,\n        MessageRole.TOOL: ToolMessage,\n    }\n    return [\n        message_type_by_role[MessageRole(message[\"role\"])](\n            role=message[\"role\"], content=message[\"content\"]\n        )\n        for message in self._parse_messages(list(message_type_by_role.keys()))\n    ]\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.tags","title":"<code>tags(args)</code>","text":"<p>A decorator for adding tags to a <code>BasePrompt</code>.</p> <p>Adding this decorator to a <code>BasePrompt</code> updates the <code>_tags</code> class attribute to the given value. This is useful for adding metadata to a <code>BasePrompt</code> that can be used for logging or filtering.</p> <p>Example:</p> <pre><code>from mirascope import BasePrompt, tags\n\n\n@tags([\"book_recommendation\", \"entertainment\"])\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    I've recently read this book: {book_title}.\n    What should I read next?\n    \"\"\"\n\n    book_title: [str]\n\nprint(BookRecommendationPrompt.dump()[\"tags\"])\n#&gt; ['book_recommendation', 'entertainment']\n</code></pre> <p>Returns:</p> Type Description <code>Callable[[Type[BasePromptT]], Type[BasePromptT]]</code> <p>The decorated class with <code>tags</code> class attribute set.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def tags(args: list[str]) -&gt; Callable[[Type[BasePromptT]], Type[BasePromptT]]:\n    '''A decorator for adding tags to a `BasePrompt`.\n\n    Adding this decorator to a `BasePrompt` updates the `_tags` class attribute to the\n    given value. This is useful for adding metadata to a `BasePrompt` that can be used\n    for logging or filtering.\n\n    Example:\n\n    ```python\n    from mirascope import BasePrompt, tags\n\n\n    @tags([\"book_recommendation\", \"entertainment\"])\n    class BookRecommendationPrompt(BasePrompt):\n        prompt_template = \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        I've recently read this book: {book_title}.\n        What should I read next?\n        \"\"\"\n\n        book_title: [str]\n\n    print(BookRecommendationPrompt.dump()[\"tags\"])\n    #&gt; ['book_recommendation', 'entertainment']\n    ```\n\n    Returns:\n        The decorated class with `tags` class attribute set.\n    '''\n\n    def tags_fn(model_class: Type[BasePromptT]) -&gt; Type[BasePromptT]:\n        \"\"\"Updates the `tags` class attribute to the given value.\"\"\"\n        setattr(model_class, \"tags\", args)\n        return model_class\n\n    return tags_fn\n</code></pre>"},{"location":"api/base/tools/","title":"base.tools","text":"<p>A base interface for using tools (function calling) when calling LLMs.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool","title":"<code>BaseTool</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[ToolCallT]</code>, <code>ABC</code></p> <p>A base class for easy use of tools with prompts.</p> <p><code>BaseTool</code> is an abstract class interface and should not be used directly. When implementing a class that extends <code>BaseTool</code>, you must include the original <code>tool_call</code> from which this till was instantiated. Make sure to skip <code>tool_call</code> when generating the schema by annotating it with <code>SkipJsonSchema</code>.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>class BaseTool(BaseModel, Generic[ToolCallT], ABC):\n    \"\"\"A base class for easy use of tools with prompts.\n\n    `BaseTool` is an abstract class interface and should not be used directly. When\n    implementing a class that extends `BaseTool`, you must include the original\n    `tool_call` from which this till was instantiated. Make sure to skip `tool_call`\n    when generating the schema by annotating it with `SkipJsonSchema`.\n    \"\"\"\n\n    tool_call: SkipJsonSchema[ToolCallT]\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def args(self) -&gt; dict[str, Any]:\n        \"\"\"The arguments of the tool as a dictionary.\"\"\"\n        return self.model_dump(exclude={\"tool_call\"})\n\n    @property\n    def fn(self) -&gt; Callable:\n        \"\"\"Returns the function that the tool describes.\"\"\"\n        raise RuntimeError(\"Tool does not have an attached function.\")\n\n    @classmethod\n    def tool_schema(cls) -&gt; Any:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n\n        fn = {\n            \"name\": model_schema.pop(\"title\"),\n            \"description\": model_schema.pop(\"description\")\n            if \"description\" in model_schema\n            else DEFAULT_TOOL_DOCSTRING,\n        }\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return fn\n\n    @classmethod\n    @abstractmethod\n    def from_tool_call(cls, tool_call: ToolCallT) -&gt; BaseTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[BaseTool]:\n        \"\"\"Constructs a `BaseTool` type from a `BaseModel` type.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[BaseTool]:\n        \"\"\"Constructs a `BaseTool` type from a function.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[BaseTool]:\n        \"\"\"Constructs a `BaseTool` type from a `BaseType` type.\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.args","title":"<code>args: dict[str, Any]</code>  <code>property</code>","text":"<p>The arguments of the tool as a dictionary.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.fn","title":"<code>fn: Callable</code>  <code>property</code>","text":"<p>Returns the function that the tool describes.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a <code>BaseTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[BaseTool]:\n    \"\"\"Constructs a `BaseTool` type from a `BaseType` type.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_fn","title":"<code>from_fn(fn)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a <code>BaseTool</code> type from a function.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[BaseTool]:\n    \"\"\"Constructs a `BaseTool` type from a function.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_model","title":"<code>from_model(model)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a <code>BaseTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[BaseTool]:\n    \"\"\"Constructs a `BaseTool` type from a `BaseModel` type.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_tool_call(cls, tool_call: ToolCallT) -&gt; BaseTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Any:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n\n    fn = {\n        \"name\": model_schema.pop(\"title\"),\n        \"description\": model_schema.pop(\"description\")\n        if \"description\" in model_schema\n        else DEFAULT_TOOL_DOCSTRING,\n    }\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return fn\n</code></pre>"},{"location":"api/base/types/","title":"base.types","text":"<p>Base types and abstract interfaces for typing LLM calls.</p>"},{"location":"api/base/types/#mirascope.base.types.AssistantMessage","title":"<code>AssistantMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>assistant</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['assistant']]</code> <p>The role of the message's author, in this case <code>assistant</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class AssistantMessage(TypedDict, total=False):\n    \"\"\"A message with the `assistant` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `assistant`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"assistant\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallParams","title":"<code>BaseCallParams</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[BaseToolT]</code></p> <p>The parameters with which to make a call.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallParams(BaseModel, Generic[BaseToolT]):\n    \"\"\"The parameters with which to make a call.\"\"\"\n\n    model: str\n    tools: Optional[list[Union[Callable, Type[BaseToolT]]]] = None\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    def kwargs(\n        self,\n        tool_type: Optional[Type[BaseToolT]] = None,\n        exclude: Optional[set[str]] = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the call as a keyword arguments dictionary.\"\"\"\n        extra_exclude = {\"tools\"}\n        exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n        kwargs = {\n            key: value\n            for key, value in self.model_dump(exclude=exclude).items()\n            if value is not None\n        }\n        if not self.tools or tool_type is None:\n            return kwargs\n        kwargs[\"tools\"] = [\n            tool if isclass(tool) else convert_function_to_tool(tool, tool_type)\n            for tool in self.tools\n        ]\n        return kwargs\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallParams.kwargs","title":"<code>kwargs(tool_type=None, exclude=None)</code>","text":"<p>Returns all parameters for the call as a keyword arguments dictionary.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>def kwargs(\n    self,\n    tool_type: Optional[Type[BaseToolT]] = None,\n    exclude: Optional[set[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the call as a keyword arguments dictionary.\"\"\"\n    extra_exclude = {\"tools\"}\n    exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n    kwargs = {\n        key: value\n        for key, value in self.model_dump(exclude=exclude).items()\n        if value is not None\n    }\n    if not self.tools or tool_type is None:\n        return kwargs\n    kwargs[\"tools\"] = [\n        tool if isclass(tool) else convert_function_to_tool(tool, tool_type)\n        for tool in self.tools\n    ]\n    return kwargs\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse","title":"<code>BaseCallResponse</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[ResponseT, BaseToolT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM call responses.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>ResponseT</code> <p>The original response from whichever model response this wraps.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallResponse(BaseModel, Generic[ResponseT, BaseToolT], ABC):\n    \"\"\"A base abstract interface for LLM call responses.\n\n    Attributes:\n        response: The original response from whichever model response this wraps.\n    \"\"\"\n\n    response: ResponseT\n    tool_types: Optional[list[Type[BaseToolT]]] = None\n    start_time: float  # The start time of the completion in ms\n    end_time: float  # The end time of the completion in ms\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def tools(self) -&gt; Optional[list[BaseToolT]]:\n        \"\"\"Returns the tools for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def tool(self) -&gt; Optional[BaseToolT]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response.\n\n        If there are multiple choices in a response, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response.</p> <p>If there are multiple choices in a response, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse.tool","title":"<code>tool: Optional[BaseToolT]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse.tools","title":"<code>tools: Optional[list[BaseToolT]]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponseChunk","title":"<code>BaseCallResponseChunk</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[ChunkT, BaseToolT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM streaming response chunks.</p> <p>Attributes:</p> Name Type Description <code>response</code> <p>The original response chunk from whichever model response this wraps.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallResponseChunk(BaseModel, Generic[ChunkT, BaseToolT], ABC):\n    \"\"\"A base abstract interface for LLM streaming response chunks.\n\n    Attributes:\n        response: The original response chunk from whichever model response this wraps.\n    \"\"\"\n\n    chunk: ChunkT\n    tool_types: Optional[list[Type[BaseToolT]]] = None\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response chunk.\n\n        If there are multiple choices in a chunk, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponseChunk.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response chunk.</p> <p>If there are multiple choices in a chunk, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/base/types/#mirascope.base.types.ModelMessage","title":"<code>ModelMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>model</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['model']]</code> <p>The role of the message's author, in this case <code>model</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class ModelMessage(TypedDict, total=False):\n    \"\"\"A message with the `model` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `model`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"model\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.SystemMessage","title":"<code>SystemMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>system</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['system']]</code> <p>The role of the message's author, in this case <code>system</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class SystemMessage(TypedDict, total=False):\n    \"\"\"A message with the `system` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `system`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"system\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>tool</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['tool']]</code> <p>The role of the message's author, in this case <code>tool</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class ToolMessage(TypedDict, total=False):\n    \"\"\"A message with the `tool` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `tool`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"tool\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.UserMessage","title":"<code>UserMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>user</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['user']]</code> <p>The role of the message's author, in this case <code>user</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class UserMessage(TypedDict, total=False):\n    \"\"\"A message with the `user` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `user`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"user\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/utils/","title":"base.utils","text":"<p>Base utility functions.</p>"},{"location":"api/base/utils/#mirascope.base.utils.convert_base_model_to_tool","title":"<code>convert_base_model_to_tool(schema, base)</code>","text":"<p>Converts a <code>BaseModel</code> schema to a <code>BaseToolT</code> type.</p> <p>By adding a docstring (if needed) and passing on fields and field information in dictionary format, a Pydantic <code>BaseModel</code> can be converted into an <code>BaseToolT</code> for performing extraction.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModel]</code> <p>The <code>BaseModel</code> schema to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseToolT]</code> <p>The constructed <code>BaseToolT</code> type.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def convert_base_model_to_tool(\n    schema: Type[BaseModel], base: Type[BaseToolT]\n) -&gt; Type[BaseToolT]:\n    \"\"\"Converts a `BaseModel` schema to a `BaseToolT` type.\n\n    By adding a docstring (if needed) and passing on fields and field information in\n    dictionary format, a Pydantic `BaseModel` can be converted into an `BaseToolT` for\n    performing extraction.\n\n    Args:\n        schema: The `BaseModel` schema to convert.\n\n    Returns:\n        The constructed `BaseToolT` type.\n    \"\"\"\n    field_definitions = {\n        field_name: (field_info.annotation, field_info)\n        for field_name, field_info in schema.model_fields.items()\n    }\n    return create_model(\n        f\"{schema.__name__}\",\n        __base__=base,\n        __doc__=schema.__doc__ if schema.__doc__ else DEFAULT_TOOL_DOCSTRING,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/base/utils/#mirascope.base.utils.convert_base_type_to_tool","title":"<code>convert_base_type_to_tool(schema, base)</code>","text":"<p>Converts a <code>BaseType</code> to a <code>BaseToolT</code> type.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def convert_base_type_to_tool(\n    schema: Type[_T], base: Type[BaseToolT]\n) -&gt; Type[BaseToolT]:\n    \"\"\"Converts a `BaseType` to a `BaseToolT` type.\"\"\"\n    return create_model(\n        f\"{schema.__name__[0].upper()}{schema.__name__[1:]}\",\n        __base__=base,\n        __doc__=DEFAULT_TOOL_DOCSTRING,\n        value=(schema, ...),\n    )\n</code></pre>"},{"location":"api/base/utils/#mirascope.base.utils.convert_function_to_tool","title":"<code>convert_function_to_tool(fn, base)</code>","text":"<p>Constructs a <code>BaseToolT</code> type from the given function.</p> <p>This method expects all function parameters to be properly documented in identical order with identical variable names, as well as descriptions of each parameter. Errors will be raised if any of these conditions are not met.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseToolT]</code> <p>The constructed <code>BaseToolT</code> type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the given function doesn't have a docstring.</p> <code>ValueError</code> <p>if the given function's parameters don't have type annotations.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but the name doesn't match the docstring's parameter name.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but doesn't have a dosctring description.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def convert_function_to_tool(fn: Callable, base: Type[BaseToolT]) -&gt; Type[BaseToolT]:\n    \"\"\"Constructs a `BaseToolT` type from the given function.\n\n    This method expects all function parameters to be properly documented in identical\n    order with identical variable names, as well as descriptions of each parameter.\n    Errors will be raised if any of these conditions are not met.\n\n    Args:\n        fn: The function to convert.\n\n    Returns:\n        The constructed `BaseToolT` type.\n\n    Raises:\n        ValueError: if the given function doesn't have a docstring.\n        ValueError: if the given function's parameters don't have type annotations.\n        ValueError: if a given function's parameter is in the docstring args section but\n            the name doesn't match the docstring's parameter name.\n        ValueError: if a given function's parameter is in the docstring args section but\n            doesn't have a dosctring description.\n    \"\"\"\n    if not fn.__doc__:\n        raise ValueError(\"Function must have a docstring.\")\n\n    docstring = parse(fn.__doc__)\n\n    doc = \"\"\n    if docstring.short_description:\n        doc = docstring.short_description\n    if docstring.long_description:\n        doc += \"\\n\\n\" + docstring.long_description\n    if docstring.returns and docstring.returns.description:\n        doc += \"\\n\\n\" + \"Returns:\\n    \" + docstring.returns.description\n\n    field_definitions = {}\n    hints = get_type_hints(fn)\n    for i, parameter in enumerate(signature(fn).parameters.values()):\n        if parameter.name == \"self\" or parameter.name == \"cls\":\n            continue\n        if parameter.annotation == Parameter.empty:\n            raise ValueError(\"All parameters must have a type annotation.\")\n\n        docstring_description = None\n        if i &lt; len(docstring.params):\n            docstring_param = docstring.params[i]\n            if docstring_param.arg_name != parameter.name:\n                raise ValueError(\n                    f\"Function parameter name {parameter.name} does not match docstring \"\n                    f\"parameter name {docstring_param.arg_name}. Make sure that the \"\n                    \"parameter names match exactly.\"\n                )\n            if not docstring_param.description:\n                raise ValueError(\"All parameters must have a description.\")\n            docstring_description = docstring_param.description\n\n        field_info = FieldInfo(annotation=hints[parameter.name])\n        if parameter.default != Parameter.empty:\n            field_info.default = parameter.default\n        if docstring_description:  # we check falsy here because this comes from docstr\n            field_info.description = docstring_description\n\n        param_name = parameter.name\n        if param_name.startswith(\"model_\"):  # model_ is a BaseModel reserved namespace\n            param_name = \"aliased_\" + param_name\n            field_info.alias = parameter.name\n            field_info.validation_alias = parameter.name\n            field_info.serialization_alias = parameter.name\n\n        field_definitions[param_name] = (\n            hints[parameter.name],\n            field_info,\n        )\n\n    return create_model(\n        \"\".join(word.title() for word in fn.__name__.split(\"_\")),\n        __base__=tool_fn(fn)(base),\n        __doc__=doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/base/utils/#mirascope.base.utils.tool_fn","title":"<code>tool_fn(fn)</code>","text":"<p>A decorator for adding a function to a tool class.</p> <p>Adding this decorator will add an <code>fn</code> property to the tool class that returns the function that the tool describes. This is convenient for calling the function given an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to add to the tool class.</p> required <p>Returns:</p> Type Description <code>Callable[[Type[BaseToolT]], Type[BaseToolT]]</code> <p>The decorated tool class.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def tool_fn(fn: Callable) -&gt; Callable[[Type[BaseToolT]], Type[BaseToolT]]:\n    \"\"\"A decorator for adding a function to a tool class.\n\n    Adding this decorator will add an `fn` property to the tool class that returns the\n    function that the tool describes. This is convenient for calling the function given\n    an instance of the tool.\n\n    Args:\n        fn: The function to add to the tool class.\n\n    Returns:\n        The decorated tool class.\n    \"\"\"\n\n    def decorator(cls: Type[BaseToolT]) -&gt; Type[BaseToolT]:\n        \"\"\"A decorator for adding a function to a tool class.\"\"\"\n        setattr(cls, \"fn\", property(lambda self: fn))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/cli/","title":"cli","text":"<p>This module contains all functionality related to the Mirascope CLI.</p>"},{"location":"api/cli/commands/","title":"cli.commands","text":"<p>The Mirascope CLI commands module</p>"},{"location":"api/cli/constants/","title":"cli.constants","text":"<p>Constants for Mirascope CLI.</p>"},{"location":"api/cli/generic/","title":"cli.generic","text":"<p>This module contains generic templates used to initialize a mirascope project.</p>"},{"location":"api/cli/schemas/","title":"cli.schemas","text":"<p>Contains the schema for files created by the mirascope cli.</p>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.MirascopeCliVariables","title":"<code>MirascopeCliVariables</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Prompt version variables used internally by mirascope.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class MirascopeCliVariables(BaseModel):\n    \"\"\"Prompt version variables used internally by mirascope.\"\"\"\n\n    prev_revision_id: Optional[str] = Field(default=None)\n    revision_id: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.MirascopeSettings","title":"<code>MirascopeSettings</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the user's mirascope settings.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class MirascopeSettings(BaseModel):\n    \"\"\"Model for the user's mirascope settings.\"\"\"\n\n    mirascope_location: str\n    versions_location: str\n    prompts_location: str\n    version_file_name: str\n    format_command: Optional[str] = None\n    auto_tag: Optional[bool] = None\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.VersionTextFile","title":"<code>VersionTextFile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the version text file.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class VersionTextFile(BaseModel):\n    \"\"\"Model for the version text file.\"\"\"\n\n    current_revision: Optional[str] = Field(default=None)\n    latest_revision: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/cli/utils/","title":"cli.utils","text":"<p>Utility functions for the mirascope library.</p>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer","title":"<code>PromptAnalyzer</code>","text":"<p>             Bases: <code>NodeVisitor</code></p> <p>Utility class for analyzing a Mirascope prompt file.</p> <p>The call to <code>ast.parse()</code> returns Python code as an AST, whereby each visitor method will be called for the corresponding nodes in the AST via <code>NodeVisitor.visit()</code>.</p> <p>Example:</p> <pre><code>analyzer = PromptAnalyzer()\ntree = ast.parse(file.read())\nanalyzer.visit(tree)\n</code></pre> Source code in <code>mirascope/cli/utils.py</code> <pre><code>class PromptAnalyzer(ast.NodeVisitor):\n    \"\"\"Utility class for analyzing a Mirascope prompt file.\n\n    The call to `ast.parse()` returns Python code as an AST, whereby each visitor method\n    will be called for the corresponding nodes in the AST via `NodeVisitor.visit()`.\n\n    Example:\n\n    ```python\n    analyzer = PromptAnalyzer()\n    tree = ast.parse(file.read())\n    analyzer.visit(tree)\n    ```\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the PromptAnalyzer.\"\"\"\n        self.imports: list[tuple[str, Optional[str]]] = []\n        self.from_imports: list[tuple[str, str, Optional[str]]] = []\n        self.variables: dict[str, Any] = {}\n        self.classes: list[ClassInfo] = []\n        self.functions: list[FunctionInfo] = []\n        self.comments: str = \"\"\n\n    def visit_Import(self, node) -&gt; None:\n        \"\"\"Extracts imports from the given node.\"\"\"\n        for alias in node.names:\n            self.imports.append((alias.name, alias.asname))\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node) -&gt; None:\n        \"\"\"Extracts from imports from the given node.\"\"\"\n        for alias in node.names:\n            self.from_imports.append((node.module, alias.name, alias.asname))\n        self.generic_visit(node)\n\n    def visit_Assign(self, node) -&gt; None:\n        \"\"\"Extracts variables from the given node.\"\"\"\n        target = node.targets[0]\n        if isinstance(target, ast.Name):\n            self.variables[target.id] = ast.unparse(node.value)\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node) -&gt; None:\n        \"\"\"Extracts classes from the given node.\"\"\"\n        class_info = ClassInfo(\n            name=node.name,\n            bases=[ast.unparse(base) for base in node.bases],\n            body=\"\",\n            decorators=[ast.unparse(decorator) for decorator in node.decorator_list],\n            docstring=None,\n        )\n\n        # Extract docstring if present\n        docstring = ast.get_docstring(node, False)\n        if docstring:\n            class_info.docstring = docstring\n\n        # Handle the rest of the class body\n        body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n        body = []\n        for node in body_nodes:\n            if (\n                isinstance(node, ast.Assign)\n                and isinstance(node.targets[0], ast.Name)\n                and node.targets[0].id == \"prompt_template\"\n                and isinstance(node.value, ast.Constant)\n                and node.end_lineno is not None\n                and node.lineno &lt; node.end_lineno\n            ):\n                # reconstruct template strings to be multi-line\n                lines = node.value.s.split(\"\\n\")\n                body.append(f'{node.targets[0].id} = \"\"\"{lines.pop(0).strip()}')\n                for i, line in enumerate(lines):\n                    stripped_line = line.strip()\n                    if stripped_line or i &lt; len(lines) - 1:\n                        body.append(line.strip())\n                body.append('\"\"\"')\n                body.append(\"\")  # adds final newline\n            else:\n                body.append(ast.unparse(node))\n        class_info.body = \"\\n\".join(body)\n\n        self.classes.append(class_info)\n\n    def visit_AsyncFunctionDef(self, node):\n        \"\"\"Extracts async functions from the given node.\"\"\"\n        return self._visit_Function(node, is_async=True)\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Extracts functions from the given node.\"\"\"\n        return self._visit_Function(node, is_async=False)\n\n    def _visit_Function(self, node, is_async):\n        \"\"\"Extracts functions or async functions from the given node.\"\"\"\n        # Initial function information setup\n        function_info = FunctionInfo(\n            name=node.name,\n            args=[ast.unparse(arg) for arg in node.args.args],\n            returns=ast.unparse(node.returns) if node.returns else None,\n            body=\"\",\n            decorators=[ast.unparse(decorator) for decorator in node.decorator_list],\n            docstring=None,\n            is_async=is_async,  # Indicates whether the function is async\n        )\n\n        # Extract docstring if present\n        docstring = ast.get_docstring(node, False)\n        if docstring:\n            function_info.docstring = docstring\n\n        # Handle the rest of the function body\n        body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n        function_info.body = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n        # Assuming you have a list to store functions\n        self.functions.append(function_info)\n\n    def visit_Module(self, node) -&gt; None:\n        \"\"\"Extracts comments from the given node.\"\"\"\n        comments = ast.get_docstring(node, False)\n        self.comments = \"\" if comments is None else comments\n        self.generic_visit(node)\n\n    def check_function_changed(self, other: PromptAnalyzer) -&gt; bool:\n        \"\"\"Compares the functions of this file with those of another file.\"\"\"\n        return self._check_definition_changed(other, \"function\")\n\n    def check_class_changed(self, other: PromptAnalyzer) -&gt; bool:\n        \"\"\"Compares the classes of this file with those of another file.\"\"\"\n        return self._check_definition_changed(other)\n\n    def _check_definition_changed(\n        self,\n        other: PromptAnalyzer,\n        definition_type: Optional[Literal[\"class\", \"function\"]] = \"class\",\n    ) -&gt; bool:\n        \"\"\"Compares classes or the functions of this file with those of another file\"\"\"\n\n        self_definitions: Union[list[ClassInfo], list[FunctionInfo]] = (\n            self.classes if definition_type == \"class\" else self.functions\n        )\n        other_definitions: Union[list[ClassInfo], list[FunctionInfo]] = (\n            other.classes if definition_type == \"class\" else other.functions\n        )\n\n        self_definitions_dict = {\n            definition.name: definition for definition in self_definitions\n        }\n        other_definitions_dict = {\n            definition.name: definition for definition in other_definitions\n        }\n\n        all_definition_names = set(self_definitions_dict.keys()) | set(\n            other_definitions_dict.keys()\n        )\n\n        for name in all_definition_names:\n            if name in self_definitions_dict and name in other_definitions_dict:\n                self_def_dict = self_definitions_dict[name].__dict__\n                other_def_dict = other_definitions_dict[name].__dict__\n                # Compare attributes of definitions with the same name\n                def_diff = {\n                    attr: (self_def_dict[attr], other_def_dict[attr])\n                    for attr in self_def_dict\n                    if self_def_dict[attr] != other_def_dict[attr]\n                }\n                if def_diff:\n                    return True\n            else:\n                return True\n\n        return False\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PromptAnalyzer.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the PromptAnalyzer.\"\"\"\n    self.imports: list[tuple[str, Optional[str]]] = []\n    self.from_imports: list[tuple[str, str, Optional[str]]] = []\n    self.variables: dict[str, Any] = {}\n    self.classes: list[ClassInfo] = []\n    self.functions: list[FunctionInfo] = []\n    self.comments: str = \"\"\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.check_class_changed","title":"<code>check_class_changed(other)</code>","text":"<p>Compares the classes of this file with those of another file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_class_changed(self, other: PromptAnalyzer) -&gt; bool:\n    \"\"\"Compares the classes of this file with those of another file.\"\"\"\n    return self._check_definition_changed(other)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.check_function_changed","title":"<code>check_function_changed(other)</code>","text":"<p>Compares the functions of this file with those of another file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_function_changed(self, other: PromptAnalyzer) -&gt; bool:\n    \"\"\"Compares the functions of this file with those of another file.\"\"\"\n    return self._check_definition_changed(other, \"function\")\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Assign","title":"<code>visit_Assign(node)</code>","text":"<p>Extracts variables from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Assign(self, node) -&gt; None:\n    \"\"\"Extracts variables from the given node.\"\"\"\n    target = node.targets[0]\n    if isinstance(target, ast.Name):\n        self.variables[target.id] = ast.unparse(node.value)\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_AsyncFunctionDef","title":"<code>visit_AsyncFunctionDef(node)</code>","text":"<p>Extracts async functions from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_AsyncFunctionDef(self, node):\n    \"\"\"Extracts async functions from the given node.\"\"\"\n    return self._visit_Function(node, is_async=True)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ClassDef","title":"<code>visit_ClassDef(node)</code>","text":"<p>Extracts classes from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ClassDef(self, node) -&gt; None:\n    \"\"\"Extracts classes from the given node.\"\"\"\n    class_info = ClassInfo(\n        name=node.name,\n        bases=[ast.unparse(base) for base in node.bases],\n        body=\"\",\n        decorators=[ast.unparse(decorator) for decorator in node.decorator_list],\n        docstring=None,\n    )\n\n    # Extract docstring if present\n    docstring = ast.get_docstring(node, False)\n    if docstring:\n        class_info.docstring = docstring\n\n    # Handle the rest of the class body\n    body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n    body = []\n    for node in body_nodes:\n        if (\n            isinstance(node, ast.Assign)\n            and isinstance(node.targets[0], ast.Name)\n            and node.targets[0].id == \"prompt_template\"\n            and isinstance(node.value, ast.Constant)\n            and node.end_lineno is not None\n            and node.lineno &lt; node.end_lineno\n        ):\n            # reconstruct template strings to be multi-line\n            lines = node.value.s.split(\"\\n\")\n            body.append(f'{node.targets[0].id} = \"\"\"{lines.pop(0).strip()}')\n            for i, line in enumerate(lines):\n                stripped_line = line.strip()\n                if stripped_line or i &lt; len(lines) - 1:\n                    body.append(line.strip())\n            body.append('\"\"\"')\n            body.append(\"\")  # adds final newline\n        else:\n            body.append(ast.unparse(node))\n    class_info.body = \"\\n\".join(body)\n\n    self.classes.append(class_info)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_FunctionDef","title":"<code>visit_FunctionDef(node)</code>","text":"<p>Extracts functions from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_FunctionDef(self, node):\n    \"\"\"Extracts functions from the given node.\"\"\"\n    return self._visit_Function(node, is_async=False)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Import","title":"<code>visit_Import(node)</code>","text":"<p>Extracts imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Import(self, node) -&gt; None:\n    \"\"\"Extracts imports from the given node.\"\"\"\n    for alias in node.names:\n        self.imports.append((alias.name, alias.asname))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ImportFrom","title":"<code>visit_ImportFrom(node)</code>","text":"<p>Extracts from imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ImportFrom(self, node) -&gt; None:\n    \"\"\"Extracts from imports from the given node.\"\"\"\n    for alias in node.names:\n        self.from_imports.append((node.module, alias.name, alias.asname))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Module","title":"<code>visit_Module(node)</code>","text":"<p>Extracts comments from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Module(self, node) -&gt; None:\n    \"\"\"Extracts comments from the given node.\"\"\"\n    comments = ast.get_docstring(node, False)\n    self.comments = \"\" if comments is None else comments\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_prompt_changed","title":"<code>check_prompt_changed(file1_path, file2_path)</code>","text":"<p>Compare two prompts to check if the given prompts have changed.</p> <p>Parameters:</p> Name Type Description Default <code>file1_path</code> <code>Optional[str]</code> <p>The path to the first prompt.</p> required <code>file2_path</code> <code>Optional[str]</code> <p>The path to the second prompt.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether there are any differences between the two prompts.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_prompt_changed(file1_path: Optional[str], file2_path: Optional[str]) -&gt; bool:\n    \"\"\"Compare two prompts to check if the given prompts have changed.\n\n    Args:\n        file1_path: The path to the first prompt.\n        file2_path: The path to the second prompt.\n\n    Returns:\n        Whether there are any differences between the two prompts.\n    \"\"\"\n    if file1_path is None or file2_path is None:\n        raise FileNotFoundError(\"Prompt or version file is missing.\")\n    # Parse the first file\n    try:\n        with open(file1_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file1_path} was not found.\") from e\n    analyzer1 = PromptAnalyzer()\n    tree1 = ast.parse(content)\n    analyzer1.visit(tree1)\n\n    # Parse the second file\n    try:\n        with open(file2_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file2_path} was not found.\") from e\n    analyzer2 = PromptAnalyzer()\n    tree2 = ast.parse(content)\n    analyzer2.visit(tree2)\n    # Compare the contents of the two files\n    differences = {\n        \"comments\": analyzer1.comments != analyzer2.comments,\n        \"imports_diff\": bool(set(analyzer1.imports) ^ set(analyzer2.imports)),\n        \"from_imports_diff\": bool(\n            set(analyzer1.from_imports) ^ set(analyzer2.from_imports)\n        ),\n        \"functions_diff\": analyzer1.check_function_changed(analyzer2),\n        \"variables_diff\": set(analyzer1.variables.keys()) - ignore_variables\n        ^ set(analyzer2.variables.keys()) - ignore_variables,\n        \"classes_diff\": analyzer1.check_class_changed(analyzer2),\n        # Add other comparisons as needed\n    }\n    return any(differences.values())\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_status","title":"<code>check_status(mirascope_settings, directory)</code>","text":"<p>Checks the status of the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>mirascope_settings</code> <code>MirascopeSettings</code> <p>The user's mirascope settings.</p> required <code>directory</code> <code>str</code> <p>The name of the prompt file (excluding the .py extension).</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The path to the prompt if the prompt has changed, otherwise <code>None</code>.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_status(\n    mirascope_settings: MirascopeSettings, directory: str\n) -&gt; Optional[str]:\n    \"\"\"Checks the status of the given directory.\n\n    Args:\n        mirascope_settings: The user's mirascope settings.\n        directory: The name of the prompt file (excluding the .py extension).\n\n    Returns:\n        The path to the prompt if the prompt has changed, otherwise `None`.\n    \"\"\"\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n    prompt_directory = os.path.join(version_directory_path, directory)\n    used_prompt_path = f\"{prompt_directory_path}/{directory}.py\"\n\n    # Get the currently used prompt version\n    versions = get_prompt_versions(f\"{prompt_directory}/{version_file_name}\")\n    current_head = versions.current_revision\n    if current_head is None:\n        return used_prompt_path\n    current_version_prompt_path = find_prompt_path(prompt_directory, current_head)\n    # Check if users prompt matches the current prompt version\n    has_file_changed = check_prompt_changed(\n        current_version_prompt_path, used_prompt_path\n    )\n    if has_file_changed:\n        return used_prompt_path\n    return None\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_file_names","title":"<code>find_file_names(directory, prefix='')</code>","text":"<p>Finds all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to search for the prompt.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the prompt to search for.</p> <code>''</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of file names found.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_file_names(directory: str, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"Finds all files in a directory.\n\n    Args:\n        directory: The directory to search for the prompt.\n        prefix: The prefix of the prompt to search for.\n\n    Returns:\n        A list of file names found.\n    \"\"\"\n    pattern = os.path.join(directory, f\"[!_]{prefix}*.py\")  # ignores private files\n    matching_files_with_dir = glob.glob(pattern)\n\n    # Removing the directory part from each path\n    return [os.path.basename(file) for file in matching_files_with_dir]\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_prompt_path","title":"<code>find_prompt_path(directory, prefix)</code>","text":"<p>Finds and opens the first found prompt with the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[Path, str]</code> <p>The directory to search for the prompt.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the prompt to search for.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The path to the prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_prompt_path(directory: Union[Path, str], prefix: str) -&gt; Optional[str]:\n    \"\"\"Finds and opens the first found prompt with the given directory.\n\n    Args:\n        directory: The directory to search for the prompt.\n        prefix: The prefix of the prompt to search for.\n\n    Returns:\n        The path to the prompt.\n    \"\"\"\n    prompt_files = find_prompt_paths(directory, prefix)\n    if prompt_files:\n        return prompt_files[0]\n    return None\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_prompt_paths","title":"<code>find_prompt_paths(directory, prefix)</code>","text":"<p>Finds and opens all prompts with the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[Path, str]</code> <p>The directory to search for the prompt.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the prompt to search for.</p> required <p>Returns:</p> Type Description <code>Optional[list[str]]</code> <p>A list of paths to the prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_prompt_paths(directory: Union[Path, str], prefix: str) -&gt; Optional[list[str]]:\n    \"\"\"Finds and opens all prompts with the given directory.\n\n    Args:\n        directory: The directory to search for the prompt.\n        prefix: The prefix of the prompt to search for.\n\n    Returns:\n        A list of paths to the prompt.\n    \"\"\"\n    pattern = os.path.join(directory, prefix + \"*.py\")\n    prompt_files = glob.glob(pattern)\n\n    if not prompt_files:\n        return None  # No files found\n\n    # Return first file found\n    return prompt_files\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_prompt_analyzer","title":"<code>get_prompt_analyzer(file)</code>","text":"<p>Gets an instance of PromptAnalyzer for a file</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file to analyze</p> required <p>Returns:</p> Type Description <code>PromptAnalyzer</code> <p>An instance of PromptAnalyzer</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_prompt_analyzer(file: str) -&gt; PromptAnalyzer:\n    \"\"\"Gets an instance of PromptAnalyzer for a file\n\n    Args:\n        file: The file to analyze\n\n    Returns:\n        An instance of PromptAnalyzer\n    \"\"\"\n    analyzer = PromptAnalyzer()\n    tree = ast.parse(file)\n    analyzer.visit(tree)\n    return analyzer\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_prompt_versions","title":"<code>get_prompt_versions(version_file_path)</code>","text":"<p>Returns the versions of the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>version_file_path</code> <code>str</code> <p>The path to the prompt.</p> required <p>Returns:</p> Type Description <code>VersionTextFile</code> <p>A <code>VersionTextFile</code> instance with the versions of current and latest revisions.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_prompt_versions(version_file_path: str) -&gt; VersionTextFile:\n    \"\"\"Returns the versions of the given prompt.\n\n    Args:\n        version_file_path: The path to the prompt.\n\n    Returns:\n        A `VersionTextFile` instance with the versions of current and latest revisions.\n    \"\"\"\n    versions = VersionTextFile()\n    try:\n        with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n            file.seek(0)\n            for line in file:\n                # Check if the current line contains the key\n                if line.startswith(CURRENT_REVISION_KEY + \"=\"):\n                    versions.current_revision = line.split(\"=\")[1].strip()\n                elif line.startswith(LATEST_REVISION_KEY + \"=\"):\n                    versions.latest_revision = line.split(\"=\")[1].strip()\n            return versions\n    except FileNotFoundError:\n        return versions\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_user_mirascope_settings","title":"<code>get_user_mirascope_settings(ini_file_path='mirascope.ini')</code>","text":"<p>Returns the user's mirascope settings.</p> <p>Parameters:</p> Name Type Description Default <code>ini_file_path</code> <code>str</code> <p>The path to the mirascope.ini file.</p> <code>'mirascope.ini'</code> <p>Returns:</p> Type Description <code>MirascopeSettings</code> <p>The user's mirascope settings as a <code>MirascopeSettings</code> instance.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the mirascope.ini file is not found.</p> <code>KeyError</code> <p>If the [mirascope] section is missing from the mirascope.ini file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_user_mirascope_settings(\n    ini_file_path: str = \"mirascope.ini\",\n) -&gt; MirascopeSettings:\n    \"\"\"Returns the user's mirascope settings.\n\n    Args:\n        ini_file_path: The path to the mirascope.ini file.\n\n    Returns:\n        The user's mirascope settings as a `MirascopeSettings` instance.\n\n    Raises:\n        FileNotFoundError: If the mirascope.ini file is not found.\n        KeyError: If the [mirascope] section is missing from the mirascope.ini file.\n    \"\"\"\n    config = ConfigParser(allow_no_value=True)\n    try:\n        read_ok = config.read(ini_file_path)\n        if not read_ok:\n            raise FileNotFoundError(\n                \"The mirascope.ini file was not found. Please run \"\n                \"`mirascope init` to create one or run the mirascope CLI from the \"\n                \"same directory as the mirascope.ini file.\"\n            )\n        mirascope_config = config[\"mirascope\"]\n        return MirascopeSettings(**mirascope_config)\n    except MissingSectionHeaderError as e:\n        raise MissingSectionHeaderError(ini_file_path, e.lineno, e.source) from e\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.parse_prompt_file_name","title":"<code>parse_prompt_file_name(prompt_file_name)</code>","text":"<p>Returns the file name without the .py extension.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def parse_prompt_file_name(prompt_file_name: str) -&gt; str:\n    \"\"\"Returns the file name without the .py extension.\"\"\"\n    if prompt_file_name.endswith(\".py\"):\n        return prompt_file_name[:-3]\n    return prompt_file_name\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.prompts_directory_files","title":"<code>prompts_directory_files()</code>","text":"<p>Returns a list of files in the user's prompts directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def prompts_directory_files() -&gt; list[str]:\n    \"\"\"Returns a list of files in the user's prompts directory.\"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    prompt_file_names = find_file_names(mirascope_settings.prompts_location)\n    return [f\"{name[:-3]}\" for name in prompt_file_names]  # remove .py extension\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.run_format_command","title":"<code>run_format_command(file)</code>","text":"<p>Runs the format command on the given file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file to format</p> required Source code in <code>mirascope/cli/utils.py</code> <pre><code>def run_format_command(file: str) -&gt; None:\n    \"\"\"Runs the format command on the given file.\n\n    Args:\n        file: The file to format\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    if mirascope_settings.format_command:\n        format_commands: list[list[str]] = [\n            command.split() for command in mirascope_settings.format_command.split(\";\")\n        ]\n        # assuming the final command takes filename as argument, and as final argument\n        format_commands[-1].append(file)\n        for command in format_commands:\n            subprocess.run(\n                [sys.executable, \"-m\"] + command,\n                check=True,\n                capture_output=True,\n                shell=False,\n            )\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.update_version_text_file","title":"<code>update_version_text_file(version_file, updates)</code>","text":"<p>Updates the version text file.</p> <p>Depending on the contents of <code>updates</code>, updates the ids of the current and latest revisions of the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>version_file</code> <code>str</code> <p>The path to the version text file.</p> required <code>updates</code> <code>dict[str, str]</code> <p>A dictionary containing updates to the current revision id and/or the latest revision id.</p> required Source code in <code>mirascope/cli/utils.py</code> <pre><code>def update_version_text_file(\n    version_file: str,\n    updates: dict[str, str],\n) -&gt; None:\n    \"\"\"Updates the version text file.\n\n    Depending on the contents of `updates`, updates the ids of the current and latest\n    revisions of the prompt.\n\n    Args:\n        version_file: The path to the version text file.\n        updates: A dictionary containing updates to the current revision id and/or\n            the latest revision id.\n    \"\"\"\n    modified_lines = []\n    edits_made = {\n        key: False for key in updates\n    }  # Track which keys already exist in the file\n    version_file_path: Path = Path(version_file)\n    if not version_file_path.is_file():\n        version_file_path.touch()\n    # Read the file and apply updates\n    with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            # Check if the current line contains any of the keys\n            for key, value in updates.items():\n                if line.startswith(key + \"=\"):\n                    modified_lines.append(f\"{key}={value}\\n\")\n                    edits_made[key] = True\n                    break\n            else:\n                # No key found, so keep the line as is\n                modified_lines.append(line)\n\n        # Add any keys that were not found at the end of the file\n        for key, value in updates.items():\n            if not edits_made[key]:\n                modified_lines.append(f\"{key}={value}\\n\")\n\n    # Write the modified content back to the file\n    with open(version_file_path, \"w\", encoding=\"utf-8\") as file:\n        file.writelines(modified_lines)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.write_prompt_to_template","title":"<code>write_prompt_to_template(file, command, variables=None)</code>","text":"<p>Writes the given prompt to the template.</p> <p>Deconstructs a prompt with ast and reconstructs it using the Jinja2 template, adding revision history into the prompt when the command is <code>MirascopeCommand.ADD</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the prompt.</p> required <code>command</code> <code>Literal[ADD, USE]</code> <p>The CLI command to execute.</p> required <code>variables</code> <code>Optional[MirascopeCliVariables]</code> <p>A dictionary of revision ids which are rendered together with variable assignments that are not inside any class. Only relevant when <code>command</code> is <code>MirascopeCommand.ADD</code> - if <code>command</code> is <code>MirascopeCommand.USE</code>, <code>variables</code> should be <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The reconstructed prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def write_prompt_to_template(\n    file: str,\n    command: Literal[MirascopeCommand.ADD, MirascopeCommand.USE],\n    variables: Optional[MirascopeCliVariables] = None,\n) -&gt; str:\n    \"\"\"Writes the given prompt to the template.\n\n    Deconstructs a prompt with ast and reconstructs it using the Jinja2 template, adding\n    revision history into the prompt when the command is `MirascopeCommand.ADD`.\n\n    Args:\n        file: The path to the prompt.\n        command: The CLI command to execute.\n        variables: A dictionary of revision ids which are rendered together with\n            variable assignments that are not inside any class. Only relevant when\n            `command` is `MirascopeCommand.ADD` - if `command` is\n            `MirascopeCommand.USE`, `variables` should be `None`.\n\n    Returns:\n        The reconstructed prompt.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    mirascope_directory = mirascope_settings.mirascope_location\n    auto_tag = mirascope_settings.auto_tag\n    template_loader = FileSystemLoader(searchpath=mirascope_directory)\n    template_env = Environment(loader=template_loader)\n    template = template_env.get_template(\"prompt_template.j2\")\n    analyzer = get_prompt_analyzer(file)\n    if variables is None:\n        variables = MirascopeCliVariables()\n\n    if command == MirascopeCommand.ADD:\n        # double quote revision ids to match how `ast.unparse()` formats strings\n        new_variables = {\n            k: f\"'{v}'\" if isinstance(v, str) else None\n            for k, v in variables.__dict__.items()\n        } | analyzer.variables\n    else:  # command == MirascopeCommand.USE\n        ignore_variable_keys = dict.fromkeys(ignore_variables, None)\n        new_variables = {\n            k: analyzer.variables[k]\n            for k in analyzer.variables\n            if k not in ignore_variable_keys\n        }\n\n    if auto_tag:\n        import_tag_name: Optional[str] = None\n        mirascope_alias = \"mirascope\"\n        for name, alias in analyzer.imports:\n            if name == \"mirascope\" and alias is not None:\n                mirascope_alias = alias\n                break\n        for module, name, alias in analyzer.from_imports:\n            if module == \"mirascope\" and name == \"tags\" and alias is not None:\n                mirascope_alias = alias\n                break\n\n        for python_class in analyzer.classes:\n            decorators = python_class.decorators\n            if python_class.bases and python_class.bases[0] in mirascope_prompt_bases:\n                import_tag_name = _update_tag_decorator_with_version(\n                    decorators, variables, mirascope_alias\n                )\n\n        if import_tag_name == \"tags\":\n            _update_mirascope_from_imports(import_tag_name, analyzer.from_imports)\n        elif import_tag_name == f\"{mirascope_alias}.tags\":\n            _update_mirascope_imports(analyzer.imports)\n\n    data = {\n        \"comments\": analyzer.comments,\n        \"variables\": new_variables,\n        \"imports\": analyzer.imports,\n        \"from_imports\": analyzer.from_imports,\n        \"classes\": analyzer.classes,\n    }\n    return template.render(**data)\n</code></pre>"},{"location":"api/gemini/","title":"gemini","text":"<p>A module for interacting with Google's Gemini models.</p>"},{"location":"api/gemini/calls/","title":"gemini.calls","text":"<p>A module for calling Google's Gemini Chat API.</p>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall","title":"<code>GeminiCall</code>","text":"<p>             Bases: <code>BaseCall[GeminiCallResponse, GeminiCallResponseChunk, GeminiTool]</code></p> <p>A class for prompting Google's Gemini Chat API.</p> <p>This prompt supports the message types: USER, MODEL, TOOL</p> <p>Example:</p> <pre><code>from google.generativeai import configure  # type: ignore\nfrom mirascope.gemini import GeminiPrompt\n\nconfigure(api_key=\"YOUR_API_KEY\")\n\n\nclass BookRecommender(GeminiPrompt):\n    prompt_template = \"\"\"\n    USER: You're the world's greatest librarian.\n    MODEL: Ok, I understand I'm the world's greatest librarian. How can I help?\n    USER: Please recommend some {genre} books.\n\n    genre: str\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.call())\n#&gt; As the world's greatest librarian, I am delighted to recommend...\n</code></pre> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>class GeminiCall(BaseCall[GeminiCallResponse, GeminiCallResponseChunk, GeminiTool]):\n    '''A class for prompting Google's Gemini Chat API.\n\n    This prompt supports the message types: USER, MODEL, TOOL\n\n    Example:\n\n    ```python\n    from google.generativeai import configure  # type: ignore\n    from mirascope.gemini import GeminiPrompt\n\n    configure(api_key=\"YOUR_API_KEY\")\n\n\n    class BookRecommender(GeminiPrompt):\n        prompt_template = \"\"\"\n        USER: You're the world's greatest librarian.\n        MODEL: Ok, I understand I'm the world's greatest librarian. How can I help?\n        USER: Please recommend some {genre} books.\n\n        genre: str\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.call())\n    #&gt; As the world's greatest librarian, I am delighted to recommend...\n    ```\n    '''\n\n    call_params: ClassVar[GeminiCallParams] = GeminiCallParams()\n\n    def messages(self) -&gt; ContentsType:\n        \"\"\"Returns the `ContentsType` messages for Gemini `generate_content`.\n\n        Raises:\n            ValueError: if the docstring contains an unknown role.\n        \"\"\"\n        return [\n            {\"role\": message[\"role\"], \"parts\": [message[\"content\"]]}\n            for message in self._parse_messages(\n                [MessageRole.MODEL, MessageRole.USER, MessageRole.TOOL]\n            )\n        ]\n\n    def call(self, **kwargs: Any) -&gt; GeminiCallResponse:\n        \"\"\"Makes an call to the model using this `GeminiCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments that will be used for generating the\n                response. These will override any existing argument settings in call\n                params.\n\n        Returns:\n            A `GeminiCallResponse` instance.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        start_time = datetime.datetime.now().timestamp() * 1000\n        response = gemini_pro_model.generate_content(\n            self.messages(),\n            stream=False,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        return GeminiCallResponse(\n            response=response,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; GeminiCallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `GeminiCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments that will be used for generating the\n                response. These will override any existing argument settings in call\n                params.\n\n        Returns:\n            A `GeminiCallResponse` instance.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        start_time = datetime.datetime.now().timestamp() * 1000\n        response = await gemini_pro_model.generate_content_async(\n            self.messages(),\n            stream=False,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        return GeminiCallResponse(\n            response=response,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    def stream(self, **kwargs: Any) -&gt; Generator[GeminiCallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `GeminiCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `GeminiCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        stream = gemini_pro_model.generate_content(\n            self.messages(),\n            stream=True,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        for chunk in stream:\n            yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[GeminiCallResponseChunk, None]:\n        \"\"\"Streams the response asynchronously for a call using this `GeminiCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `GeminiCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        stream = await gemini_pro_model.generate_content_async(\n            self.messages(),\n            stream=True,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes an call to the model using this <code>GeminiCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that will be used for generating the response. These will override any existing argument settings in call params.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeminiCallResponse</code> <p>A <code>GeminiCallResponse</code> instance.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; GeminiCallResponse:\n    \"\"\"Makes an call to the model using this `GeminiCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments that will be used for generating the\n            response. These will override any existing argument settings in call\n            params.\n\n    Returns:\n        A `GeminiCallResponse` instance.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    start_time = datetime.datetime.now().timestamp() * 1000\n    response = gemini_pro_model.generate_content(\n        self.messages(),\n        stream=False,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    return GeminiCallResponse(\n        response=response,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>GeminiCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that will be used for generating the response. These will override any existing argument settings in call params.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeminiCallResponse</code> <p>A <code>GeminiCallResponse</code> instance.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; GeminiCallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `GeminiCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments that will be used for generating the\n            response. These will override any existing argument settings in call\n            params.\n\n    Returns:\n        A `GeminiCallResponse` instance.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    start_time = datetime.datetime.now().timestamp() * 1000\n    response = await gemini_pro_model.generate_content_async(\n        self.messages(),\n        stream=False,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    return GeminiCallResponse(\n        response=response,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.messages","title":"<code>messages()</code>","text":"<p>Returns the <code>ContentsType</code> messages for Gemini <code>generate_content</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the docstring contains an unknown role.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>def messages(self) -&gt; ContentsType:\n    \"\"\"Returns the `ContentsType` messages for Gemini `generate_content`.\n\n    Raises:\n        ValueError: if the docstring contains an unknown role.\n    \"\"\"\n    return [\n        {\"role\": message[\"role\"], \"parts\": [message[\"content\"]]}\n        for message in self._parse_messages(\n            [MessageRole.MODEL, MessageRole.USER, MessageRole.TOOL]\n        )\n    ]\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>GeminiCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>GeminiCallResponseChunk</code> <p>A <code>GeminiCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>def stream(self, **kwargs: Any) -&gt; Generator[GeminiCallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `GeminiCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `GeminiCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    stream = gemini_pro_model.generate_content(\n        self.messages(),\n        stream=True,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    for chunk in stream:\n        yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response asynchronously for a call using this <code>GeminiCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[GeminiCallResponseChunk, None]</code> <p>A <code>GeminiCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[GeminiCallResponseChunk, None]:\n    \"\"\"Streams the response asynchronously for a call using this `GeminiCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `GeminiCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    stream = await gemini_pro_model.generate_content_async(\n        self.messages(),\n        stream=True,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/gemini/extractors/","title":"gemini.extractors","text":""},{"location":"api/gemini/extractors/#mirascope.gemini.extractors.GeminiExtractor","title":"<code>GeminiExtractor</code>","text":"<p>             Bases: <code>BaseExtractor[GeminiCall, GeminiTool, T]</code>, <code>Generic[T]</code></p> <p>A class for extracting structured information using Google's Gemini Chat models.</p> <p>Example:</p> <pre><code>from typing import Literal, Type\nfrom pydantic import BaseModel\nfrom mirascope.gemini import GeminiExtractor\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\nclass TaskExtractor(GeminiExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n\n    prompt_template = \"\"\"\n    USER: I need to extract task details.\n    MODEL: Sure, please provide the task description.\n    USER: {task}\n    \"\"\"\n\n    task: str\n\ntask_description = \"Prepare the budget report by next Monday. It's a high priority task.\"\ntask = TaskExtractor(task=task_description).extract(retries=3)\nassert isinstance(task, TaskDetails)\nprint(task)\n#&gt; title='Prepare the budget report' priority='high' due_date='next Monday'\n</code></pre> Source code in <code>mirascope/gemini/extractors.py</code> <pre><code>class GeminiExtractor(BaseExtractor[GeminiCall, GeminiTool, T], Generic[T]):\n    '''A class for extracting structured information using Google's Gemini Chat models.\n\n    Example:\n\n    ```python\n    from typing import Literal, Type\n    from pydantic import BaseModel\n    from mirascope.gemini import GeminiExtractor\n\n    class TaskDetails(BaseModel):\n        title: str\n        priority: Literal[\"low\", \"normal\", \"high\"]\n        due_date: str\n\n    class TaskExtractor(GeminiExtractor[TaskDetails]):\n        extract_schema: Type[TaskDetails] = TaskDetails\n\n        prompt_template = \"\"\"\n        USER: I need to extract task details.\n        MODEL: Sure, please provide the task description.\n        USER: {task}\n        \"\"\"\n\n        task: str\n\n    task_description = \"Prepare the budget report by next Monday. It's a high priority task.\"\n    task = TaskExtractor(task=task_description).extract(retries=3)\n    assert isinstance(task, TaskDetails)\n    print(task)\n    #&gt; title='Prepare the budget report' priority='high' due_date='next Monday'\n    ```\n    '''\n\n    call_params: ClassVar[GeminiCallParams] = GeminiCallParams()\n\n    def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Extracts `extract_schema` from the Gemini call response.\n\n        The `extract_schema` is converted into a `GeminiTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Gemini's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            GeminiError: raises any Gemini errors.\n        \"\"\"\n        return self._extract(GeminiCall, GeminiTool, retries, **kwargs)\n\n    async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Asynchronously extracts `extract_schema` from the Gemini call response.\n\n        The `extract_schema` is converted into a `GeminiTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Gemini's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            GeminiError: raises any Gemini errors.\n        \"\"\"\n        return await self._extract_async(GeminiCall, GeminiTool, retries, **kwargs)\n</code></pre>"},{"location":"api/gemini/extractors/#mirascope.gemini.extractors.GeminiExtractor.extract","title":"<code>extract(retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the Gemini call response.</p> <p>The <code>extract_schema</code> is converted into a <code>GeminiTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Gemini's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>GeminiError</code> <p>raises any Gemini errors.</p> Source code in <code>mirascope/gemini/extractors.py</code> <pre><code>def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Extracts `extract_schema` from the Gemini call response.\n\n    The `extract_schema` is converted into a `GeminiTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Gemini's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        GeminiError: raises any Gemini errors.\n    \"\"\"\n    return self._extract(GeminiCall, GeminiTool, retries, **kwargs)\n</code></pre>"},{"location":"api/gemini/extractors/#mirascope.gemini.extractors.GeminiExtractor.extract_async","title":"<code>extract_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously extracts <code>extract_schema</code> from the Gemini call response.</p> <p>The <code>extract_schema</code> is converted into a <code>GeminiTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Gemini's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>GeminiError</code> <p>raises any Gemini errors.</p> Source code in <code>mirascope/gemini/extractors.py</code> <pre><code>async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Asynchronously extracts `extract_schema` from the Gemini call response.\n\n    The `extract_schema` is converted into a `GeminiTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Gemini's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        GeminiError: raises any Gemini errors.\n    \"\"\"\n    return await self._extract_async(GeminiCall, GeminiTool, retries, **kwargs)\n</code></pre>"},{"location":"api/gemini/tools/","title":"gemini.tools","text":"<p>Classes for using tools with Google's Gemini API.</p>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool","title":"<code>GeminiTool</code>","text":"<p>             Bases: <code>BaseTool[FunctionCall]</code></p> <p>A base class for easy use of tools with the Gemini API.</p> <p><code>GeminiTool</code> internally handles the logic that allows you to use tools with simple calls such as <code>GeminiCompletion.tool</code> or <code>GeminiTool.fn</code>, as seen in the examples below.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall, GeminiCallParams, GeminiTool\n\n\nclass CurrentWeather(GeminiTool):\n    \"\"\"A tool for getting the current weather in a location.\"\"\"\n\n    location: str\n\n\nclass WeatherForecast(GeminiPrompt):\n    prompt_template = \"What is the current weather in {city}?\"\n\n    city: str\n\n    call_params = GeminiCallParams(\n        model=\"gemini-pro\",\n        tools=[CurrentWeather],\n    )\n\n\nprompt = WeatherPrompt()\nforecast = WeatherForecast(city=\"Tokyo\").call().tool\nprint(forecast.location)\n#&gt; Tokyo\n</code></pre> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>class GeminiTool(BaseTool[FunctionCall]):\n    '''A base class for easy use of tools with the Gemini API.\n\n    `GeminiTool` internally handles the logic that allows you to use tools with simple\n    calls such as `GeminiCompletion.tool` or `GeminiTool.fn`, as seen in the\n    examples below.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall, GeminiCallParams, GeminiTool\n\n\n    class CurrentWeather(GeminiTool):\n        \"\"\"A tool for getting the current weather in a location.\"\"\"\n\n        location: str\n\n\n    class WeatherForecast(GeminiPrompt):\n        prompt_template = \"What is the current weather in {city}?\"\n\n        city: str\n\n        call_params = GeminiCallParams(\n            model=\"gemini-pro\",\n            tools=[CurrentWeather],\n        )\n\n\n    prompt = WeatherPrompt()\n    forecast = WeatherForecast(city=\"Tokyo\").call().tool\n    print(forecast.location)\n    #&gt; Tokyo\n    ```\n    '''\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a tool schema for use with the Gemini API.\n\n        A Mirascope `GeminiTool` is deconstructed into a `Tool` schema for use with the\n        Gemini API.\n\n        Returns:\n            The constructed `Tool` schema.\n        \"\"\"\n        tool_schema = super().tool_schema()\n        if \"parameters\" in tool_schema:\n            if \"$defs\" in tool_schema[\"parameters\"]:\n                raise ValueError(\n                    \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                    \"with $defs.\"\n                )\n            tool_schema[\"parameters\"][\"properties\"] = {\n                prop: {\n                    key: value for key, value in prop_schema.items() if key != \"title\"\n                }\n                for prop, prop_schema in tool_schema[\"parameters\"][\"properties\"].items()\n            }\n        return Tool(function_declarations=[FunctionDeclaration(**tool_schema)])\n\n    @classmethod\n    def from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given a `GenerateContentResponse` from a Gemini chat completion response, this\n        method extracts the tool call and constructs an instance of the tool.\n\n        Args:\n            tool_call: The `GenerateContentResponse` from which to extract the tool.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValueError: if the tool call doesn't have any arguments.\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        if not tool_call.args:\n            raise ValueError(\"Tool call doesn't have any arguments.\")\n        model_json = {key: value for key, value in tool_call.args.items()}\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, GeminiTool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, GeminiTool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a function.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given a <code>GenerateContentResponse</code> from a Gemini chat completion response, this method extracts the tool call and constructs an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>FunctionCall</code> <p>The <code>GenerateContentResponse</code> from which to extract the tool.</p> required <p>Returns:</p> Type Description <code>GeminiTool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the tool call doesn't have any arguments.</p> <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given a `GenerateContentResponse` from a Gemini chat completion response, this\n    method extracts the tool call and constructs an instance of the tool.\n\n    Args:\n        tool_call: The `GenerateContentResponse` from which to extract the tool.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValueError: if the tool call doesn't have any arguments.\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json = {key: value for key, value in tool_call.args.items()}\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the Gemini API.</p> <p>A Mirascope <code>GeminiTool</code> is deconstructed into a <code>Tool</code> schema for use with the Gemini API.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>The constructed <code>Tool</code> schema.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a tool schema for use with the Gemini API.\n\n    A Mirascope `GeminiTool` is deconstructed into a `Tool` schema for use with the\n    Gemini API.\n\n    Returns:\n        The constructed `Tool` schema.\n    \"\"\"\n    tool_schema = super().tool_schema()\n    if \"parameters\" in tool_schema:\n        if \"$defs\" in tool_schema[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n        tool_schema[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value for key, value in prop_schema.items() if key != \"title\"\n            }\n            for prop, prop_schema in tool_schema[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**tool_schema)])\n</code></pre>"},{"location":"api/gemini/types/","title":"gemini.types","text":"<p>Types for interacting with Google's Gemini models using Mirascope.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallParams","title":"<code>GeminiCallParams</code>","text":"<p>             Bases: <code>BaseCallParams[GeminiTool]</code></p> <p>The parameters to use when calling the Gemini API calls.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall, GeminiCallParams\n\n\nclass BookRecommendation(GeminiPrompt):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n    call_params = GeminiCallParams(\n        model=\"gemini-1.0-pro-001\",\n        generation_config={\"candidate_count\": 2},\n    )\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Name of the Wind\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallParams(BaseCallParams[GeminiTool]):\n    \"\"\"The parameters to use when calling the Gemini API calls.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall, GeminiCallParams\n\n\n    class BookRecommendation(GeminiPrompt):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n        call_params = GeminiCallParams(\n            model=\"gemini-1.0-pro-001\",\n            generation_config={\"candidate_count\": 2},\n        )\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Name of the Wind\n    ```\n    \"\"\"\n\n    model: str = \"gemini-1.0-pro\"\n    generation_config: Optional[dict[str, Any]] = {\"candidate_count\": 1}\n    safety_settings: Optional[Any] = None\n    request_options: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse","title":"<code>GeminiCallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[GenerateContentResponse, GeminiTool]</code></p> <p>Convenience wrapper around Gemini's <code>GenerateContentResponse</code>.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.call()</code> will return a <code>GeminiCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiPrompt\n\n\nclass BookRecommender(GeminiPrompt):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Lord of the Rings\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallResponse(BaseCallResponse[GenerateContentResponse, GeminiTool]):\n    \"\"\"Convenience wrapper around Gemini's `GenerateContentResponse`.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.call()` will return a\n    `GeminiCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiPrompt\n\n\n    class BookRecommender(GeminiPrompt):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Lord of the Rings\n    ```\n    \"\"\"\n\n    @property\n    def tools(self) -&gt; Optional[list[GeminiTool]]:\n        \"\"\"Returns the list of tools for the 0th candidate's 0th content part.\"\"\"\n        if self.tool_types is None:\n            return None\n\n        tool_calls = [\n            part.function_call for part in self.response.candidates[0].content.parts\n        ]\n\n        extracted_tools = []\n        for tool_call in tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[GeminiTool]:\n        \"\"\"Returns the 0th tool for the 0th candidate's 0th content part.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if self.tool_types is None:\n            return None\n\n        tool_call = self.response.candidates[0].content.parts[0].function_call\n        for tool_type in self.tool_types:\n            if tool_call.name == tool_type.__name__:\n                return tool_type.from_tool_call(tool_call)\n\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.response.candidates[0].content.parts[0].text\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": str(self.response),\n        }\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the contained string content for the 0th choice.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.tool","title":"<code>tool: Optional[GeminiTool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.tools","title":"<code>tools: Optional[list[GeminiTool]]</code>  <code>property</code>","text":"<p>Returns the list of tools for the 0th candidate's 0th content part.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/gemini/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": str(self.response),\n    }\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponseChunk","title":"<code>GeminiCallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[GenerateContentResponse, GeminiTool]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.stream()</code> will return a <code>GeminiCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall\n\n\nclass BookRecommender(GeminiCall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n\nfor chunk in BookRecommender(genre=\"science fiction\").stream():\n    print(chunk)\n\n#&gt; D\n#  u\n#\n#  ne\n#\n#  by F\n#  r\n#  an\n#  k\n#  .\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallResponseChunk(\n    BaseCallResponseChunk[GenerateContentResponse, GeminiTool]\n):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.stream()` will return a\n    `GeminiCallResponseChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall\n\n\n    class BookRecommender(GeminiCall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n\n    for chunk in BookRecommender(genre=\"science fiction\").stream():\n        print(chunk)\n\n    #&gt; D\n    #  u\n    #\n    #  ne\n    #\n    #  by F\n    #  r\n    #  an\n    #  k\n    #  .\n    ```\n    \"\"\"\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.chunk.candidates[0].content.parts[0].text\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the chunk content for the 0th choice.</p>"},{"location":"api/openai/","title":"openai","text":"<p>A module for interacting with OpenAI models.</p>"},{"location":"api/openai/calls/","title":"openai.calls","text":"<p>A module for calling OpenAI's Chat Completion models.</p>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall","title":"<code>OpenAICall</code>","text":"<p>             Bases: <code>BaseCall[OpenAICallResponse, OpenAICallResponseChunk, OpenAITool]</code></p> <p>A base class for calling OpenAI's Chat Completion models.</p> <p>Example:</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; There are many great books to read, it ultimately depends...\n</code></pre> Source code in <code>mirascope/openai/calls.py</code> <pre><code>class OpenAICall(BaseCall[OpenAICallResponse, OpenAICallResponseChunk, OpenAITool]):\n    \"\"\"A base class for calling OpenAI's Chat Completion models.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall\n\n\n    class BookRecommender(OpenAICall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; There are many great books to read, it ultimately depends...\n    ```\n    \"\"\"\n\n    call_params: ClassVar[OpenAICallParams] = OpenAICallParams()\n\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        message_type_by_role = {\n            MessageRole.SYSTEM: ChatCompletionSystemMessageParam,\n            MessageRole.USER: ChatCompletionUserMessageParam,\n            MessageRole.ASSISTANT: ChatCompletionAssistantMessageParam,\n            MessageRole.TOOL: ChatCompletionToolMessageParam,\n        }\n        return [\n            message_type_by_role[MessageRole(message[\"role\"])](\n                role=message[\"role\"], content=message[\"content\"]\n            )\n            for message in self._parse_messages(list(message_type_by_role.keys()))\n        ]\n\n    def call(self, **kwargs: Any) -&gt; OpenAICallResponse:\n        \"\"\"Makes a call to the model using this `OpenAICall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `OpenAICallResponse` instance.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, OpenAITool)\n        client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = client.chat.completions.create(\n            messages=self.messages(),\n            stream=False,\n            **kwargs,\n        )\n        return OpenAICallResponse(\n            response=completion,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; OpenAICallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `OpenAICall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            An `OpenAICallResponse` instance.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, OpenAITool)\n        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = await client.chat.completions.create(\n            messages=self.messages(),\n            stream=False,\n            **kwargs,\n        )\n        return OpenAICallResponse(\n            response=completion,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    def stream(self, **kwargs: Any) -&gt; Generator[OpenAICallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `OpenAICall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `OpenAICallResponseChunk` for each chunk of the response.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, OpenAITool)\n        client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        stream = client.chat.completions.create(\n            messages=self.messages(),\n            stream=True,\n            **kwargs,\n        )\n        for chunk in stream:\n            yield OpenAICallResponseChunk(chunk=chunk, tool_types=tool_types)\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[OpenAICallResponseChunk, None]:\n        \"\"\"Streams the response for an asynchronous call using this `OpenAICall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `OpenAICallResponseChunk` for each chunk of the response.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, OpenAITool)\n        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        stream = await client.chat.completions.create(\n            messages=self.messages(),\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield OpenAICallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>OpenAICall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAICallResponse</code> <p>A <code>OpenAICallResponse</code> instance.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; OpenAICallResponse:\n    \"\"\"Makes a call to the model using this `OpenAICall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `OpenAICallResponse` instance.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, OpenAITool)\n    client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    start_time = datetime.datetime.now().timestamp() * 1000\n    completion = client.chat.completions.create(\n        messages=self.messages(),\n        stream=False,\n        **kwargs,\n    )\n    return OpenAICallResponse(\n        response=completion,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>OpenAICall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAICallResponse</code> <p>An <code>OpenAICallResponse</code> instance.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; OpenAICallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `OpenAICall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        An `OpenAICallResponse` instance.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, OpenAITool)\n    client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    start_time = datetime.datetime.now().timestamp() * 1000\n    completion = await client.chat.completions.create(\n        messages=self.messages(),\n        stream=False,\n        **kwargs,\n    )\n    return OpenAICallResponse(\n        response=completion,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>def messages(self) -&gt; list[ChatCompletionMessageParam]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    message_type_by_role = {\n        MessageRole.SYSTEM: ChatCompletionSystemMessageParam,\n        MessageRole.USER: ChatCompletionUserMessageParam,\n        MessageRole.ASSISTANT: ChatCompletionAssistantMessageParam,\n        MessageRole.TOOL: ChatCompletionToolMessageParam,\n    }\n    return [\n        message_type_by_role[MessageRole(message[\"role\"])](\n            role=message[\"role\"], content=message[\"content\"]\n        )\n        for message in self._parse_messages(list(message_type_by_role.keys()))\n    ]\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>OpenAICall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>OpenAICallResponseChunk</code> <p>A <code>OpenAICallResponseChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>def stream(self, **kwargs: Any) -&gt; Generator[OpenAICallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `OpenAICall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `OpenAICallResponseChunk` for each chunk of the response.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, OpenAITool)\n    client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    stream = client.chat.completions.create(\n        messages=self.messages(),\n        stream=True,\n        **kwargs,\n    )\n    for chunk in stream:\n        yield OpenAICallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response for an asynchronous call using this <code>OpenAICall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[OpenAICallResponseChunk, None]</code> <p>A <code>OpenAICallResponseChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[OpenAICallResponseChunk, None]:\n    \"\"\"Streams the response for an asynchronous call using this `OpenAICall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `OpenAICallResponseChunk` for each chunk of the response.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, OpenAITool)\n    client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    stream = await client.chat.completions.create(\n        messages=self.messages(),\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield OpenAICallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/openai/extractors/","title":"openai.extractors","text":"<p>A class for extracting structured information using OpenAI chat models.</p>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor","title":"<code>OpenAIExtractor</code>","text":"<p>             Bases: <code>BaseExtractor[OpenAICall, OpenAITool, T]</code>, <code>Generic[T]</code></p> <p>A class for extracting structured information using OpenAI chat models.</p> <p>Example:</p> <pre><code>from typing import Literal, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n\n    prompt_template = \"\"\"\n    Please extract the task details:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask_description = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask = TaskExtractor(task=task_description).extract(retries=3)\nassert isinstance(task, TaskDetails)\nprint(task)\n#&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n</code></pre> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>class OpenAIExtractor(BaseExtractor[OpenAICall, OpenAITool, T], Generic[T]):\n    '''A class for extracting structured information using OpenAI chat models.\n\n    Example:\n\n    ```python\n    from typing import Literal, Type\n\n    from mirascope.openai import OpenAIExtractor\n    from pydantic import BaseModel\n\n\n    class TaskDetails(BaseModel):\n        title: str\n        priority: Literal[\"low\", \"normal\", \"high\"]\n        due_date: str\n\n\n    class TaskExtractor(OpenAIExtractor[TaskDetails]):\n        extract_schema: Type[TaskDetails] = TaskDetails\n\n        prompt_template = \"\"\"\n        Please extract the task details:\n        {task}\n        \"\"\"\n\n        task: str\n\n\n    task_description = \"Submit quarterly report by next Friday. Task is high priority.\"\n    task = TaskExtractor(task=task_description).extract(retries=3)\n    assert isinstance(task, TaskDetails)\n    print(task)\n    #&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n    ```\n    '''\n\n    call_params: ClassVar[OpenAICallParams] = OpenAICallParams()\n\n    def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Extracts `extract_schema` from the OpenAI call response.\n\n        The `extract_schema` is converted into an `OpenAITool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of OpenAI's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        return self._extract(OpenAICall, OpenAITool, retries, **kwargs)\n\n    async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Asynchronously extracts `extract_schema` from the OpenAI call response.\n\n        The `extract_schema` is converted into an `OpenAITool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of OpenAI's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        return await self._extract_async(OpenAICall, OpenAITool, retries, **kwargs)\n</code></pre>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor.extract","title":"<code>extract(retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the OpenAI call response.</p> <p>The <code>extract_schema</code> is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Extracts `extract_schema` from the OpenAI call response.\n\n    The `extract_schema` is converted into an `OpenAITool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of OpenAI's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    return self._extract(OpenAICall, OpenAITool, retries, **kwargs)\n</code></pre>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor.extract_async","title":"<code>extract_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously extracts <code>extract_schema</code> from the OpenAI call response.</p> <p>The <code>extract_schema</code> is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Asynchronously extracts `extract_schema` from the OpenAI call response.\n\n    The `extract_schema` is converted into an `OpenAITool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of OpenAI's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    return await self._extract_async(OpenAICall, OpenAITool, retries, **kwargs)\n</code></pre>"},{"location":"api/openai/tools/","title":"openai.tools","text":"<p>Classes for using tools with OpenAI Chat APIs.</p>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>             Bases: <code>BaseTool[ChatCompletionMessageToolCall]</code></p> <p>A base class for easy use of tools with the OpenAI Chat client.</p> <p><code>OpenAITool</code> internally handles the logic that allows you to use tools with simple calls such as <code>OpenAICallResponse.tool</code> or <code>OpenAITool.fn</code>, as seen in the examples below.</p> <p>Example:</p> <pre><code>from mirascope import OpenAICall\n\n\ndef animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n    \"\"\"Tells you your most likely favorite animal from personality traits.\n\n    Args:\n        fav_food: your favorite food.\n        fav_color: your favorite color.\n\n    Returns:\n        The animal most likely to be your favorite based on traits.\n    \"\"\"\n    return \"Your favorite animal is the best one, a frog.\"\n\n\nclass AnimalMatcher(OpenAICall):\n    prompt_template = \"\"\"\n    Tell me my favorite animal if my favorite food is {food} and my\n    favorite color is {color}.\n    \"\"\"\n\n    food: str\n    color: str\n\n    call_params = OpenAICallParams(tools=[animal_matcher])\n\n\nresponse = AnimalMatcher(food=\"pizza\", color=\"red\").call\ntool = response.tool\nprint(tool.fn(**tool.args))\n#&gt; Your favorite animal is the best one, a frog.\n</code></pre> Source code in <code>mirascope/openai/tools.py</code> <pre><code>class OpenAITool(BaseTool[ChatCompletionMessageToolCall]):\n    '''A base class for easy use of tools with the OpenAI Chat client.\n\n    `OpenAITool` internally handles the logic that allows you to use tools with simple\n    calls such as `OpenAICallResponse.tool` or `OpenAITool.fn`, as seen in the\n    examples below.\n\n    Example:\n\n    ```python\n    from mirascope import OpenAICall\n\n\n    def animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n        \"\"\"Tells you your most likely favorite animal from personality traits.\n\n        Args:\n            fav_food: your favorite food.\n            fav_color: your favorite color.\n\n        Returns:\n            The animal most likely to be your favorite based on traits.\n        \"\"\"\n        return \"Your favorite animal is the best one, a frog.\"\n\n\n    class AnimalMatcher(OpenAICall):\n        prompt_template = \"\"\"\n        Tell me my favorite animal if my favorite food is {food} and my\n        favorite color is {color}.\n        \"\"\"\n\n        food: str\n        color: str\n\n        call_params = OpenAICallParams(tools=[animal_matcher])\n\n\n    response = AnimalMatcher(food=\"pizza\", color=\"red\").call\n    tool = response.tool\n    print(tool.fn(**tool.args))\n    #&gt; Your favorite animal is the best one, a frog.\n    ```\n    '''\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n        A Mirascope `OpenAITool` is deconstructed into a JSON schema, and relevant keys\n        are renamed to match the OpenAI `ChatCompletionToolParam` schema used to make\n        function/tool calls in OpenAI API.\n\n        Returns:\n            The constructed `ChatCompletionToolParam` schema.\n        \"\"\"\n        fn = super().tool_schema()\n        return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given `ChatCompletionMessageToolCall` from an OpenAI chat completion response,\n        takes its function arguments and creates an `OpenAITool` instance from it.\n\n        Args:\n            tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        try:\n            model_json = json.loads(tool_call.function.arguments)\n        except json.JSONDecodeError as e:\n            raise ValueError() from e\n\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, OpenAITool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, OpenAITool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a function.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given <code>ChatCompletionMessageToolCall</code> from an OpenAI chat completion response, takes its function arguments and creates an <code>OpenAITool</code> instance from it.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The <code>ChatCompletionMessageToolCall</code> to extract the tool from.</p> required <p>Returns:</p> Type Description <code>OpenAITool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given `ChatCompletionMessageToolCall` from an OpenAI chat completion response,\n    takes its function arguments and creates an `OpenAITool` instance from it.\n\n    Args:\n        tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    try:\n        model_json = json.loads(tool_call.function.arguments)\n    except json.JSONDecodeError as e:\n        raise ValueError() from e\n\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the OpenAI Chat client.</p> <p>A Mirascope <code>OpenAITool</code> is deconstructed into a JSON schema, and relevant keys are renamed to match the OpenAI <code>ChatCompletionToolParam</code> schema used to make function/tool calls in OpenAI API.</p> <p>Returns:</p> Type Description <code>ChatCompletionToolParam</code> <p>The constructed <code>ChatCompletionToolParam</code> schema.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n    A Mirascope `OpenAITool` is deconstructed into a JSON schema, and relevant keys\n    are renamed to match the OpenAI `ChatCompletionToolParam` schema used to make\n    function/tool calls in OpenAI API.\n\n    Returns:\n        The constructed `ChatCompletionToolParam` schema.\n    \"\"\"\n    fn = super().tool_schema()\n    return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n</code></pre>"},{"location":"api/openai/types/","title":"openai.types","text":"<p>Types for interacting with OpenAI models using Mirascope.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>             Bases: <code>BaseCallParams[OpenAITool]</code></p> <p>The parameters to use when calling the OpenAI API.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallParams(BaseCallParams[OpenAITool]):\n    \"\"\"The parameters to use when calling the OpenAI API.\"\"\"\n\n    model: str = \"gpt-3.5-turbo-0125\"\n    frequency_penalty: Optional[float] = None\n    logit_bias: Optional[dict[str, int]] = None\n    logprobs: Optional[bool] = None\n    max_tokens: Optional[int] = None\n    n: Optional[int] = None\n    presence_penalty: Optional[float] = None\n    response_format: Optional[ResponseFormat] = None\n    seed: Optional[int] = None\n    stop: Union[Optional[str], list[str]] = None\n    temperature: Optional[float] = None\n    tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None\n    top_logprobs: Optional[int] = None\n    top_p: Optional[float] = None\n    user: Optional[str] = None\n    # Values defined below take precedence over values defined elsewhere. Use these\n    # params to pass additional parameters to the API if necessary that aren't already\n    # available as params.\n    extra_headers: Optional[Headers] = None\n    extra_query: Optional[Query] = None\n    extra_body: Optional[Body] = None\n    timeout: Optional[Union[float, Timeout]] = None\n\n    wrapper: Optional[Callable[[OpenAI], OpenAI]] = None\n    wrapper_async: Optional[Callable[[AsyncOpenAI], AsyncOpenAI]] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def kwargs(\n        self,\n        tool_type: Optional[Type[OpenAITool]] = OpenAITool,\n        exclude: Optional[set[str]] = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the keyword argument call parameters.\"\"\"\n        extra_exclude = {\"wrapper\", \"wrapper_async\"}\n        exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n        return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallParams.kwargs","title":"<code>kwargs(tool_type=OpenAITool, exclude=None)</code>","text":"<p>Returns the keyword argument call parameters.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def kwargs(\n    self,\n    tool_type: Optional[Type[OpenAITool]] = OpenAITool,\n    exclude: Optional[set[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the keyword argument call parameters.\"\"\"\n    extra_exclude = {\"wrapper\", \"wrapper_async\"}\n    exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n    return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse","title":"<code>OpenAICallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[ChatCompletion, OpenAITool]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletion</code> response.</p> <p>When using Mirascope's convenience wrappers to interact with OpenAI models via <code>OpenAICall</code>, responses using <code>OpenAICall.call()</code> will return a <code>OpenAICallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n\nresponse = Bookrecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Name of the Wind\n\nprint(response.message)\n#&gt; ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n#  function_call=None, tool_calls=None)\n\nprint(response.choices)\n#&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n#  message=ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n#  function_call=None, tool_calls=None))]\n</code></pre> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallResponse(BaseCallResponse[ChatCompletion, OpenAITool]):\n    \"\"\"A convenience wrapper around the OpenAI `ChatCompletion` response.\n\n    When using Mirascope's convenience wrappers to interact with OpenAI models via\n    `OpenAICall`, responses using `OpenAICall.call()` will return a\n    `OpenAICallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall\n\n\n    class BookRecommender(OpenAICall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n\n    response = Bookrecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Name of the Wind\n\n    print(response.message)\n    #&gt; ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n    #  function_call=None, tool_calls=None)\n\n    print(response.choices)\n    #&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n    #  message=ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n    #  function_call=None, tool_calls=None))]\n    ```\n    \"\"\"\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChatCompletionMessageToolCall]]:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @property\n    def tools(self) -&gt; Optional[list[OpenAITool]]:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[OpenAITool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls or len(self.tool_calls) == 0:\n            return None\n\n        tool_call = self.tool_calls[0]\n        for tool_type in self.tool_types:\n            if self.tool_calls[0].function.name == tool_type.__name__:\n                return tool_type.from_tool_call(tool_call)\n\n        return None\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": self.response.model_dump(),\n        }\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.tool","title":"<code>tool: Optional[OpenAITool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.tool_calls","title":"<code>tool_calls: Optional[list[ChatCompletionMessageToolCall]]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.tools","title":"<code>tools: Optional[list[OpenAITool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": self.response.model_dump(),\n    }\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk","title":"<code>OpenAICallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[ChatCompletionChunk, OpenAITool]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with OpenAI models via <code>OpenAICall.stream</code>, responses will return an <code>OpenAICallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <p>```python from mirascope.openai import OpenAICall</p> <p>class Math(OpenAICall):     prompt_template = \"What is 1 + 2?\"</p> <p>for chunk in OpenAICall().stream():     print(chunk.content)</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--1","title":"&gt; 1","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--_1","title":"+","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--2","title":"2","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--equals","title":"equals","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--_2","title":"types","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--3","title":"3","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--_3","title":".","text":"Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallResponseChunk(BaseCallResponseChunk[ChatCompletionChunk, OpenAITool]):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with OpenAI models via\n    `OpenAICall.stream`, responses will return an `OpenAICallResponseChunk`, whereby\n    the implemented properties allow for simpler syntax and a convenient developer\n    experience.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall\n\n\n    class Math(OpenAICall):\n        prompt_template = \"What is 1 + 2?\"\n\n\n    for chunk in OpenAICall().stream():\n        print(chunk.content)\n\n    #&gt; 1\n    #  +\n    #  2\n    #   equals\n    #\n    #  3\n    #  .\n    \"\"\"\n\n    @property\n    def choices(self) -&gt; list[ChunkChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChunkChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        return self.choices[0].delta\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return self.delta.content if self.delta.content is not None else \"\"\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChoiceDeltaToolCall]]:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        return self.delta.tool_calls\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.choice","title":"<code>choice: ChunkChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.choices","title":"<code>choices: list[ChunkChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.delta","title":"<code>delta: ChoiceDelta</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.tool_calls","title":"<code>tool_calls: Optional[list[ChoiceDeltaToolCall]]</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/openai/wandb/","title":"openai.wandb","text":"<p>Prompts with WandB and OpenAI integration to support logging functionality.</p>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.WandbBasePrompt","title":"<code>WandbBasePrompt</code>","text":"<p>             Bases: <code>BasePrompt</code></p> <p>An extension of <code>BasePrompt</code> with Weights &amp; Biases specific functionality.</p> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>class WandbBasePrompt(BasePrompt):\n    \"\"\"An extension of `BasePrompt` with Weights &amp; Biases specific functionality.\"\"\"\n\n    span_type: Literal[\"tool\", \"llm\", \"chain\", \"agent\"]\n</code></pre>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.WandbOpenAICall","title":"<code>WandbOpenAICall</code>","text":"<p>             Bases: <code>OpenAICall</code>, <code>WandbBasePrompt</code></p> <p>A <code>OpenAICall</code> with added convenience for integrating with Weights &amp; Biases.</p> <p>Use this class's built in <code>call_with_trace</code> method to log traces to WandB along with your calls to OpenAI. These calls will include all of the additional metadata information such as the prompt template, template variables, and more.</p> <p>Example:</p> <pre><code>import os\n\nfrom mirascope.wandb.openai import WandbOpenAICall\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass BookRecommender(WandbOpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nresponse, span = BookRecommender(span_type=\"llm\", genre=\"fantasy\").call_with_trace()\n#           ^ this is a `Span` returned from the trace (or trace error).\n</code></pre> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>class WandbOpenAICall(OpenAICall, WandbBasePrompt):\n    '''A `OpenAICall` with added convenience for integrating with Weights &amp; Biases.\n\n    Use this class's built in `call_with_trace` method to log traces to WandB along with\n    your calls to OpenAI. These calls will include all of the additional metadata\n    information such as the prompt template, template variables, and more.\n\n    Example:\n\n    ```python\n    import os\n\n    from mirascope.wandb.openai import WandbOpenAICall\n    import wandb\n\n    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n    wandb.init(project=\"wandb_logged_chain\")\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n    class BookRecommender(WandbOpenAICall):\n        prompt_template = \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        Please recommend a {genre} book.\n        \"\"\"\n\n        genre: str\n\n\n    response, span = BookRecommender(span_type=\"llm\", genre=\"fantasy\").call_with_trace()\n    #           ^ this is a `Span` returned from the trace (or trace error).\n    ```\n    '''\n\n    def call_with_trace(\n        self,\n        parent: Optional[Trace] = None,\n        **kwargs: Any,\n    ) -&gt; tuple[Optional[OpenAICallResponse], Trace]:\n        \"\"\"Creates an OpenAI chat completion and logs it via a W&amp;B `Trace`.\n\n        Args:\n            parent: The parent trace to connect to.\n\n        Returns:\n            A tuple containing the completion and its trace (which has been connected\n                to the parent).\n        \"\"\"\n        try:\n            start_time = datetime.datetime.now().timestamp() * 1000\n            response = super().call()\n            span = trace(self, response, parent, **kwargs)\n            return response, span\n        except Exception as e:\n            return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.WandbOpenAICall.call_with_trace","title":"<code>call_with_trace(parent=None, **kwargs)</code>","text":"<p>Creates an OpenAI chat completion and logs it via a W&amp;B <code>Trace</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Optional[OpenAICallResponse], Trace]</code> <p>A tuple containing the completion and its trace (which has been connected to the parent).</p> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>def call_with_trace(\n    self,\n    parent: Optional[Trace] = None,\n    **kwargs: Any,\n) -&gt; tuple[Optional[OpenAICallResponse], Trace]:\n    \"\"\"Creates an OpenAI chat completion and logs it via a W&amp;B `Trace`.\n\n    Args:\n        parent: The parent trace to connect to.\n\n    Returns:\n        A tuple containing the completion and its trace (which has been connected\n            to the parent).\n    \"\"\"\n    try:\n        start_time = datetime.datetime.now().timestamp() * 1000\n        response = super().call()\n        span = trace(self, response, parent, **kwargs)\n        return response, span\n    except Exception as e:\n        return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.WandbOpenAIExtractor","title":"<code>WandbOpenAIExtractor</code>","text":"<p>             Bases: <code>OpenAIExtractor[T]</code>, <code>WandbBasePrompt</code>, <code>Generic[T]</code></p> <p>A <code>OpenAIExtractor</code> with added convenience for integrating with Weights &amp; Biases.</p> <p>Use this class's built in <code>extract_with_trace</code> method to log traces to WandB along with your calls to OpenAI. These calls will include all of the additional metadata information such as the prompt template, template variables, and more.</p> <p>Example:</p> <pre><code>import os\nfrom typing import Type\n\nfrom mirascope.wandb.openai import WandbOpenAIExtractor\nfrom pydantic import BaseModel\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(WandbOpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nbook, span = BookRecommender(span_type=\"tool\", genre=\"fantasy\").extract_with_trace()\n#           ^ this is a `Span` returned from the trace (or trace error).\n</code></pre> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>class WandbOpenAIExtractor(OpenAIExtractor[T], WandbBasePrompt, Generic[T]):\n    '''A `OpenAIExtractor` with added convenience for integrating with Weights &amp; Biases.\n\n    Use this class's built in `extract_with_trace` method to log traces to WandB along\n    with your calls to OpenAI. These calls will include all of the additional metadata\n    information such as the prompt template, template variables, and more.\n\n    Example:\n\n    ```python\n    import os\n    from typing import Type\n\n    from mirascope.wandb.openai import WandbOpenAIExtractor\n    from pydantic import BaseModel\n    import wandb\n\n    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n    wandb.init(project=\"wandb_logged_chain\")\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n    class Book(BaseModel):\n        title: str\n        author: str\n\n\n    class BookRecommender(WandbOpenAIExtractor[Book]):\n        extract_schema: Type[Book] = Book\n        prompt_template = \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        Please recommend a {genre} book.\n        \"\"\"\n\n        genre: str\n\n\n    book, span = BookRecommender(span_type=\"tool\", genre=\"fantasy\").extract_with_trace()\n    #           ^ this is a `Span` returned from the trace (or trace error).\n    ```\n    '''\n\n    def extract_with_trace(\n        self,\n        parent: Optional[Trace] = None,\n        retries: int = 0,\n        **kwargs: Any,\n    ) -&gt; tuple[Optional[T], Trace]:\n        \"\"\"Extracts `extract_schema` from the OpenAI call response and traces it.\n\n        The `extract_schema` is converted into an `OpenAITool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of OpenAI's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        try:\n            start_time = datetime.datetime.now().timestamp() * 1000\n            model = super().extract(retries=retries, **kwargs)\n            span = trace(self, model._response, parent, **kwargs)  # type: ignore\n            return model, span\n        except Exception as e:\n            return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.WandbOpenAIExtractor.extract_with_trace","title":"<code>extract_with_trace(parent=None, retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the OpenAI call response and traces it.</p> <p>The <code>extract_schema</code> is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Optional[T], Trace]</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>def extract_with_trace(\n    self,\n    parent: Optional[Trace] = None,\n    retries: int = 0,\n    **kwargs: Any,\n) -&gt; tuple[Optional[T], Trace]:\n    \"\"\"Extracts `extract_schema` from the OpenAI call response and traces it.\n\n    The `extract_schema` is converted into an `OpenAITool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of OpenAI's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    try:\n        start_time = datetime.datetime.now().timestamp() * 1000\n        model = super().extract(retries=retries, **kwargs)\n        span = trace(self, model._response, parent, **kwargs)  # type: ignore\n        return model, span\n    except Exception as e:\n        return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.trace","title":"<code>trace(call, response, parent, **kwargs)</code>","text":"<p>Returns a trace connected to parent.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>OpenAICallResponse</code> <p>The completion to trace. Handles <code>OpenAIChatCompletion</code> output from both standard OpenAI chat completions, and <code>BaseModel</code> for extractions.</p> required <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> required <p>Returns:</p> Type Description <code>Trace</code> <p>The created trace, connected to the parent.</p> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>def trace(\n    call: Union[WandbOpenAICall, WandbOpenAIExtractor],\n    response: OpenAICallResponse,\n    parent: Optional[Trace],\n    **kwargs: Any,\n) -&gt; Trace:\n    \"\"\"Returns a trace connected to parent.\n\n    Args:\n        response: The completion to trace. Handles `OpenAIChatCompletion` output\n            from both standard OpenAI chat completions, and `BaseModel` for\n            extractions.\n        parent: The parent trace to connect to.\n\n    Returns:\n        The created trace, connected to the parent.\n    \"\"\"\n    tool = response.tool\n    if tool is not None:\n        outputs = {\n            \"assistant\": tool.model_dump(),\n            \"tool_output\": tool.fn(**tool.args),\n        }\n    else:\n        outputs = {\"assistant\": response.content}\n\n    metadata = {\n        \"call_params\": call.call_params.model_copy(update=kwargs).kwargs(OpenAITool)\n    }\n    if response.response.usage is not None:\n        metadata[\"usage\"] = response.response.usage.model_dump()\n    span = Trace(\n        name=call.__class__.__name__,\n        kind=call.span_type,\n        status_code=\"success\",\n        status_message=None,\n        metadata=metadata,\n        start_time_ms=round(response.start_time),\n        end_time_ms=round(response.end_time),\n        inputs={message[\"role\"]: message[\"content\"] for message in call.messages()},\n        outputs=outputs,\n    )\n    if parent:\n        parent.add_child(span)\n    return span\n</code></pre>"},{"location":"api/openai/wandb/#mirascope.openai.wandb.trace_error","title":"<code>trace_error(call, error, parent, start_time, **kwargs)</code>","text":"<p>Returns an error trace connected to parent.</p> <p>Start time is set to time of prompt creation, and end time is set to the time function is called.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The error to trace.</p> required <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> required <code>start_time</code> <code>float</code> <p>The time the call to OpenAI was started.</p> required <p>Returns:</p> Type Description <code>Trace</code> <p>The created error trace, connected to the parent.</p> Source code in <code>mirascope/openai/wandb.py</code> <pre><code>def trace_error(\n    call: Union[WandbOpenAICall, WandbOpenAIExtractor],\n    error: Exception,\n    parent: Optional[Trace],\n    start_time: float,\n    **kwargs: Any,\n) -&gt; Trace:\n    \"\"\"Returns an error trace connected to parent.\n\n    Start time is set to time of prompt creation, and end time is set to the time\n    function is called.\n\n    Args:\n        error: The error to trace.\n        parent: The parent trace to connect to.\n        start_time: The time the call to OpenAI was started.\n\n    Returns:\n        The created error trace, connected to the parent.\n    \"\"\"\n    span = Trace(\n        name=call.__class__.__name__,\n        kind=call.span_type,\n        status_code=\"error\",\n        status_message=str(error),\n        metadata={\"call_params\": call.call_params.model_copy(update=kwargs).kwargs()},\n        start_time_ms=round(start_time),\n        end_time_ms=round(datetime.datetime.now().timestamp() * 1000),\n        inputs={message[\"role\"]: message[\"content\"] for message in call.messages()},\n        outputs=None,\n    )\n    if parent:\n        parent.add_child(span)\n    return span\n</code></pre>"},{"location":"concepts/attaching_and_calling_tool_functions/","title":"Attaching tool functions to Mirascope Calls","text":""},{"location":"concepts/attaching_and_calling_tool_functions/#using-mirascope-openai-tool","title":"Using Mirascope OpenAI Tool","text":"<p>Create your call and pass in your <code>OpenAITool</code>:</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\n\nfrom mirascope.base import tool_fn\nfrom mirascope.openai import OpenAICall, OpenAITool\n\n@tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nclass TodaysForecast(OpenAICall):\n    prompt_template = \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\", tools=[GetCurrentWeather]\n    )\n</code></pre> <p>The tools are attached to the <code>call_params</code> attribute in a Mirascope Call. For more information check out Learn why colocation is so important and how combining it with the Mirascope CLI makes engineering better prompts and calls easy.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#using-a-function-properly-documented-with-a-docstring","title":"Using a function properly documented with a docstring","text":"<p>Create your call and pass in your function:</p> <pre><code>import json\n\nfrom typing import Literal\n\nfrom mirascope.openai import OpenAICall\n\n\ndef get_current_weather(\n        location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON object containing the location, temperature, and unit.\n    \"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\nclass TodaysForecast(OpenAICall):\n    prompt_template = \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\", tools=[get_current_weather]\n    )\n</code></pre>"},{"location":"concepts/attaching_and_calling_tool_functions/#calling-tools","title":"Calling Tools","text":"<p>Generate content by calling the <code>call</code> method:</p> <pre><code># using same code as above\nforecast = TodaysForecast()\nresponse = forecast.call()\nif tools := response.tools:\n    for tool in tools:\n        print(tool.fn(**tool.args))\n\n#&gt; {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n</code></pre> <p>The\u00a0<code>response.tools</code>\u00a0property returns an actual instance of the tool.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#async","title":"Async","text":"<p>All of the examples above also work with\u00a0<code>async</code>\u00a0by replacing <code>call</code> with <code>call_async</code> or <code>stream</code> with <code>stream_async</code>.</p>"},{"location":"concepts/defining_and_extracting_schemas/","title":"Defining and extracting schemas","text":"<p>Mirascope's extraction functionality is built on top of Pydantic. We will walk through the high-level concepts you need to know to get started extracting structured information with LLMs. We recommend reading their docs for more detailed explanations of everything that you can do with Pydantic.</p>"},{"location":"concepts/defining_and_extracting_schemas/#model","title":"Model","text":"<p>Defining the schema for extraction is done via models, which are classes that inherit from <code>pydantic.BaseModel</code>. We can then use an extractor to extract this schema:</p> <pre><code>from typing import Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"The Name of the Wind by Patrick Rothfuss.\"\n\n\nbook = BookExtractor().extract()\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>You can use tool classes like <code>OpenAITool</code> directly if you want to extract a single tool instead of just a schema (which is useful for calling attached functions).</p>"},{"location":"concepts/defining_and_extracting_schemas/#field","title":"Field","text":"<p>You can also use <code>pydantic.Fields</code> to add additional information for each field in your schema. Again, this information will be included in the prompt, and we can take advantage of that:</p> <pre><code>from typing import Type\n\nfrom mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel, Field\n\n\nclass Book(BaseModel):\n    title: str\n    author: str = Field(..., description=\"Last, First\")\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"The Name of the Wind by Patrick Rothfuss.\"\n\n\nbook = BookExtractor().extract()\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='The Name of the Wind' author='Rothfuss, Patrick'\n</code></pre> <p>Notice how instead of \u201cPatrick Rothfuss\u201d the extracted author is \u201cRothfuss, Patrick\u201d as desired.</p>"},{"location":"concepts/defining_and_extracting_schemas/#retries","title":"Retries","text":"<p>Sometimes the model will fail to extract the schema. This can often be a result of the prompt; however, sometimes it\u2019s simply a failure of the model. If you want to retry the extraction some number of times, you can set <code>retries</code> equal to however many retries you want to run (defaults to 0).</p> <pre><code>book = BookExtractor().extract(retries=3)  # will retry up to 3 times \n</code></pre>"},{"location":"concepts/defining_and_extracting_schemas/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<p>In the above examples, we\u2019re extracting information present in the prompt text into structured form. We can also use <code>extract</code> to generate structured information from a prompt:</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"A science fiction book.\"\"\"\n\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIPrompt[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a book.\"\n\nbook = BookRecommender().extract()\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='Dune' author='Frank Herbert'\n</code></pre> <p>Notice that the docstring for the <code>Book</code> schema specified a science fiction book, which resulted in the model recommending a science fiction book. The docstring gets included with the prompt as part of the schema definition, and you can use this to your advantage for better prompting.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/","title":"Defining tools (function calls)","text":"<p>Tools are extremely useful when you want the model to intelligently choose to output the arguments to call one or more functions. With Mirascope it is extremely easy to use tools.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/#using-tools-in-mirascope","title":"Using tools in Mirascope","text":"<p>Mirascope will automatically convert any function properly documented with a docstring into a tool. This means that you can use any such function as a tool with no additional work. The function below is taken from OpenAI documentation with Google style python docstrings:</p> <pre><code>import json\n\nfrom typing import Literal\n\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON object containing the location, temperature, and unit.\n    \"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n</code></pre> <p>Note</p> <p>We support Google, ReST, Numpydoc, and Epydoc style docstrings.</p> <p>You can also define your own\u00a0<code>OpenAITool</code>\u00a0class. This is necessary when the function you want to use as a tool does not have a docstring. Additionally, the\u00a0<code>OpenAITool</code>\u00a0class makes it easy to further update the descriptions, which is useful when you want to further engineer your prompt:</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\n\nfrom mirascope.base import tool_fn\nfrom mirascope.openai import OpenAITool\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    # Assume this function does not have a docstring\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\n@tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n</code></pre> <p>Using the\u00a0tool_fn\u00a0decorator will attach the function defined by the tool to the tool for easier calling of the function. This happens automatically when using the function directly.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/#tools-with-openai-api-only","title":"Tools with OpenAI API only","text":"<p>Using the same OpenAI docs, the function call is defined as such:</p> <pre><code>import json\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n</code></pre> <p>OpenAI uses JSON Schema to define the tool call:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n</code></pre> <p>You can quickly see how bloated OpenAI tools become when defining multiple tools:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                },\n                \"required\": [\"location\", \"format\"],\n            },\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_n_day_weather_forecast\",\n            \"description\": \"Get an N-day weather forecast\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                    \"num_days\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The number of days to forecast\",\n                    }\n                },\n                \"required\": [\"location\", \"format\", \"num_days\"]\n            },\n        }\n    },\n]\n</code></pre> <p>With Mirascope, it will look like this:</p> <pre><code>class GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\n\nclass GetNDayWeatherForecast(GetCurrentWeather):\n    \"\"\"Get an N-day weather forecast\"\"\"\n\n    num_days: int = Field(..., description=\"The number of days to forecast\")\n</code></pre> <p>We can take advantage of class inheritance and reduce repetition. </p>"},{"location":"concepts/defining_tools_%28function_calls%29/#other-providers","title":"Other Providers","text":"<p>If you are using a function property documented with a docstring, you do not need to make any code changes when using other providers. Mirascope will automatically convert these functions to their proper format for you under the hood.</p> <p>For classes, simply replace <code>OpenAITool</code> with your provider of choice e.g. <code>GeminiTool</code> to match your choice of call.</p>"},{"location":"concepts/dumping_prompts_and_calls/","title":"Dumping prompts and calls","text":"<p>The <code>.dump()</code> function can be called from prompts, calls, and responses to output a dictionary of associated data. </p>"},{"location":"concepts/dumping_prompts_and_calls/#dumping-from-the-prompt","title":"Dumping from the Prompt","text":"<p>When called from <code>BasePrompt</code> or any of its subclasses like <code>BaseCall</code>, <code>.dump()</code> will give you:</p> <ul> <li>the prompt template</li> <li>inputs used to construct the prompt</li> <li>the prompt\u2019s tags</li> <li>any parameters specific to the model provider\u2019s API call, if they are not None:</li> <li>start and end times of its affiliated completion, if it has happened</li> </ul> <pre><code>import os\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n@tags([\"recommendation_project\", \"version:0001\"])\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n\nrecommender = BookRecommender(topic=\"how to bake a cake\")\nprint(recommender.dump())\n\n\"\"\"\nOutput:\n{\n    \"template\": \"Can you recommend some books on {topic}?\",\n    \"inputs\": {\"api_key\": None, \"topic\": \"how to bake a cake\"},\n    \"tags\": [\"recommendation_project\", \"version:0001\"],\n    \"call_params\": {\"model\": \"gpt-3.5-turbo-0125\"},\n    \"start_time_ms\": None,\n    \"end_time_ms\": None,\n}\n\"\"\"\n\nrecommender.call()\nprint(recommender.dump())\n\n\"\"\"\nOutput:\n{\n    # ... same as above\n    \"start_time_ms\": 1709847166609.473,\n    \"end_time_ms\": 1709847169424.146,\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts_and_calls/#dumping-from-the-response","title":"Dumping from the Response","text":"<p>You can also call <code>.dump()</code> on responses themselves, which will contain:</p> <ul> <li>start and end times of the response</li> <li>parameters of the call to the API associated with the response, within the key \u201coutput\u201d</li> </ul> <pre><code>response = recomender.call()  # call is an OpenAICall, continued from above\nprint(response.dump())\n\n\"\"\"\nOutput:\n{\n    \"output\": {\n        \"id\": \"chatcmpl-8zuVFGO2zgRsyckc9iW8CTSOgiNQm\",\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"index\": 0,\n                \"logprobs\": None,\n                \"message\": {\n                    \"content\": '1. \"The Cake Bible\" by Rose Levy Beranbaum...\n                    \"role\": \"assistant\",\n                    \"function_call\": None,\n                    \"tool_calls\": None,\n                },\n            }\n        ],\n        \"created\": 1709765897,\n        \"model\": \"gpt-3.5-turbo-0125\",\n        \"object\": \"chat.completion\",\n        \"system_fingerprint\": \"fp_2b778c6b35\",\n        \"usage\": {\"completion_tokens\": 210, \"prompt_tokens\": 19, \"total_tokens\": 229},\n    },\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts_and_calls/#combining-both","title":"Combining Both","text":"<p>We also give you an option to see everything at once by calling <code>BasePrompt.dump() | response.dump()</code> , which will union the two dictionaries and display them in one. Note that the <code>.dump()</code> function outputs a dictionary, so feel free to use it flexibly to suit your needs.</p> <pre><code>print(recommender.dump(response.dump()))\n\n\"\"\"\nOutput:\n{\n    \"template\": \"Can you recommend some books on {topic}?\",\n    \"inputs\": {\"api_key\": None, \"topic\": \"how to bake a cake\"},\n    \"tags\": [\"recommendation_project\", \"version:0001\"],\n    \"call_params\": {\"model\": \"gpt-3.5-turbo-0125\"},\n    \"start_time\": 1709837824962.49,\n    \"end_time\": 1709837825585.0588,\n    \"output\": {\n        \"id\": \"chatcmpl-8zuVFGO2zgRsyckc9iW8CTSOgiNQm\",\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"index\": 0,\n                \"logprobs\": None,\n                \"message\": {\n                    \"content\": '1. \"The Cake Bible\" by Rose Levy Beranbaum...\n                    \"role\": \"assistant\",\n                    \"function_call\": None,\n                    \"tool_calls\": None,\n                },\n            }\n        ],\n        \"created\": 1709765897,\n        \"model\": \"gpt-3.5-turbo-0125\",\n        \"object\": \"chat.completion\",\n        \"system_fingerprint\": \"fp_2b778c6b35\",\n        \"usage\": {\"completion_tokens\": 210, \"prompt_tokens\": 19, \"total_tokens\": 229},\n    },\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts_and_calls/#logging","title":"Logging","text":"<p>Now that you have the JSON dump, it can be useful to log your responses:</p> <pre><code>\"\"\"A basic example on how to log the data from a prompt and a chat completion.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Optional\n\nimport pandas as pd\nfrom sqlalchemy import JSON, Float, Integer, MetaData, String, create_engine\nfrom sqlalchemy.dialects.postgresql import JSONB\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, sessionmaker\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\nlogger = logging.getLogger(\"mirascope\")\nTABLE_NAME = \"openai_call_responses\"\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass OpenAICallResponseTable(Base):\n    __tablename__ = TABLE_NAME\n    id: Mapped[int] = mapped_column(\n        Integer(), primary_key=True, autoincrement=True, nullable=False\n    )\n    template: Mapped[str] = mapped_column(String(), nullable=False)\n    inputs: Mapped[Optional[dict]] = mapped_column(JSONB)\n    tags: Mapped[Optional[list[str]]] = mapped_column(JSON)\n    call_params: Mapped[Optional[dict]] = mapped_column(JSONB)\n    start_time: Mapped[Optional[float]] = mapped_column(Float(), nullable=False)\n    end_time: Mapped[Optional[float]] = mapped_column(Float(), nullable=False)\n    output: Mapped[Optional[dict]] = mapped_column(JSONB)\n\n\n@tags([\"recommendation_project\"])\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n\nUSERNAME = \"root\"\nPASSWORD = \"\"\nHOST = \"localhost\"\nPORT = \"5432\"\nDB_NAME = \"mirascope\"\nengine = create_engine(f\"postgresql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DB_NAME}\")\n\n\ndef create_database():\n    \"\"\"Create the database and table for the OpenAI call response.\"\"\"\n    metadata = MetaData()\n    table_objects = [Base.metadata.tables[TABLE_NAME]]\n    metadata.create_all(engine, tables=table_objects)\n\n\ndef log_to_database(recommender_response: dict[str, Any]):\n    \"\"\"Create a call response and log it to the database.\"\"\"\n    create_database()\n    Session = sessionmaker(engine)\n    with Session() as session:\n        openai_completion_db = OpenAICallResponseTable(**recommender_response)\n        session.add(openai_call_responses)\n        session.commit()\n\n\ndef log_to_csv(recommender_response: dict[str, Any]):\n    \"\"\"Log the call response to a CSV file.\"\"\"\n    df = pd.DataFrame([recommender_response])\n    with open(\"log.csv\", \"w\") as f:\n        df.to_csv(f, index=False)\n\n\ndef log_to_logger(recommender_response: dict[str, Any]):\n    \"\"\"Log the call response to the logger.\"\"\"\n    logger.info(recommender_response)\n\n\nif __name__ == \"__main__\":\n    recommender = BookRecommender(topic=\"how to bake a cake\")\n    response = recommender.call()\n    recommender_response = recommender.dump() | response.dump()\n    log_to_database(recommender_response)\n    log_to_csv(recommender_response)\n    log_to_logger(recommender_response)\n</code></pre>"},{"location":"concepts/extracting_base_types/","title":"Extracting base types","text":"<p>Mirascope also makes it possible to extract base types without defining a <code>pydantic.BaseModel</code> with the same exact format for extraction:</p> <pre><code>from mirascope.openai import OpenAIExtractor\n\n\nclass BookRecommender(OpenAIExtractor[list[str]]):\n    extract_schema: Type[list[str]] = list[str]\n    prompt_template = \"Please recommend some science fiction books.\"\n\n\nbooks = BookRecommendation().extract()\nprint(books)\n#&gt; ['Dune', 'Neuromancer', \"Ender's Game\", \"The Hitchhiker's Guide to the Galaxy\", 'Foundation', 'Snow Crash']\n</code></pre> <p>We currently support: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>list</code>, <code>set</code>, <code>tuple</code>, and <code>Enum</code>.</p> <p>We also support using <code>Union</code>, <code>Literal</code>, and <code>Annotated</code> </p> <p>Note</p> <p>If you\u2019re using <code>mypy</code> you\u2019ll need to add <code>#  type: ignore</code> due to how these types are handled differently by Python.</p>"},{"location":"concepts/extracting_base_types/#using-enum-or-literal-for-classification","title":"Using <code>Enum</code> or <code>Literal</code> for classification","text":"<p>One nice feature of extracting base types is that we can easily use <code>Enum</code> or <code>Literal</code> to define a set of labels that the model should use to classify the prompt. For example, let\u2019s classify whether or not some email text is spam:</p> <pre><code>from enum import Enum\n# from typing import Literal\n\nfrom mirascope.openai import OpenAIExtractor\n\n# Label = Literal[\"is spam\", \"is not spam\"]\n\n\nclass Label(Enum):\n    NOT_SPAM = \"not_spam\"\n    SPAM = \"spam\"\n\n\nclass NotSpam(OpenAIExtractor[Label]):\n    extract_schema: Type[Label] = Label\n    prompt_template = \"Your car insurance payment has been processed. Thank you for your business.\"\n\n\nclass Spam(OpenAIExtractor[Label]):\n    extract_schema: Type[Label] = Label\n    prompt_template = \"I can make you $1000 in just an hour. Interested?\"\n\n\n# assert NotSpam().extract() == \"is not spam\"\n# assert Spam().extract() == \"is spam\"\nassert NotSpam().extract() == Label.NOT_SPAM\nassert Spam().extract() == Label.SPAM\n</code></pre>"},{"location":"concepts/extracting_structured_information_using_llms/","title":"Extracting structured information with LLMs","text":"<p>Large Language Models (LLMs) are powerful at generating human-like text, but their outputs are inherently unstructured. Many real-world applications require structured data to function properly, such as extracting due dates, priorities, and task descriptions from user inputs for a task management application, or extracting tabular data from unstructured text sources for data analysis pipelines.</p> <p>Mirascope provides tools and techniques to address this challenge, allowing you to extract structured information from LLM outputs reliably.</p>"},{"location":"concepts/extracting_structured_information_using_llms/#challenges-in-extracting-structured-information","title":"Challenges in Extracting Structured Information","text":"<p>The key challenges in extracting structured information from LLMs include:</p> <ol> <li>Unstructured Outputs: LLMs are trained on vast amounts of unstructured text data, causing their outputs to be unstructured as well.</li> <li>Hallucinations and Inaccuracies: LLMs can sometimes generate factually incorrect information, complicating the extraction of accurate structured data.</li> </ol>"},{"location":"concepts/extracting_structured_information_using_llms/#mirascopes-approach","title":"Mirascope's Approach","text":"<p>Mirascope offers a convenient <code>extract</code> method on extractor classes to extract structured information from LLM outputs. This method leverages tools (function calling) to reliably extract the required structured data. While you can find more details in the following pages, let's consider a simple example where we want to extract task details like due date, priority, and description from a user's natural language input:</p> <pre><code>from typing import Literal\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    description: str\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract(TaskDetails)\nassert isinstance(task_details, TaskDetails)\nprint(TaskDetails)\n#&gt; due_date='next Friday' priority='high' description='Submit quarterly report'\n</code></pre> <p>As you can see, Mirascope makes this extremely simple. Under the hood, Mirascope uses the provided schema to extract the generated content and validate it (see Validation for more details).</p>"},{"location":"concepts/generating_content/","title":"Generating content","text":"<p>Now that you have your prompt, you can combine it with a model call to generate content. Mirascope provides high-level wrappers around common providers so you can focus on prompt engineering instead of learning the interface for providers. Our high-level wrappers are not required to use our prompts but simply provide convenience if you wish to use it.</p> <p>Note</p> <p>This doc uses OpenAI. See using different model providers for how to generate content with other model providers like Anyscale, Together, Gemini, and more.</p>"},{"location":"concepts/generating_content/#openaicall","title":"OpenAICall","text":"<p><code>OpenAICall</code> extends <code>BasePrompt</code> and <code>BaseCall</code> to support interacting with the OpenAI API.</p>"},{"location":"concepts/generating_content/#call","title":"Call","text":"<p>You can initialize an <code>OpenAICall</code> instance and call the <code>call</code> method to generate an <code>OpenAICallResponse</code>:</p> <pre><code>import os\n\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nresponse = RecipeRecommender(ingredient=\"apples\").call()\nprint(response.content)  # prints the string content of the call\n</code></pre> <p>The <code>call_params</code> of the OpenAI client is tied to the call (and thereby the prompt). Refer to Engineering better prompts [Add link] for more information.</p>"},{"location":"concepts/generating_content/#async","title":"Async","text":"<p>If you are want concurrency, you can use the <code>async</code> function instead.</p> <pre><code>import asyncio\nimport os\n\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nasync def recommend_recipes():\n    \"\"\"Asynchronously calls the model using `OpenAICall` to generate a recipe.\"\"\"\n    return await RecipeRecommender(ingredient=\"apples\").call_async()\n\n\nprint(asyncio.run(recommend_recipes())) \n</code></pre>"},{"location":"concepts/generating_content/#callresponse","title":"CallResponse","text":"<p>The <code>call</code> method returns an <code>OpenAICallResponse</code> class instance, which is a simple wrapper around the <code>ChatCompletion</code> class in <code>openai</code> that extends <code>BaseCallResponse</code>. In fact, you can access everything from the original response as desired. The primary purpose of the class is to provide convenience.</p> <pre><code>from mirascope.openai.types import OpenAICallResponse\n\nresponse = OpenAICallResponse(...)\n\ncompletion.content     # original.choices[0].message.content\ncompletion.tool_calls  # original.choices[0].message.tool_calls\ncompletion.message     # original.choices[0].message\ncompletion.choice      # original.choices[0]\ncompletion.choices     # original.choices\nresponse.response      # ChatCompletion(...)\n</code></pre>"},{"location":"concepts/generating_content/#chaining","title":"Chaining","text":"<p>Adding a chain of calls is as simple as writing a function:</p> <pre><code>import os\nfrom functools import cached_property\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass ChefSelector(OpenAICall):\n    prompt_template = \"Name a chef who is really good at cooking {food_type} food\"\n\n    food_type: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nclass RecipeRecommender(ChefSelector):\n    prompt_template = \"\"\"\n    SYSTEM:\n    Imagine that you are chef {chef}.\n    Your task is to recommend recipes that you, {chef}, would be excited to serve.\n\n    USER:\n    Recommend a {food_type} recipe using {ingredient}.\n    \"\"\"\n\n    ingredient: str\n\n    @cached_property\n    def chef(self) -&gt; str:\n        \"\"\"Uses `ChefSelector` to select the chef based on the food type.\"\"\"\n        return ChefSelector(food_type=self.food_type).call().content\n\n\nresponse = RecipeRecommender(food_type=\"japanese\", ingredient=\"apples\").call()\nprint(response.content)\n# &gt; Certainly! Here's a recipe for a delicious and refreshing Japanese Apple Salad: ...\n</code></pre>"},{"location":"concepts/integrations/","title":"Integrations","text":""},{"location":"concepts/integrations/#client-wrappers","title":"Client Wrappers","text":"<p>If you want to use Mirascope in conjunction with another library which implements an OpenAI wrapper (such as LangSmith), you can do so easily by setting the <code>wrapper</code> parameter within <code>OpenAICallParams</code>. Setting this parameter will internally wrap the <code>OpenAI</code> client within an <code>OpenAICall</code>, giving you access to both sets of functionalities.</p> <pre><code>from some_library import some_wrapper\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\",\n        wrapper=some_wrapper\n    )\n</code></pre> <p>Now, every call to <code>call</code>, <code>call_async</code>, <code>stream</code>, and <code>stream_async</code> will be executed on top of the wrapped <code>OpenAI</code> client.</p>"},{"location":"concepts/integrations/#weights-biases","title":"Weights &amp; Biases","text":"<p>If you want to seamlessly use Weights &amp; Biases\u2019 logging functionality, we\u2019ve got you covered -  <code>WandbOpenAICall</code> is an <code>OpenAICall</code> with unique creation methods that internally call W&amp;B\u2019s <code>Trace()</code> function and log your runs. For standard chat completions, you can use <code>WandbOpenAICall.call_with_trace()</code>, and for extractions, you can use <code>WandbOpenAIExtractor</code>'s <code>extract_with_trace</code> method.</p>"},{"location":"concepts/integrations/#generating-content-with-a-wb-trace","title":"Generating Content with a W&amp;B Trace","text":"<p>The <code>call_with_trace()</code> function internally calls both <code>OpenAICall.call()</code> and <code>wandb.Trace()</code> and is configured to properly log both successful completions and errors. </p> <p>Note that unlike a standard <code>OpenAICall</code>, it requires the argument <code>type</code> to specify the type of <code>Trace</code> it initializes.  Once called, it will return a tuple of the <code>OpenAICallResponse</code>  and the created span <code>Trace</code>. </p> <pre><code>import os\nfrom mirascope.openai.wandb import WandbPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Explainer(WandbOpenAICall):\n    prompt_template = \"Tell me more about {topic} in detail.\"\n\n    topic: str\n\n\nresponse, span = Explainer(type=\"llm\",topic=\"the Roman Empire\").call_with_trace()\nspan.log(name=\"my_trace\")\n</code></pre> <p>In addition, <code>call_with_trace</code> can take an argument  <code>parent</code> for chained completions, and the initialized <code>Trace</code> will be linked with its parent within W&amp;B logs. </p> <pre><code>import os\nfrom mirascope.wandb import WandbPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Explainer(WandbOpenAICall):\n    prompt_template = \"Tell me more about {topic} in detail.\"\n\n    topic: str\n\n\nclass Summarizer(WandbOpenAICall):\n    prompt_template = \"Summarize the following: {text}\"\n\n    text: str\n\n\nexplainer = Explainer(type=\"llm\",topic=\"the Roman Empire\")\nresponse, explain_span = explainer.call_with_trace()\n\nsummarizer = Summarizer(type=\"llm\", text=explanation.content)\nresponse, _ = summarizer.call_with_trace(explain_span)\n\nexplain_span.log(name=\"my_trace\")\n</code></pre> <p>Since <code>WandbOpenAICall</code> inherits from <code>OpenAICall</code>, it will support function calling the same way you would a standard <code>OpenAICall</code>, as seen here</p>"},{"location":"concepts/integrations/#extracting-with-a-wb-trace","title":"Extracting with a W&amp;B Trace","text":"<p>When working with longer chains, it is often useful to use extractions so that data is passed along in a structured format. Just like <code>call_with_trace()</code> , you will need to pass in a <code>type</code> argument to the extractor and a <code>parent</code> to the extraction.</p> <pre><code>import os\nfrom typing import Type\n\nfrom mirascope.wandb import WandbOpenAIExtractor\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass OceanCounter(WandbOpenAIExtractor[int]):\n    extract_schema: Type[int] = int\n    prompt_template = \"There are 7 oceans on earth.\"\n\n\nnum_oceans, span = OceanCounter(type=\"tool\").extract_with_trace()\n\nspan.log(name=\"mirascope_trace\")\n</code></pre>"},{"location":"concepts/integrations/#langchain-and-langsmith","title":"LangChain and LangSmith","text":""},{"location":"concepts/integrations/#logging-a-langsmith-trace","title":"Logging a LangSmith trace","text":"<p>You can use client wrappers (as mentioned in the first section of this doc) to integrate Mirascope with LangSmith. When using a wrapper, you can generate content as you would with a normal <code>OpenAICall</code>:</p> <pre><code>import os\nfrom langsmith import wrappers\n\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\",\n        wrapper=wrappers.wrap_openai\n    )\n\nresponse = BookRecommender(topic=\"sci-fi\").call()\n</code></pre> <p>Now, if you log into LangSmith , you will be see your results have been traced. Of course, this integration works not just for <code>call</code>, but also for <code>stream</code> and <code>extract</code>.</p>"},{"location":"concepts/integrations/#using-mirascope-baseprompt-with-langchain","title":"Using Mirascope <code>BasePrompt</code> with LangChain","text":"<p>You may also want to use LangChain given it\u2019s tight integration with LangSmith. For us, one issue we had when we first started using LangChain was that their <code>invoke</code> function had no type-safety or lint help. This means that calling <code>invoke({\"foox\": \"foo\"})</code> was a difficult bug to catch. There\u2019s so much functionality in LangChain, and we wanted to make using it more pleasant.</p> <p>With Mirascope prompts, you can instantiate a <code>ChatPromptTemplate</code> from a Mirascope prompt template, and you can use the prompt\u2019s <code>model_dump</code> method so you don\u2019t have to worry about the invocation dictionary:</p> <pre><code>import os\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom mirascope import BasePrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass JokePrompt(BasePrompt):\n    prompt_template = \"Tell me a short joke about {topic}\"\n\n    topic: str\n\n\njoke_prompt = JokePrompt(topic=\"ice cream\")\nprompt = ChatPromptTemplate.from_template(joke_prompt.template())\n# ^ instead of:\n# prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n\nmodel = ChatOpenAI(model=\"gpt-4\")\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\n\njoke = chain.invoke(joke_prompt.model_dump())\n# ^ instead of:\n# joke = chain.invoke({\"topic\": \"ice cream\"})\nprint(joke)\n</code></pre>"},{"location":"concepts/integrations/#want-more-integrations","title":"Want more integrations?","text":"<p>If there are features you\u2019d like that we haven\u2019t yet implemented, please submit a feature request to our GitHub Issues.</p> <p>We also welcome and greatly appreciate contributions if you\u2019re interested in helping us out!</p>"},{"location":"concepts/streaming_generated_content/","title":"Streaming generated content","text":"<p>Streaming generated content is similar to Generating Content so check that out if you haven\u2019t already.</p>"},{"location":"concepts/streaming_generated_content/#openaiprompt","title":"OpenAIPrompt","text":"<p>We will be using the same <code>OpenAICall</code> in Generating Content. Feel free to swap it out with a different provider.</p>"},{"location":"concepts/streaming_generated_content/#streaming","title":"Streaming","text":"<p>You can use the <code>stream</code> method to stream a response. All this is doing is setting <code>stream=True</code> and providing the <code>OpenAICallResponseChunk</code> convenience wrappers around the response chunks.</p> <pre><code>import os\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nstream = RecipeRecommender(ingredient=\"apples\").stream()\n\nfor chunk in stream:\n    print(chunk.content, end=\"\")\n</code></pre>"},{"location":"concepts/streaming_generated_content/#async","title":"Async","text":"<p>If you want concurrency, you can use the <code>stream_async</code> function instead.</p> <pre><code>import os\n\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nasync def stream_recipe_recommendation():\n    \"\"\"Asynchronously streams the response for a call to the model using `OpenAICall`.\"\"\"\n    stream = RecipeRecommender(ingredient=\"apples\").async_stream()\n    async for chunk in astream:\n        print(chunk.content, end=\"\")\n\nasyncio.run(stream_recipe_recommendation())\n</code></pre>"},{"location":"concepts/streaming_generated_content/#openaicallresponsechunk","title":"OpenAICallResponseChunk","text":"<p>The <code>stream</code> method returns an <code>OpenAICallResponseChunk</code> instance, which is a convenience wrapper around the <code>ChatCompletionChunk</code> class in <code>openai</code></p> <pre><code>from mirascope.openai.types import OpenAIChatCompletionChunk\n\nchunk = OpenAICallResponseChunk(...)\n\nchunk.content  # original.choices[0].delta.content\nchunk.delta    # original.choices[0].delta\nchunk.choice   # original.choices[0]\nchunk.choices  # original.choices\nchunk.chunk    # ChatCompletionChunk(...)\n</code></pre>"},{"location":"concepts/tools_%28function_calling%29/","title":"Tools (Function Calling)","text":"<p>Large Language Models (LLMs) are incredibly powerful at generating human-like text, but their capabilities extend far beyond mere text generation. With the help of tools (often called function calling), LLMs can perform a wide range of tasks, from mathematical calculations to code execution and information retrieval.</p>"},{"location":"concepts/tools_%28function_calling%29/#what-are-tools","title":"What are Tools?","text":"<p>Tools, in the context of LLMs, are essentially functions or APIs that the model can call upon to perform specific tasks. These tools can range from simple arithmetic operations to complex web APIs or custom-built functions. By leveraging tools, LLMs can augment their capabilities and provide more accurate and useful outputs.</p>"},{"location":"concepts/tools_%28function_calling%29/#why-are-tools-important","title":"Why are Tools Important?","text":"<p>Traditionally, LLMs have been limited to generating text based solely on their training data and the provided prompt. While this approach can produce impressive results, it also has inherent limitations. Tools allow LLMs to break free from these constraints by accessing external data sources, performing calculations, and executing code, among other capabilities.</p> <p>Incorporating tools into LLM workflows opens up a wide range of possibilities, including:</p> <ol> <li>Improved Accuracy: By leveraging external data sources and APIs, LLMs can provide more accurate and up-to-date information, reducing the risk of hallucinations or factual errors.</li> <li>Enhanced Capabilities: Tools allow LLMs to perform tasks that would be challenging or impossible with text generation alone, such as mathematical computations, code execution, and data manipulation.</li> <li>Contextualized Responses: By incorporating external data and contextual information, LLMs can provide more relevant and personalized responses, tailored to the user's specific needs and context.</li> </ol>"},{"location":"concepts/tools_%28function_calling%29/#tools-in-mirascope","title":"Tools in Mirascope","text":"<p>Mirascope provides a clean and intuitive way to incorporate tools into your LLM workflows. The simplest form-factor we offer is to extract a single tool automatically generated from a function. We can then call that function with the extracted arguments:</p> <pre><code>from mirascope.openai import OpenAICall\n\n\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get's the weather for `location` and prints it.\n\n    Args:\n        location: The \"City, State\" or \"City, Country\" for which to get the weather.\n    \"\"\"\n    print(location)\n    if location == \"Tokyo, Japan\":\n        return f\"The weather in {location} is 72 degrees and sunny.\"\n    elif location == \"San Francisco, CA\":\n        return f\"The weather in {location} is 45 degrees and cloudy.\"\n    else:\n        return f\"I'm sorry, I don't have the weather for {location}.\"\n\n\nclass Forecast(OpenAICall):\n    \"\"\"What's the weather in Tokyo?\"\"\"\n\n\nweather_tool = Forecast().extract(get_weather)\nprint(weather_tool.fn(**weather_tool.args))\n#&gt; The weather in Tokyo, Japan is 72 degrees and sunny.\n</code></pre> <p>Note</p> <p>While it may not be clear from the above example, <code>tool.fn</code> is an extremely powerful simplification. When using multiple tools, having the function attached to the tool makes it immediately accessible and callable with a single line of code.</p> <p>In the following pages, we\u2019ll go into greater detail around how to define and use tools effectively.</p>"},{"location":"concepts/using_different_model_providers/","title":"Using different model providers","text":"<p>Testing out various providers is a powerful way to boost the performance of your calls. Mirascope makes it fast and simple to swap between various providers.</p> <p>We currently support the following providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Gemini</li> <li>Mistral</li> </ul> <p>This also means that we support any providers that use these APIs.</p>"},{"location":"concepts/using_different_model_providers/#using-providers-that-support-the-openai-api","title":"Using providers that support the OpenAI API","text":"<p>If you want to use a provider that supports the OpenAI API, simply update the base_url.</p> <p>This works for endpoints such as:</p> <ul> <li>Ollama</li> <li>Anyscale</li> <li>Together</li> <li>Groq</li> <li>and more\u2026</li> </ul> <pre><code>import os\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"OTHER_PROVIDER_API_KEY\"\n\n\nclass RecipeRecommender(OpenAICall):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    base_url = \"BASE_URL\"\n    call_params = OpenAICallParams(model=\"...\")\n</code></pre>"},{"location":"concepts/using_different_model_providers/#swapping-from-openai-to-gemini","title":"Swapping from OpenAI to Gemini","text":"<p>The generative-ai library and openai-python library are vastly different from each other, so swapping between them to attempt to gain better prompt responses is not worth the engineering effort and maintenance. </p> <p>This leads to people typically sticking with one provider even when providers release new features frequently. Take for example when Google announced Gemini 1.5, it would be very useful to implement prompts with the new context window. Thankfully, Mirascope makes this swap trivial.</p>"},{"location":"concepts/using_different_model_providers/#assuming-you-are-starting-with-openaicall","title":"Assuming you are starting with OpenAICall","text":"<pre><code>import os\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAICall):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"...\")\n\n\nresponse = RecipeRecommender(ingredient=\"apples\").call()\nprint(response.content)\n</code></pre> <ol> <li>First install Mirascope\u2019s integration with gemini if you haven\u2019t already.</li> </ol> <pre><code>pip install mirascope[gemini]\n</code></pre> <ol> <li>Swap out OpenAI with Gemini:<ol> <li>Replace <code>OpenAICall</code> and <code>OpenAICallParams</code> with <code>GeminiCall</code> and <code>GeminiCallParams</code> respectively </li> <li>Configure your Gemini API Key</li> <li>Update <code>GeminiCallParams</code> with new model and other attributes</li> </ol> </li> </ol> <pre><code>from google.generativeai import configure\nfrom mirasope.gemini import GeminiPrompt, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass RecipeRecommender(GeminiCall):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n\n\nresponse = RecipeRecommender(ingredient=\"apples\").call()\nprint(response.content)\n</code></pre> <p>That\u2019s it for the basic example! Now you can evaluate the quality of your prompt with Gemini.</p>"},{"location":"concepts/using_different_model_providers/#something-a-bit-more-advanced","title":"Something a bit more advanced","text":"<p>What if you want to use a more complex message? The steps above are all the same except with one extra step.</p> <p>Consider this OpenAI example:</p> <pre><code>from mirascope import OpenAIPrompt, OpenAICallParams\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass Recipe(OpenAICall):\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n    @property\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\"role\": \"system\", \"content\": \"You are the world's greatest chef.\"},\n            {\"role\": \"user\", \"content\": f\"Can you recommend some recipes that use {self.ingredient} as an ingredient?\"},\n        ]\n</code></pre> <p>The Gemini example will look like this:</p> <pre><code>from google.generativeai import configure\nfrom google.generativeai.types import ContentsType\nfrom mirasope.gemini import GeminiPrompt, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass Recipe(GeminiPrompt):\n    \"\"\"A normal docstring\"\"\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n\n    @property\n    def messages(self) -&gt; ContentsType:\n        return [\n            {\"role\": \"user\", \"parts\": [\"You are the world's greatest chef.\"]},\n            {\"role\": \"model\", \"parts\": [\"I am the world's greatest chef.\"]},\n            {\"role\": \"user\", \"parts\": [f\"Can you recommend some recipes that use {self.ingredient} as an ingredient?\"]},\n        ]\n</code></pre> <p>Update the return type from <code>list[ChatCompletionMessageParam]</code> to <code>ContentsType</code> for the <code>messages</code> method. Gemini doesn\u2019t have a <code>system</code> role, so instead we need to simulate OpenAI\u2019s <code>system</code> message using a <code>user</code> \u2192 <code>model</code> pair. Refer to the providers documentation on how to format their messages array.</p>"},{"location":"concepts/using_different_model_providers/#swapping-to-another-provider","title":"Swapping to another provider","text":"<p>While the example above uses OpenAI to Gemini, the same applies to any provider that we support. If there\u2019s a provider you would like us to support request the feature on our GitHub Issues page or contribute a PR yourself.</p>"},{"location":"concepts/using_the_mirascope_cli/","title":"Using the Mirascope CLI","text":"<p>One of the main frustrations of dealing with prompts and calls is keeping track of all the various revisions. Taking inspiration from alembic and git, the Mirascope CLI provides a couple of key commands to make managing prompts and calls easier.</p>"},{"location":"concepts/using_the_mirascope_cli/#the-prompt-management-environment","title":"The prompt management environment","text":"<p>The first step to using the Mirascope CLI is to use the <code>init</code> command in your project's root directory.</p> <pre><code>mirascope init\n</code></pre> <p>This will create the directories and files to help manage prompts and calls. Here is a sample structure created by the <code>init</code> function:</p> <pre><code>|\n|-- mirascope.ini\n|-- mirascope\n|   |-- prompt_template.j2\n|   |-- versions/\n|   |   |-- &lt;directory_name&gt;/\n|   |   |   |-- version.txt\n|   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n|-- prompts/\n</code></pre> <p>Here is a rundown of each directory and file:</p> <ul> <li><code>mirascope.ini</code> - The INI file that can be customized for your project</li> <li><code>mirascope</code> - The default name of the directory that is home to the prompt management environment</li> <li><code>prompt_template.j2</code> - The Jinja2 template file that is used to generate prompt versions</li> <li><code>versions</code> - The directory that holds the various prompt versions</li> <li><code>versions/&lt;directory_name</code> - The sub-directory that is created for each prompt file in the <code>prompts</code> directory</li> <li><code>version.txt</code> - A file system method of keeping track of current and latest revisions. Coming soon is revision tracking using a database instead</li> <li><code>&lt;revision_id&gt;_&lt;directory_name&gt;.py</code> - A prompt version that is created by the <code>mirascope add</code> command, more on this later.</li> <li><code>prompts</code> - The user's directory that stores all prompt and call files</li> </ul> <p>The directory names can be changed anytime by modifying the <code>mirascope.ini</code> file or when running the <code>init</code> command.</p> <pre><code>mirascope init --mirascope_location my_mirascope --prompts_location calls\n</code></pre>"},{"location":"concepts/using_the_mirascope_cli/#saving-your-first-prompt","title":"Saving your first prompt","text":"<p>After creating the prompt management directory, you are now ready to build and iterate on some prompts. Begin by adding a Mirascope Prompt to the prompts directory.</p> <pre><code># prompts/book_recommender.py\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic} in a list format?\"\n\n    topic: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo\")\n</code></pre> <p>Once you are happy with the first iteration of this prompt, you can run:</p> <pre><code>mirascope add book_recommender\n</code></pre> <p>This will commit <code>book_recommender.py</code> to your <code>versions/</code> directory, creating a <code>book_recommender</code> sub-directory and a <code>0001_book_recommender.py</code>.</p> <p>Here is what <code>0001_book_recommender.py</code> will look like:</p> <pre><code># versions/book_recommender/0001_book_recommender.py\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nprev_revision_id = \"None\"\nrevision_id = \"0001\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic} in a list format?\"\n\n    topic: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo\")\n</code></pre> <p>The prompt inside the versions directory is almost identical to the prompt inside the prompts directory with a few differences.</p> <p>The variables <code>prev_revision_id</code> and <code>revision_id</code> will be used for features coming soon, so stay tuned for updates.</p>"},{"location":"concepts/using_the_mirascope_cli/#colocate","title":"Colocate","text":"<p>Everything that affects the quality of a prompt lives in the prompt. This is why <code>call_params</code> exists in <code>BaseCall</code> and why <code>OpenAICall</code> and other provider wrappers extend the <code>BasePrompt</code> class.</p>"},{"location":"concepts/using_the_mirascope_cli/#iterating-on-the-prompt","title":"Iterating on the prompt","text":"<p>Now that this version of <code>book_recommender</code> has been saved, you are now free to modify the original <code>book_recommender.py</code> and iterate. Maybe, you want to switch to a different provider and compare results.</p> <p>Here is what the next iteration of <code>book_recommender.py</code> will look like:</p> <pre><code># prompts/book_recommender.py\nfrom google.generativeai import configure\nfrom mirasope.gemini import GeminiCall, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass BookRecommender(GeminiCall):\n    prompt_template = \"Can you recommend some books on {topic} in a list format?\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n</code></pre> <p>Before adding the next revision of <code>my_prompt</code>, you may want to check the status of your prompt.</p> <pre><code># You can specify a specific prompt\nmirascope status my_prompt\n\n# or, you can check the status of all prompts\nmirascope status\n</code></pre> <p>Note that status will also be checked before the <code>add</code> or <code>use</code> command is run. Now we can run the same <code>add</code> command in the previous section to commit another version <code>0002_book_recommender.py</code></p>"},{"location":"concepts/using_the_mirascope_cli/#switching-between-versions","title":"Switching between versions","text":"<p>You can now freely switch different providers or use the same provider with a different model to iterate to the best results.</p> <p>You can use the <code>use</code> command to quickly switch between the prompts:</p> <pre><code>mirascope use book_recommender 0001\n</code></pre> <p>Here you specify which prompt and also which version you want to use. This will update your <code>prompts/book_recommender.py</code> with the contents of <code>versions/0001_book_recommender.py</code> (minus the variables used internally).</p> <p>This will let you quickly swap prompts or providers with no code change, the exception being when prompts have different attributes. In that case, your linter will detect missing or additional attributes that need to be addressed.</p>"},{"location":"concepts/using_the_mirascope_cli/#removing-prompts","title":"Removing prompts","text":"<p>Often times when experimenting with prompts, a lot of experimental prompts will need to be cleaned up in your project.</p> <p>You can use the <code>remove</code> command to delete any version:</p> <pre><code>mirascope remove book_recommender 0001\n</code></pre> <p>Here you specify which prompt and version you want to remove. Removal will delete the file but also update any versions that have the deleted version in their <code>prev_revision_id</code> to <code>None</code>.</p> <p>Note</p> <p><code>mirascope remove</code> will not remove the prompt if <code>current_revision</code> is the same as the prompt you are trying to remove. You can use <code>mirascope add</code> if you have incoming changes or <code>mirascope use</code> to swap <code>current_revision</code>.</p>"},{"location":"concepts/using_the_mirascope_cli/#mirascope-ini","title":"Mirascope INI","text":"<p>The Mirascope INI provides some customization for you. Feel free to update any field.</p> <pre><code>[mirascope]\n\n# path to mirascope directory\nmirascope_location = .mirascope\n\n# path to versions directory\nversions_location = %(mirascope_location)s/versions\n\n# path to prompts directory\nprompts_location = prompts\n\n# name of versions text file\nversion_file_name = version.txt\n\n# formats the version file\n# leave blank to not format \nformat_command = ruff check --select I --fix; ruff format\n\n# auto tag prompts with version\nauto_tag = True\n</code></pre> <ul> <li><code>auto_tag</code> - Adds <code>@tags([\"version:0001\"])</code> to Mirascope Prompts. This will auto increment the version number if a new version is added.</li> </ul>"},{"location":"concepts/using_the_mirascope_cli/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to the Mirascope CLI. Here is a list in no order of things we are thinking about adding next:</p> <ul> <li>prompt comparison - A way to compare two different versions with a golden test</li> <li>history - View the revision history of a version</li> <li>testing - Adding input and outputs to the revision for CI testing</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"concepts/validation/","title":"Validation","text":"<p>When extracting structured information from LLMs, it\u2019s important that we validate the extracted information, especially the types. We want to make sure that if we\u2019re looking for an integer that we actual get an <code>int</code> back. One of the primary benefits of building on top of Pydantic is that it makes validation easy \u2014 in fact, we get validation on types out-of-the-box.</p> <p>We recommend you check out their thorough documentation for detailed information on everything you can do with their validators. This document will be brief and specifically related to LLM extraction to avoid duplication.</p>"},{"location":"concepts/validation/#validating-types","title":"Validating Types","text":"<p>When we extract information \u2014 for base types, <code>BaseModel</code>, or any of our tools \u2014 everything is powered by Pydantic. This means that we automatically get type validation and can handle it gracefully:</p> <pre><code>from typing import Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Book(BaseModel):\n    title: str\n    price: float\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a book.\"\n\n\ntry:\n    book = BookRecommender().extract()\n    assert isinstance(book, Book)\n    print(book)\n    #&gt; title='The Alchemist' price=12.99\nexcept ValidationError as e:\n    print(e)\n  #&gt; 1 validation error for Book\n  #  price\n  #    Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='standard', input_type=str]\n  #      For further information visit https://errors.pydantic.dev/2.6/v/float_parsing\n</code></pre> <p>Now we can proceed with our extracted information knowing that it will behave as the expected type.</p>"},{"location":"concepts/validation/#custom-validation","title":"Custom Validation","text":"<p>It\u2019s often useful to write custom validation when working with LLMs so that we can automatically handle things that are difficult to hard-code. For example, consider determining whether the generated content adheres to your company\u2019s guidelines. It\u2019s a difficult task to determine this, but an LLM is well-suited to do the task well.</p> <p>We can use an LLM to make the determination by adding an <code>AfterValidator</code> to our extracted output:</p> <pre><code>from enum import Enum\nfrom typing import Annotated, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\nclass Label(Enum):\n    HAPPY = \"happy story\"\n    SAD = \"sad story\"\n\n\nclass Sentiment(OpenAIExtractor[Label]):\n    extract_schema: Type[Label] = Label\n    prompt_template = \"Is the following happy or sad? {text}.\"\n\n    text: str\n\n\ndef validate_happy(story: str) -&gt; str:\n    \"\"\"Check if the content follows the guidelines.\"\"\"\n    label = Sentiment(text=story).extract()\n    assert label == Label.HAPPY, \"Story wasn't happy.\"\n    return story\n\n\nclass HappyStory(BaseModel):\n    story: Annotated[str, AfterValidator(validate_happy)]\n\n\nclass StoryTeller(OpenAIExtractor[HappyStory]):\n    extract_template: Type[HappyStory] = HappyStory\n    prompt_template = \"Please tell me a story that's really sad.\"\n\n\ntry:\n    story = StoryTeller().extract()\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for HappyStoryTool\n    #   story\n    #     Assertion failed, Story wasn't happy. [type=assertion_error, input_value=\"Once upon a time, there ...er every waking moment.\", input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n</code></pre>"},{"location":"concepts/writing_prompts/","title":"Writing prompts","text":""},{"location":"concepts/writing_prompts/#the-baseprompt-class","title":"The <code>BasePrompt</code> Class","text":"<p>The <code>prompt_template</code> class variable is the template used to generate the list of messages. The properties of the class are the template variables, which get formatted into the template during the creation of the list of messages.</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages())\n#&gt; [{\"role\": \"user\", \"content\": \"Can you recommend some books on coding?\"}]\n</code></pre> <p>The <code>messages</code> method parses the <code>prompt_template</code> into a list of messages. In this case, there's just a single user message.</p>"},{"location":"concepts/writing_prompts/#editor-support","title":"Editor Support","text":"<ul> <li> <p>Inline Errors</p> <p></p> </li> <li> <p>Autocomplete</p> <p></p> </li> </ul>"},{"location":"concepts/writing_prompts/#template-variables","title":"Template Variables","text":"<p>When you call <code>str(prompt)</code> the template will be formatted using the properties of the class that match the template variables. This means that you can define more complex properties through code. This is particularly useful when you want to inject template variables with custom formatting or template variables that depend on multiple attributes.</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    Can you recommend some books on the following topic and genre pairs?\n    {topics_x_genres}\n    \"\"\"\n\n    topics: list[str]\n    genres: list[str]\n\n    @property\n    def topics_x_genres(self) -&gt; str:\n        \"\"\"Returns `topics` as a comma separated list.\"\"\"\n        return \"\\n\".join(\n            [\n                f\"Topic: {topic}, Genre: {genre}\"\n                for topic in self.topics\n                for genre in self.genres\n            ]\n        )\n\n\nprompt = BookRecommendationPrompt(\n    topics=[\"coding\", \"music\"], genres=[\"fiction\", \"fantasy\"]\n)\nprint(prompt)\n#&gt; Can you recommend some books on the following topic and genre pairs?\n#  Topic: coding, Genre: fiction\n#  Topic: coding, Genre: fantasy\n#  Topic: music, Genre: fiction\n#  Topic: music, Genre: fantasy\n</code></pre>"},{"location":"concepts/writing_prompts/#messages","title":"Messages","text":"<p>By default, the <code>BasePrompt</code> class treats the prompt template as a single user message. If you want to specify a list of messages instead, use the message keywords SYSTEM, USER, ASSISTANT, MODEL, or TOOL (depending on what's supported by your choice of provider):</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages())\n</code></pre> <pre><code>[{\"role\": \"system\", \"content\": \"You are the world's greatest librarian\"}, {\"role\": \"user\", \"content\": \"Can you recommend some books on coding?\"}]\n</code></pre> <p>Note</p> <p>This example is using Mirascope base `Message. If you are using a different provider, refer to the provider's documentation on their message roles.</p>"},{"location":"concepts/writing_prompts/#magic-is-optional","title":"Magic is optional","text":"<p>We understand that there are users that do not want to use template string magic. Mirascope allows the user to write the messages array manually, which has the added benefit of accessing functionality that is not yet supported by the template parser (such as Vision support).</p> <pre><code>from mirascope import BasePrompt\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    topic: str\n\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n                        },\n                    },\n                ],\n            },\n        ]\n</code></pre>"},{"location":"concepts/writing_prompts/#integrations-with-providers","title":"Integrations with Providers","text":"<p>The <code>BasePrompt</code> class should be used for providers that are not yet supported by Mirascope. Pass in the messages from the prompt into the LLM provider client messages array.</p> <pre><code>from mirascope import BasePrompt\nfrom some_llm_provider import LLMProvider\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nclient = LLMProvider(api_key=...)\nmessage = client.messages.create(\n    model=\"some-model\",\n    max_tokens=1000,\n    temperature=0.0,\n        stream=False,\n    messages=prompt.messages()\n)\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":"<p>Warning</p> <p>This page is under construction...for now check out our README and Concepts pages.</p>"}]}