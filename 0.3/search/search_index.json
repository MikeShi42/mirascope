{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mirascope","text":"Simplicity through idiomatic syntax \u2192 Faster and more reliable releases Semi-opinionated methods \u2192 Reduced complexity that speeds up development Reliability through validation \u2192 More robust applications with fewer bugs <p>Mirascope is an open-source Python toolkit built on top of Pydantic that makes working with Large Language Models (LLMs):</p> <ul> <li>Durable: Seamlessly customize and extend functionality.</li> <li>Intuitive: Editor support that you expect (e.g.\u00a0autocompletion,\u00a0inline errors)</li> <li>Clean: Pydantic together with our Prompt CLI\u00a0eliminates prompt-related bugs.</li> <li>Integrable: Easily integrate with\u00a0JSON Schema\u00a0and other tools such as\u00a0FastAPI</li> <li>Convenient: Tooling that is\u00a0clean,\u00a0elegant, and\u00a0delightful\u00a0that\u00a0you don't need to maintain.</li> <li>Open: Dedication to building\u00a0open-source tools\u00a0you can use with\u00a0your choice of LLM.</li> </ul> <p>We support any model that works with the OpenAI API, as well as other models such as Gemini.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install Mirascope and start building with LLMs in minutes.</p> <pre><code>pip install mirascope\n</code></pre> <p>You can also install additional optional dependencies if you\u2019re using those features:</p> <pre><code>pip install mirascope[wandb]   # WandbPrompt\npip install mirascope[gemini]  # GeminiPrompt, ...\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>With Mirascope, everything happens with prompts. The idea is to colocate any functionality that may impact the quality of your prompt \u2014 from the template variables to the temperature \u2014 so that you don\u2019t need to worry about code changes external to your prompt affecting quality. For simple use-cases, we find that writing prompts as docstrings provides enhanced readability:</p> <pre><code>from mirascope.openai import OpenAICallParams, OpenAIPrompt\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"Please recommend a {genre} book.\"\"\"\n\n    genre: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-4\",\n        temperature=0.3,\n    )\n\n\nrecommendation = BookRecommendation(genre=\"fantasy\").create()\nprint(recommendation)\n#&gt; I recommend \"The Name of the Wind\" by Patrick Rothfuss. It is...\n</code></pre> <p>If you add any of the OpenAI message roles (SYSTEM, USER, ASSISTANT, TOOL) as keywords to your prompt docstring, they will automatically get parsed into a list of messages:</p> <pre><code>from mirascope.openai import OpenAIPrompt\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nprompt = BookRecommendation(genre=\"fantasy\")\nprint(prompt.messages)\n#&gt; [{'role': 'system', 'content': \"You are the world's greatest librarian.\"},\n#   {'role': 'user', 'content': 'Please recommend a fantasy book.'}]\n</code></pre> <p>If you want to write the messages yourself instead of using the docstring message parsing, there\u2019s nothing stopping you!</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"This is now just a normal docstring.\n\n    Note that you'll lose any functionality dependent on it,\n    such as `template`.\n    \"\"\"\n\n    genre: str\n\n    @property\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"Returns the list of OpenAI prompt messages.\"\"\"\n        return [\n            {\"role\": \"system\", \"content\": \"You are the world's greatest librarian.\"},\n            {\"role\": \"user\", \"content\": f\"Please recommend a {self.genre} book.\"},\n        ]\n\n\nrecommendation = BookRecommendation(genre=\"fantasy\").create()\nprint(recommendation)\n#&gt; I recommend \"The Name of the Wind\" by Patrick Rothfuss. It is...\n</code></pre>"},{"location":"#create-stream-extract","title":"Create, Stream, Extract","text":"<p>Prompt classes such as <code>OpenAIPrompt</code> have three methods for interacting with the LLM:</p> <ul> <li><code>create</code>: Generate a response given a prompt. This will generate raw text unless tools are provided as part of the <code>call_params</code>.</li> <li><code>stream</code>: Same as <code>create</code> except the generated response is returned as a stream of chunks. All chunks together become the full completion.</li> <li><code>extract</code>: Convenience tooling built on top of tools to make it easy to extract structured information given a prompt and schema.</li> </ul>"},{"location":"#using-different-llm-providers","title":"Using Different LLM Providers","text":"<p>The <code>OpenAIPrompt</code> class supports any endpoint that supports the OpenAI API, including (but not limited to) Anyscale, Together, and Groq. Simply update the <code>base_url</code> and set the proper api key in your environment:</p> <pre><code>import os\n\nfrom mirascope.openai import OpenAICallParams, OpenAIPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"TOGETHER_API_KEY\"\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"Please recommend a {genre} book.\"\"\"\n\n    genre: str\n\n    call_params = OpenAICallParams(\n            model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n            base_url=\"https://api.together.xyz/v1\",\n    )\n\n\nrecommendation = BookRecommendation(genre=\"fantasy\").create()\n</code></pre> <p>We also support other providers such as Gemini.</p>"},{"location":"#dive-deeper","title":"Dive Deeper","text":"<ul> <li>Learn why colocation is so important and how combining it with the Mirascope CLI makes engineering better prompts easy.</li> <li>Check out how to write better prompts using Mirascope.</li> <li>Become a master of extracting structured information using LLMs.</li> <li>Take a look at how Mirascope makes using tools (function calling) simple and clean.</li> <li>The API Reference contains full details on all classes, methods, functions, etc.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>You can find more usage examples in our examples directory, such as how to easily integrate with FastAPI.</p> <p>We also have more detailed walkthroughs in our Cookbook docs section. Each cookbook has corresponding full code examples in the cookbook directory.</p>"},{"location":"#whats-next","title":"What\u2019s Next?","text":"<p>We have a lot on our minds for what to build next, but here are a few things (in no particular order) that come to mind first:</p> <ul> <li> Extracting structured information using LLMs</li> <li> Agents</li> <li> Support for more LLM providers:<ul> <li> Claude</li> <li> Mistral</li> <li> HuggingFace</li> </ul> </li> <li> Integrations:<ul> <li> Weights &amp; Biases</li> <li> LangSmith</li> <li> \u2026 tell us what you\u2019d like integrated!</li> </ul> </li> <li> Evaluating prompts and their quality by version</li> <li> Additional docstring parsing for more complex messages</li> </ul>"},{"location":"#versioning","title":"Versioning","text":"<p>Mirascope uses\u00a0Semantic Versioning.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the\u00a0MIT License.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use poetry as our package and dependency manager.</p> <p>To create a virtual environment for development, run the following in your shell:</p> <pre><code>pip install poetry\npoetry shell\npoetry install --with dev\n</code></pre> <p>Simply use <code>exit</code> to deactivate the environment. The next time you call <code>poetry shell</code> the environment will already be setup and ready to go.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout dev\ngit pull upstream dev\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run mypy .\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR. Make sure to fill in a detailed title and description. Submit your PR for review.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p> <p>Furthermore, we have a full coverage requirement, so all incoming code must have 100% coverage. This policy ensures that every line of code is run in our tests. However, while achieving full coverage is essential, it is not sufficient on its own. Coverage metrics ensure code execution but do not guarantee correctness under all conditions. Make sure to stress test beyond coverage to reduce bugs.</p> <p>We use a Codecov dashboard to monitor and track our coverage.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>mypy</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>mypy</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>mypy</code> errors.</p>"},{"location":"HELP/","title":"How to help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li> <p>Follow us on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow William Bakst on Twitter/X</p> <ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with William Bakst on LinkedIn</p> <ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how Mirascope has helped you and how you are using it.</p> </li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues / Discussions.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"api/","title":"mirascope api","text":"<p>mirascope package.</p>"},{"location":"api/enums/","title":"enums","text":"<p>Enum Classes for mirascope.</p>"},{"location":"api/enums/#mirascope.enums.MirascopeCommand","title":"<code>MirascopeCommand</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>CLI commands to be executed.</p> <ul> <li>ADD: save a modified prompt to the <code>versions/</code> folder.</li> <li>USE: load a specific version of the prompt from <code>versions/</code> as the current prompt.</li> <li>STATUS: display if any changes have been made to prompts, and if a prompt is     specified, displays changes for only said prompt.</li> <li>INIT: initializes the necessary folders for prompt versioning with CLI.</li> </ul> Source code in <code>mirascope/enums.py</code> <pre><code>class MirascopeCommand(_Enum):\n    \"\"\"CLI commands to be executed.\n\n    - ADD: save a modified prompt to the `versions/` folder.\n    - USE: load a specific version of the prompt from `versions/` as the current prompt.\n    - STATUS: display if any changes have been made to prompts, and if a prompt is\n        specified, displays changes for only said prompt.\n    - INIT: initializes the necessary folders for prompt versioning with CLI.\n    \"\"\"\n\n    ADD = \"add\"\n    USE = \"use\"\n    STATUS = \"status\"\n    INIT = \"init\"\n</code></pre>"},{"location":"api/prompts/","title":"prompts (deprecated)","text":"<p>A class for better prompting.</p>"},{"location":"api/prompts/#mirascope.prompts.Prompt","title":"<code>Prompt</code>","text":"<p>             Bases: <code>BasePrompt</code></p> <p>(DEPRECATED): Use <code>BasePrompt</code> or specific prompt (e.g. <code>OpenAIPrompt</code>) instead.</p> <p>A Pydantic model for prompts.</p> <p>Example:</p> <pre><code>from mirascope import Prompt, BaseCallParams\n\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    I've recently read the following books: {titles_in_quotes}.\n    What should I read next?\n    \"\"\"\n\n    book_titles: list[str]\n\n    call_params = BaseCallParams(model=\"gpt-3.5-turbo-0125\")\n\n    @property\n    def titles_in_quotes(self) -&gt; str:\n        \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n        return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\nprompt = BookRecommendationPrompt(\n    book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n)\n\nprint(BookRecommendationPrompt.template())\n#&gt; I've recently read the following books: {titles_in_quotes}. What should I read\n#  next?\n\nprint(str(prompt))\n#&gt; I've recently read the following books: \"The Name of the Wind\", \"The Lord of the\n#  Rings\". What should I read next?\n\nprint(prompt.messages)\n#&gt; [{\n#     'role': 'user',\n#     'content': 'I've recently read the following books: \"The Name of the Wind\",\n#                \"The Lord of the Rings\". What should I read next?',\n#  }]\n</code></pre> Source code in <code>mirascope/prompts.py</code> <pre><code>class Prompt(BasePrompt):\n    '''(DEPRECATED): Use `BasePrompt` or specific prompt (e.g. `OpenAIPrompt`) instead.\n\n    A Pydantic model for prompts.\n\n    Example:\n\n    ```python\n    from mirascope import Prompt, BaseCallParams\n\n\n    class BookRecommendationPrompt(Prompt):\n        \"\"\"\n        I've recently read the following books: {titles_in_quotes}.\n        What should I read next?\n        \"\"\"\n\n        book_titles: list[str]\n\n        call_params = BaseCallParams(model=\"gpt-3.5-turbo-0125\")\n\n        @property\n        def titles_in_quotes(self) -&gt; str:\n            \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n            return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\n    prompt = BookRecommendationPrompt(\n        book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n    )\n\n    print(BookRecommendationPrompt.template())\n    #&gt; I've recently read the following books: {titles_in_quotes}. What should I read\n    #  next?\n\n    print(str(prompt))\n    #&gt; I've recently read the following books: \"The Name of the Wind\", \"The Lord of the\n    #  Rings\". What should I read next?\n\n    print(prompt.messages)\n    #&gt; [{\n    #     'role': 'user',\n    #     'content': 'I\\'ve recently read the following books: \"The Name of the Wind\",\n    #                \"The Lord of the Rings\". What should I read next?',\n    #  }]\n    ```\n    '''\n\n    call_params: ClassVar[BaseCallParams] = BaseCallParams(model=\"gpt-3.5-turbo-0125\")\n    tags: ClassVar[list[str]] = []\n\n    def __init__(self, **data: Any):\n        \"\"\"Initializes the prompt with the given data.\"\"\"\n        warnings.warn(\n            \"The `Prompt` class is deprecated. Instead use either `BasePrompt` or a \"\n            \"specific prompt (e.g. `OpenAIPrompt`) instead; version&gt;=0.3.0\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        super().__init__(**data)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initializes the prompt with the given data.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def __init__(self, **data: Any):\n    \"\"\"Initializes the prompt with the given data.\"\"\"\n    warnings.warn(\n        \"The `Prompt` class is deprecated. Instead use either `BasePrompt` or a \"\n        \"specific prompt (e.g. `OpenAIPrompt`) instead; version&gt;=0.3.0\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    super().__init__(**data)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.messages","title":"<code>messages(cls)</code>","text":"<p>A decorator for updating the <code>messages</code> class attribute of a <code>Prompt</code>.</p> <p>Adding this decorator to a <code>Prompt</code> updates the <code>messages</code> class attribute to parse the docstring as a list of messages. Each message is a tuple containing the role and the content. The docstring should have the following format:</p> <pre><code>&lt;role&gt;:\n&lt;content&gt;\n</code></pre> <p>Example:</p> <pre><code>from mirascope import Prompt, messages\n\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    I recently read {book_title}. What should I read next?\n    \"\"\"\n\n    book_title: [str]\n\n\nprompt = BookRecommendationPrompt(book_title=\"The Name of the Wind\")\n\nprint(prompt.messages)\n#&gt; [\n#    {\n#      'role': 'system',\n#      'content': \"You are the world's greatest librarian.\"\n#    },\n#    {\n#      'role': 'user',\n#      'content': \"I recently read The Name of the  Wind. What should I read next?'\n#    },\n#  ]\n</code></pre> <p>This decorator currently supports the SYSTEM, USER, ASSISTANT, and TOOL roles.</p> <p>Returns:</p> Type Description <code>Type[T]</code> <p>The decorated class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the docstring is empty.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def messages(cls: Type[T]) -&gt; Type[T]:\n    '''A decorator for updating the `messages` class attribute of a `Prompt`.\n\n    Adding this decorator to a `Prompt` updates the `messages` class attribute\n    to parse the docstring as a list of messages. Each message is a tuple containing\n    the role and the content. The docstring should have the following format:\n\n        &lt;role&gt;:\n        &lt;content&gt;\n\n    Example:\n\n    ```python\n    from mirascope import Prompt, messages\n\n\n    @messages\n    class BookRecommendationPrompt(Prompt):\n        \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        I recently read {book_title}. What should I read next?\n        \"\"\"\n\n        book_title: [str]\n\n\n    prompt = BookRecommendationPrompt(book_title=\"The Name of the Wind\")\n\n    print(prompt.messages)\n    #&gt; [\n    #    {\n    #      'role': 'system',\n    #      'content': \"You are the world's greatest librarian.\"\n    #    },\n    #    {\n    #      'role': 'user',\n    #      'content': \"I recently read The Name of the  Wind. What should I read next?'\n    #    },\n    #  ]\n    ```\n\n    This decorator currently supports the SYSTEM, USER, ASSISTANT, and TOOL roles.\n\n    Returns:\n        The decorated class.\n\n    Raises:\n        ValueError: If the docstring is empty.\n    '''\n    warnings.warn(\n        \"The `messages` decorator is deprecated and no longer necessary. You can write \"\n        \"a `messages` style docstring without the decorator; version&gt;=0.3.0\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return cls\n</code></pre>"},{"location":"api/base/","title":"base","text":"<p>Base modules for the mirascope library.</p>"},{"location":"api/base/prompt/","title":"base.prompt","text":"<p>A class for better prompting.</p>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for prompts.</p> <p>Example:</p> <pre><code>from mirascope import BasePrompt, BaseCallParams\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    I've recently read the following books: {titles_in_quotes}.\n    What should I read next?\n    \"\"\"\n\n    book_titles: list[str]\n\n    call_params = BaseCallParams(model=\"gpt-3.5-turbo-0125\")\n\n    @property\n    def titles_in_quotes(self) -&gt; str:\n        \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n        return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\nprompt = BookRecommendationPrompt(\n    book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n)\n\nprint(BookRecommendationPrompt.template())\n#&gt; I've recently read the following books: {titles_in_quotes}. What should I read\n#  next?\n\nprint(str(prompt))\n#&gt; I've recently read the following books: \"The Name of the Wind\", \"The Lord of the\n#  Rings\". What should I read next?\n\nprint(prompt.messages)\n#&gt; [{\n#     'role': 'user',\n#     'content': 'I've recently read the following books: \"The Name of the Wind\",\n#                \"The Lord of the Rings\". What should I read next?',\n#  }]\n</code></pre> Source code in <code>mirascope/base/prompt.py</code> <pre><code>class BasePrompt(BaseModel):\n    '''A Pydantic model for prompts.\n\n    Example:\n\n    ```python\n    from mirascope import BasePrompt, BaseCallParams\n\n\n    class BookRecommendationPrompt(BasePrompt):\n        \"\"\"\n        I've recently read the following books: {titles_in_quotes}.\n        What should I read next?\n        \"\"\"\n\n        book_titles: list[str]\n\n        call_params = BaseCallParams(model=\"gpt-3.5-turbo-0125\")\n\n        @property\n        def titles_in_quotes(self) -&gt; str:\n            \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n            return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\n    prompt = BookRecommendationPrompt(\n        book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n    )\n\n    print(BookRecommendationPrompt.template())\n    #&gt; I've recently read the following books: {titles_in_quotes}. What should I read\n    #  next?\n\n    print(str(prompt))\n    #&gt; I've recently read the following books: \"The Name of the Wind\", \"The Lord of the\n    #  Rings\". What should I read next?\n\n    print(prompt.messages)\n    #&gt; [{\n    #     'role': 'user',\n    #     'content': 'I\\'ve recently read the following books: \"The Name of the Wind\",\n    #                \"The Lord of the Rings\". What should I read next?',\n    #  }]\n    ```\n    '''\n\n    call_params: ClassVar[BaseCallParams] = BaseCallParams(model=\"gpt-3.5-turbo-0125\")\n    tags: ClassVar[list[str]] = []\n\n    @classmethod\n    def template(cls) -&gt; str:\n        \"\"\"Custom parsing functionality for docstring prompt.\n\n        This function is the first step in formatting the prompt template docstring.\n        For the default `BasePrompt`, this function dedents the docstring and replaces\n        all repeated sequences of newlines with one fewer newline character. This\n        enables writing blocks of text instead of really long single lines. To include\n        any number of newline characters, simply include one extra.\n\n        Raises:\n            ValueError: If the class docstring is empty.\n        \"\"\"\n        if cls.__doc__ is None:\n            raise ValueError(\"`BasePrompt` must have a prompt template docstring.\")\n\n        return re.sub(\n            \"(\\n+)\",\n            lambda x: x.group(0)[:-1] if len(x.group(0)) &gt; 1 else \" \",\n            dedent(cls.__doc__).strip(\"\\n\"),\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the docstring prompt template formatted with template variables.\"\"\"\n        return format_template(self, self.template())\n\n    @property\n    def messages(self) -&gt; Union[list[Message], Any]:\n        \"\"\"Returns the docstring as a list of messages.\"\"\"\n        message_param_map = {\n            \"system\": SystemMessage,\n            \"user\": UserMessage,\n            \"assistant\": AssistantMessage,\n            \"tool\": ToolMessage,\n        }\n        messages = []\n        for match in re.finditer(\n            r\"(SYSTEM|USER|ASSISTANT|TOOL): \"\n            r\"((.|\\n)+?)(?=\\n(SYSTEM|USER|ASSISTANT|TOOL):|\\Z)\",\n            self.template(),\n        ):\n            role = match.group(1).lower()\n            content = format_template(self, match.group(2))\n            messages.append(message_param_map[role](role=role, content=content))\n        if len(messages) == 0:\n            messages.append(UserMessage(role=\"user\", content=str(self)))\n        return messages\n\n    def dump(self, completion: Optional[dict[str, Any]] = None) -&gt; dict[str, Any]:\n        \"\"\"Dumps the prompt template to a dictionary.\"\"\"\n        prompt_dict: dict[str, Any] = {\n            \"template\": self.template(),\n            \"inputs\": self.model_dump(),\n            \"tags\": self.tags,\n            \"call_params\": {\n                key: value\n                for key, value in self.call_params.model_dump().items()\n                if value is not None\n            },\n        }\n        if completion is not None:\n            return prompt_dict | completion\n        return prompt_dict\n\n    def save(self, filepath: str):\n        \"\"\"Saves the prompt to the given filepath.\"\"\"\n        with open(filepath, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, filepath: str) -&gt; BasePrompt:\n        \"\"\"Loads the prompt from the given filepath.\"\"\"\n        with open(filepath, \"rb\") as f:\n            return pickle.load(f)\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt.messages","title":"<code>messages: Union[list[Message], Any]</code>  <code>property</code>","text":"<p>Returns the docstring as a list of messages.</p>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the docstring prompt template formatted with template variables.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the docstring prompt template formatted with template variables.\"\"\"\n    return format_template(self, self.template())\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt.dump","title":"<code>dump(completion=None)</code>","text":"<p>Dumps the prompt template to a dictionary.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>def dump(self, completion: Optional[dict[str, Any]] = None) -&gt; dict[str, Any]:\n    \"\"\"Dumps the prompt template to a dictionary.\"\"\"\n    prompt_dict: dict[str, Any] = {\n        \"template\": self.template(),\n        \"inputs\": self.model_dump(),\n        \"tags\": self.tags,\n        \"call_params\": {\n            key: value\n            for key, value in self.call_params.model_dump().items()\n            if value is not None\n        },\n    }\n    if completion is not None:\n        return prompt_dict | completion\n    return prompt_dict\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Loads the prompt from the given filepath.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str) -&gt; BasePrompt:\n    \"\"\"Loads the prompt from the given filepath.\"\"\"\n    with open(filepath, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt.save","title":"<code>save(filepath)</code>","text":"<p>Saves the prompt to the given filepath.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>def save(self, filepath: str):\n    \"\"\"Saves the prompt to the given filepath.\"\"\"\n    with open(filepath, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.BasePrompt.template","title":"<code>template()</code>  <code>classmethod</code>","text":"<p>Custom parsing functionality for docstring prompt.</p> <p>This function is the first step in formatting the prompt template docstring. For the default <code>BasePrompt</code>, this function dedents the docstring and replaces all repeated sequences of newlines with one fewer newline character. This enables writing blocks of text instead of really long single lines. To include any number of newline characters, simply include one extra.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class docstring is empty.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>@classmethod\ndef template(cls) -&gt; str:\n    \"\"\"Custom parsing functionality for docstring prompt.\n\n    This function is the first step in formatting the prompt template docstring.\n    For the default `BasePrompt`, this function dedents the docstring and replaces\n    all repeated sequences of newlines with one fewer newline character. This\n    enables writing blocks of text instead of really long single lines. To include\n    any number of newline characters, simply include one extra.\n\n    Raises:\n        ValueError: If the class docstring is empty.\n    \"\"\"\n    if cls.__doc__ is None:\n        raise ValueError(\"`BasePrompt` must have a prompt template docstring.\")\n\n    return re.sub(\n        \"(\\n+)\",\n        lambda x: x.group(0)[:-1] if len(x.group(0)) &gt; 1 else \" \",\n        dedent(cls.__doc__).strip(\"\\n\"),\n    )\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.format_template","title":"<code>format_template(prompt, template)</code>","text":"<p>Formats the template with the prompt's attributes.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>def format_template(prompt: BasePrompt, template: str) -&gt; str:\n    \"\"\"Formats the template with the prompt's attributes.\"\"\"\n    template_vars = [\n        var for _, var, _, _ in Formatter().parse(template) if var is not None\n    ]\n    return template.format(**{var: getattr(prompt, var) for var in template_vars})\n</code></pre>"},{"location":"api/base/prompt/#mirascope.base.prompt.tags","title":"<code>tags(args)</code>","text":"<p>A decorator for adding tags to a <code>BasePrompt</code>.</p> <p>Adding this decorator to a <code>BasePrompt</code> updates the <code>_tags</code> class attribute to the given value. This is useful for adding metadata to a <code>BasePrompt</code> that can be used for logging or filtering.</p> <p>Example:</p> <pre><code>from mirascope import BasePrompt, tags\n\n\n@tags([\"book_recommendation\", \"entertainment\"])\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    I've recently read this book: {book_title}.\n    What should I read next?\n    \"\"\"\n\n    book_title: [str]\n\nprint(BookRecommendationPrompt.dump()[\"tags\"])\n#&gt; ['book_recommendation', 'entertainment']\n</code></pre> <p>Returns:</p> Type Description <code>Callable[[Type[BasePromptT]], Type[BasePromptT]]</code> <p>The decorated class with <code>_tags</code> class attribute set.</p> Source code in <code>mirascope/base/prompt.py</code> <pre><code>def tags(args: list[str]) -&gt; Callable[[Type[BasePromptT]], Type[BasePromptT]]:\n    '''A decorator for adding tags to a `BasePrompt`.\n\n    Adding this decorator to a `BasePrompt` updates the `_tags` class attribute to the given\n    value. This is useful for adding metadata to a `BasePrompt` that can be used for logging\n    or filtering.\n\n    Example:\n\n    ```python\n    from mirascope import BasePrompt, tags\n\n\n    @tags([\"book_recommendation\", \"entertainment\"])\n    class BookRecommendationPrompt(BasePrompt):\n        \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        I've recently read this book: {book_title}.\n        What should I read next?\n        \"\"\"\n\n        book_title: [str]\n\n    print(BookRecommendationPrompt.dump()[\"tags\"])\n    #&gt; ['book_recommendation', 'entertainment']\n    ```\n\n    Returns:\n        The decorated class with `_tags` class attribute set.\n    '''\n\n    def tags_fn(model_class: Type[BasePromptT]) -&gt; Type[BasePromptT]:\n        \"\"\"Updates the `_tags` class attribute to the given value.\"\"\"\n        setattr(model_class, \"tags\", args)\n        return model_class\n\n    return tags_fn\n</code></pre>"},{"location":"api/base/tools/","title":"base.tools","text":"<p>A base tool class for easy use of tools with prompts.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool","title":"<code>BaseTool</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A base class for easy use of tools with prompts.</p> <p><code>BaseTool</code> is an abstract class interface and should not be used directly.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>class BaseTool(BaseModel, ABC):\n    \"\"\"A base class for easy use of tools with prompts.\n\n    `BaseTool` is an abstract class interface and should not be used directly.\n    \"\"\"\n\n    @property\n    def args(self) -&gt; dict[str, Any]:\n        \"\"\"The arguments of the tool as a dictionary.\"\"\"\n        return self.model_dump(exclude={\"tool_call\"})\n\n    @property\n    def fn(self) -&gt; Callable:\n        \"\"\"Returns the function that the tool describes.\"\"\"\n        raise RuntimeError(\"Tool does not have an attached function.\")\n\n    @classmethod\n    def tool_schema(cls) -&gt; Any:\n        \"\"\"Constructs a tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        if \"description\" not in model_schema:\n            raise ValueError(\"Tool must have a docstring description.\")\n\n        fn = {\n            \"name\": model_schema[\"title\"],\n            \"description\": model_schema[\"description\"],\n        }\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = {\n                \"type\": \"object\",\n                \"properties\": {\n                    prop: {key: value for key, value in prop_schema.items()}\n                    for prop, prop_schema in model_schema[\"properties\"].items()\n                    if prop != \"tool_call\"\n                },\n                \"required\": [\n                    prop for prop in model_schema[\"required\"] if prop != \"tool_call\"\n                ]\n                if \"required\" in model_schema\n                else [],\n                \"$defs\": {\n                    key: value\n                    for key, value in model_schema[\"$defs\"].items()\n                    if key != \"ChatCompletionMessageToolCall\" and key != \"Function\"\n                }\n                if \"$defs\" in model_schema\n                else {},\n            }\n        return fn\n\n    @classmethod\n    @abstractmethod\n    def from_tool_call(cls, tool_call: Any) -&gt; BaseTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.args","title":"<code>args: dict[str, Any]</code>  <code>property</code>","text":"<p>The arguments of the tool as a dictionary.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.fn","title":"<code>fn: Callable</code>  <code>property</code>","text":"<p>Returns the function that the tool describes.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_tool_call(cls, tool_call: Any) -&gt; BaseTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Any:\n    \"\"\"Constructs a tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    if \"description\" not in model_schema:\n        raise ValueError(\"Tool must have a docstring description.\")\n\n    fn = {\n        \"name\": model_schema[\"title\"],\n        \"description\": model_schema[\"description\"],\n    }\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = {\n            \"type\": \"object\",\n            \"properties\": {\n                prop: {key: value for key, value in prop_schema.items()}\n                for prop, prop_schema in model_schema[\"properties\"].items()\n                if prop != \"tool_call\"\n            },\n            \"required\": [\n                prop for prop in model_schema[\"required\"] if prop != \"tool_call\"\n            ]\n            if \"required\" in model_schema\n            else [],\n            \"$defs\": {\n                key: value\n                for key, value in model_schema[\"$defs\"].items()\n                if key != \"ChatCompletionMessageToolCall\" and key != \"Function\"\n            }\n            if \"$defs\" in model_schema\n            else {},\n        }\n    return fn\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.convert_base_model_to_tool","title":"<code>convert_base_model_to_tool(schema, base)</code>","text":"<p>Converts a <code>BaseModel</code> schema to a <code>BaseToolT</code> type.</p> <p>By adding a docstring (if needed) and passing on fields and field information in dictionary format, a Pydantic <code>BaseModel</code> can be converted into an <code>BaseToolT</code> for performing extraction.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModel]</code> <p>The <code>BaseModel</code> schema to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseToolT]</code> <p>The constructed <code>BaseToolT</code> type.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>def convert_base_model_to_tool(\n    schema: Type[BaseModel], base: Type[BaseToolT]\n) -&gt; Type[BaseToolT]:\n    \"\"\"Converts a `BaseModel` schema to a `BaseToolT` type.\n\n    By adding a docstring (if needed) and passing on fields and field information in\n    dictionary format, a Pydantic `BaseModel` can be converted into an `BaseToolT` for\n    performing extraction.\n\n    Args:\n        schema: The `BaseModel` schema to convert.\n\n    Returns:\n        The constructed `BaseToolT` type.\n    \"\"\"\n    field_definitions = {\n        field_name: (field_info.annotation, field_info)\n        for field_name, field_info in schema.model_fields.items()\n    }\n    return create_model(\n        f\"{schema.__name__}Tool\",\n        __base__=base,\n        __doc__=schema.__doc__\n        if schema.__doc__\n        else INTERNAL_DOC.format(name=schema.__name__),\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.convert_base_type_to_tool","title":"<code>convert_base_type_to_tool(schema, base)</code>","text":"<p>Converts a <code>BaseType</code> to a <code>BaseToolT</code> type.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>def convert_base_type_to_tool(\n    schema: Type[T], base: Type[BaseToolT]\n) -&gt; Type[BaseToolT]:\n    \"\"\"Converts a `BaseType` to a `BaseToolT` type.\"\"\"\n    return create_model(\n        f\"{schema.__name__[0].upper()}{schema.__name__[1:]}Tool\",\n        __base__=base,\n        __doc__=INTERNAL_DOC.format(name=schema.__name__),\n        value=(schema, ...),\n    )\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.convert_function_to_tool","title":"<code>convert_function_to_tool(fn, base)</code>","text":"<p>Constructs a <code>BaseToolT</code> type from the given function.</p> <p>This method expects all function parameters to be properly documented in identical order with identical variable names, as well as descriptions of each parameter. Errors will be raised if any of these conditions are not met.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseToolT]</code> <p>The constructed <code>BaseToolT</code> type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the given function doesn't have a docstring.</p> <code>ValueError</code> <p>if the given function's parameters don't have type annotations.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but the name doesn't match the docstring's parameter name.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but doesn't have a dosctring description.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>def convert_function_to_tool(fn: Callable, base: Type[BaseToolT]) -&gt; Type[BaseToolT]:\n    \"\"\"Constructs a `BaseToolT` type from the given function.\n\n    This method expects all function parameters to be properly documented in identical\n    order with identical variable names, as well as descriptions of each parameter.\n    Errors will be raised if any of these conditions are not met.\n\n    Args:\n        fn: The function to convert.\n\n    Returns:\n        The constructed `BaseToolT` type.\n\n    Raises:\n        ValueError: if the given function doesn't have a docstring.\n        ValueError: if the given function's parameters don't have type annotations.\n        ValueError: if a given function's parameter is in the docstring args section but\n            the name doesn't match the docstring's parameter name.\n        ValueError: if a given function's parameter is in the docstring args section but\n            doesn't have a dosctring description.\n    \"\"\"\n    if not fn.__doc__:\n        raise ValueError(\"Function must have a docstring.\")\n\n    docstring = parse(fn.__doc__)\n\n    doc = \"\"\n    if docstring.short_description:\n        doc = docstring.short_description\n    if docstring.long_description:\n        doc += \"\\n\\n\" + docstring.long_description\n\n    field_definitions = {}\n    hints = get_type_hints(fn)\n    for i, parameter in enumerate(signature(fn).parameters.values()):\n        if parameter.name == \"self\" or parameter.name == \"cls\":\n            continue\n        if parameter.annotation == Parameter.empty:\n            raise ValueError(\"All parameters must have a type annotation.\")\n\n        docstring_description = None\n        if i &lt; len(docstring.params):\n            docstring_param = docstring.params[i]\n            if docstring_param.arg_name != parameter.name:\n                raise ValueError(\n                    f\"Function parameter name {parameter.name} does not match docstring \"\n                    f\"parameter name {docstring_param.arg_name}. Make sure that the \"\n                    \"parameter names match exactly.\"\n                )\n            if not docstring_param.description:\n                raise ValueError(\"All parameters must have a description.\")\n            docstring_description = docstring_param.description\n\n        field_info = FieldInfo(annotation=hints[parameter.name])\n        if parameter.default != Parameter.empty:\n            field_info.default = parameter.default\n        if docstring_description:  # we check falsy here because this comes from docstr\n            field_info.description = docstring_description\n\n        param_name = parameter.name\n        if param_name.startswith(\"model_\"):  # model_ is a BaseModel reserved namespace\n            param_name = \"aliased_\" + param_name\n            field_info.alias = parameter.name\n            field_info.validation_alias = parameter.name\n            field_info.serialization_alias = parameter.name\n\n        field_definitions[param_name] = (\n            hints[parameter.name],\n            field_info,\n        )\n\n    return create_model(\n        \"\".join(word.title() for word in fn.__name__.split(\"_\")),\n        __base__=tool_fn(fn)(base),\n        __doc__=doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.tool_fn","title":"<code>tool_fn(fn)</code>","text":"<p>A decorator for adding a function to a tool class.</p> <p>Adding this decorator will add an <code>fn</code> property to the tool class that returns the function that the tool describes. This is convenient for calling the function given an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to add to the tool class.</p> required <p>Returns:</p> Type Description <code>Callable[[Type[BaseToolT]], Type[BaseToolT]]</code> <p>The decorated tool class.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>def tool_fn(fn: Callable) -&gt; Callable[[Type[BaseToolT]], Type[BaseToolT]]:\n    \"\"\"A decorator for adding a function to a tool class.\n\n    Adding this decorator will add an `fn` property to the tool class that returns the\n    function that the tool describes. This is convenient for calling the function given\n    an instance of the tool.\n\n    Args:\n        fn: The function to add to the tool class.\n\n    Returns:\n        The decorated tool class.\n    \"\"\"\n\n    def decorator(cls: Type[BaseToolT]) -&gt; Type[BaseToolT]:\n        \"\"\"A decorator for adding a function to a tool class.\"\"\"\n        setattr(cls, \"fn\", property(lambda self: fn))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/base/types/","title":"base.types","text":"<p>Types for working with Mirascope prompts.</p>"},{"location":"api/base/types/#mirascope.base.types.AssistantMessage","title":"<code>AssistantMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>assistant</code> role.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Optional[str]</code> <p>The contents of the message.</p> <code>role</code> <code>Required[Literal['assistant']]</code> <p>The role of the messages author, in this case <code>assistant</code>.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class AssistantMessage(TypedDict, total=False):\n    \"\"\"A message with the `assistant` role.\n\n    Attributes:\n        content: The contents of the message.\n        role: The role of the messages author, in this case `assistant`.\n    \"\"\"\n\n    content: Optional[str]\n    role: Required[Literal[\"assistant\"]]\n    tool_calls: Iterable\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallParams","title":"<code>BaseCallParams</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The base parameters for calling a model with a prompt.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallParams(BaseModel):\n    \"\"\"The base parameters for calling a model with a prompt.\"\"\"\n\n    model: str\n    tools: Optional[list[Union[Callable, Type[BaseTool]]]] = None\n\n    @property\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the keyword argument call parameters as a dictioanry.\"\"\"\n        return self.model_dump(exclude={\"tools\"})\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallParams.kwargs","title":"<code>kwargs: dict[str, Any]</code>  <code>property</code>","text":"<p>Returns the keyword argument call parameters as a dictioanry.</p>"},{"location":"api/base/types/#mirascope.base.types.SystemMessage","title":"<code>SystemMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>system</code> role.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> <code>role</code> <code>Required[Literal['system']]</code> <p>The role of the messages author, in this case <code>system</code>.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class SystemMessage(TypedDict, total=False):\n    \"\"\"A message with the `system` role.\n\n    Attributes:\n        content: The contents of the message.\n        role: The role of the messages author, in this case `system`.\n    \"\"\"\n\n    content: Required[str]\n    role: Required[Literal[\"system\"]]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>tool</code> role.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> <code>role</code> <code>Required[Literal['system']]</code> <p>The role of the messages author, in this case <code>tool</code>.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class ToolMessage(TypedDict, total=False):\n    \"\"\"A message with the `tool` role.\n\n    Attributes:\n        content: The contents of the message.\n        role: The role of the messages author, in this case `tool`.\n    \"\"\"\n\n    content: Required[str]\n    role: Required[Literal[\"system\"]]\n    tool_call_id: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.UserMessage","title":"<code>UserMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>user</code> role.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Required[Union[str, Iterable]]</code> <p>The contents of the message.</p> <code>role</code> <code>Required[Literal['user']]</code> <p>The role of the messages author, in this case <code>user</code>.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class UserMessage(TypedDict, total=False):\n    \"\"\"A message with the `user` role.\n\n    Attributes:\n        content: The contents of the message.\n        role: The role of the messages author, in this case `user`.\n    \"\"\"\n\n    content: Required[Union[str, Iterable]]\n    role: Required[Literal[\"user\"]]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.is_base_type","title":"<code>is_base_type(type_)</code>","text":"<p>Check if a type is a base type.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>def is_base_type(type_: Any) -&gt; bool:\n    \"\"\"Check if a type is a base type.\"\"\"\n    if isclass(type_) and issubclass(type_, Enum):\n        return True\n    base_types = {str, int, float, bool, list, set, tuple}\n    if type_ in base_types or get_origin(type_) in base_types.union(\n        {Literal, Union, Annotated}\n    ):\n        return True\n    return False\n</code></pre>"},{"location":"api/cli/","title":"cli","text":"<p>This module contains all functionality related to the Mirascope CLI.</p>"},{"location":"api/cli/commands/","title":"cli.commands","text":"<p>The Mirascope CLI commands module</p>"},{"location":"api/cli/constants/","title":"cli.constants","text":"<p>Constants for Mirascope CLI.</p>"},{"location":"api/cli/generic/","title":"cli.generic","text":"<p>This package contains generic templates that are used to initialize a mirascope project.</p>"},{"location":"api/cli/schemas/","title":"cli.schemas","text":"<p>Contains the schema for files created by the mirascope cli.</p>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.MirascopeCliVariables","title":"<code>MirascopeCliVariables</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Prompt version variables used internally by mirascope.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class MirascopeCliVariables(BaseModel):\n    \"\"\"Prompt version variables used internally by mirascope.\"\"\"\n\n    prev_revision_id: Optional[str] = Field(default=None)\n    revision_id: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.MirascopeSettings","title":"<code>MirascopeSettings</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the user's mirascope settings.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class MirascopeSettings(BaseModel):\n    \"\"\"Model for the user's mirascope settings.\"\"\"\n\n    mirascope_location: str\n    versions_location: str\n    prompts_location: str\n    version_file_name: str\n    format_command: Optional[str] = None\n    auto_tag: Optional[bool] = None\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.VersionTextFile","title":"<code>VersionTextFile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the version text file.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class VersionTextFile(BaseModel):\n    \"\"\"Model for the version text file.\"\"\"\n\n    current_revision: Optional[str] = Field(default=None)\n    latest_revision: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/cli/utils/","title":"cli.utils","text":"<p>Utility functions for the mirascope library.</p>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer","title":"<code>PromptAnalyzer</code>","text":"<p>             Bases: <code>NodeVisitor</code></p> <p>Utility class for analyzing a Mirascope prompt file.</p> <p>The call to <code>ast.parse()</code> returns Python code as an AST, whereby each visitor method will be called for the corresponding nodes in the AST via <code>NodeVisitor.visit()</code>.</p> <p>Example:</p> <pre><code>analyzer = PromptAnalyzer()\ntree = ast.parse(file.read())\nanalyzer.visit(tree)\n</code></pre> Source code in <code>mirascope/cli/utils.py</code> <pre><code>class PromptAnalyzer(ast.NodeVisitor):\n    \"\"\"Utility class for analyzing a Mirascope prompt file.\n\n    The call to `ast.parse()` returns Python code as an AST, whereby each visitor method\n    will be called for the corresponding nodes in the AST via `NodeVisitor.visit()`.\n\n    Example:\n\n    ```python\n    analyzer = PromptAnalyzer()\n    tree = ast.parse(file.read())\n    analyzer.visit(tree)\n    ```\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the PromptAnalyzer.\"\"\"\n        self.imports: list[tuple[str, Optional[str]]] = []\n        self.from_imports: list[tuple[str, str, Optional[str]]] = []\n        self.variables: dict[str, Any] = {}\n        self.classes: list[ClassInfo] = []\n        self.functions: list[FunctionInfo] = []\n        self.comments: str = \"\"\n\n    def visit_Import(self, node) -&gt; None:\n        \"\"\"Extracts imports from the given node.\"\"\"\n        for alias in node.names:\n            self.imports.append((alias.name, alias.asname))\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node) -&gt; None:\n        \"\"\"Extracts from imports from the given node.\"\"\"\n        for alias in node.names:\n            self.from_imports.append((node.module, alias.name, alias.asname))\n        self.generic_visit(node)\n\n    def visit_Assign(self, node) -&gt; None:\n        \"\"\"Extracts variables from the given node.\"\"\"\n        target = node.targets[0]\n        if isinstance(target, ast.Name):\n            self.variables[target.id] = ast.unparse(node.value)\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node) -&gt; None:\n        \"\"\"Extracts classes from the given node.\"\"\"\n        class_info = ClassInfo(\n            name=node.name,\n            bases=[ast.unparse(base) for base in node.bases],\n            body=\"\",\n            decorators=[ast.unparse(decorator) for decorator in node.decorator_list],\n            docstring=None,\n        )\n\n        # Extract docstring if present\n        docstring = ast.get_docstring(node, False)\n        if docstring:\n            class_info.docstring = docstring\n\n        # Handle the rest of the class body\n        body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n        class_info.body = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n        self.classes.append(class_info)\n\n    def visit_AsyncFunctionDef(self, node):\n        \"\"\"Extracts async functions from the given node.\"\"\"\n        return self._visit_Function(node, is_async=True)\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Extracts functions from the given node.\"\"\"\n        return self._visit_Function(node, is_async=False)\n\n    def _visit_Function(self, node, is_async):\n        \"\"\"Extracts functions or async functions from the given node.\"\"\"\n        # Initial function information setup\n        function_info = FunctionInfo(\n            name=node.name,\n            args=[ast.unparse(arg) for arg in node.args.args],\n            returns=ast.unparse(node.returns) if node.returns else None,\n            body=\"\",\n            decorators=[ast.unparse(decorator) for decorator in node.decorator_list],\n            docstring=None,\n            is_async=is_async,  # Indicates whether the function is async\n        )\n\n        # Extract docstring if present\n        docstring = ast.get_docstring(node, False)\n        if docstring:\n            function_info.docstring = docstring\n\n        # Handle the rest of the function body\n        body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n        function_info.body = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n        # Assuming you have a list to store functions\n        self.functions.append(function_info)\n\n    def visit_Module(self, node) -&gt; None:\n        \"\"\"Extracts comments from the given node.\"\"\"\n        comments = ast.get_docstring(node, False)\n        self.comments = \"\" if comments is None else comments\n        self.generic_visit(node)\n\n    def check_function_changed(self, other: PromptAnalyzer) -&gt; bool:\n        \"\"\"Compares the functions of this file with those of another file.\"\"\"\n        return self._check_definition_changed(other, \"function\")\n\n    def check_class_changed(self, other: PromptAnalyzer) -&gt; bool:\n        \"\"\"Compares the classes of this file with those of another file.\"\"\"\n        return self._check_definition_changed(other)\n\n    def _check_definition_changed(\n        self,\n        other: PromptAnalyzer,\n        definition_type: Optional[Literal[\"class\", \"function\"]] = \"class\",\n    ) -&gt; bool:\n        \"\"\"Compares classes or the functions of this file with those of another file\"\"\"\n\n        self_definitions: Union[list[ClassInfo], list[FunctionInfo]] = (\n            self.classes if definition_type == \"class\" else self.functions\n        )\n        other_definitions: Union[list[ClassInfo], list[FunctionInfo]] = (\n            other.classes if definition_type == \"class\" else other.functions\n        )\n\n        self_definitions_dict = {\n            definition.name: definition for definition in self_definitions\n        }\n        other_definitions_dict = {\n            definition.name: definition for definition in other_definitions\n        }\n\n        all_definition_names = set(self_definitions_dict.keys()) | set(\n            other_definitions_dict.keys()\n        )\n\n        for name in all_definition_names:\n            if name in self_definitions_dict and name in other_definitions_dict:\n                self_def_dict = self_definitions_dict[name].__dict__\n                other_def_dict = other_definitions_dict[name].__dict__\n                # Compare attributes of definitions with the same name\n                def_diff = {\n                    attr: (self_def_dict[attr], other_def_dict[attr])\n                    for attr in self_def_dict\n                    if self_def_dict[attr] != other_def_dict[attr]\n                }\n                if def_diff:\n                    return True\n            else:\n                return True\n\n        return False\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PromptAnalyzer.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the PromptAnalyzer.\"\"\"\n    self.imports: list[tuple[str, Optional[str]]] = []\n    self.from_imports: list[tuple[str, str, Optional[str]]] = []\n    self.variables: dict[str, Any] = {}\n    self.classes: list[ClassInfo] = []\n    self.functions: list[FunctionInfo] = []\n    self.comments: str = \"\"\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.check_class_changed","title":"<code>check_class_changed(other)</code>","text":"<p>Compares the classes of this file with those of another file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_class_changed(self, other: PromptAnalyzer) -&gt; bool:\n    \"\"\"Compares the classes of this file with those of another file.\"\"\"\n    return self._check_definition_changed(other)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.check_function_changed","title":"<code>check_function_changed(other)</code>","text":"<p>Compares the functions of this file with those of another file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_function_changed(self, other: PromptAnalyzer) -&gt; bool:\n    \"\"\"Compares the functions of this file with those of another file.\"\"\"\n    return self._check_definition_changed(other, \"function\")\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Assign","title":"<code>visit_Assign(node)</code>","text":"<p>Extracts variables from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Assign(self, node) -&gt; None:\n    \"\"\"Extracts variables from the given node.\"\"\"\n    target = node.targets[0]\n    if isinstance(target, ast.Name):\n        self.variables[target.id] = ast.unparse(node.value)\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_AsyncFunctionDef","title":"<code>visit_AsyncFunctionDef(node)</code>","text":"<p>Extracts async functions from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_AsyncFunctionDef(self, node):\n    \"\"\"Extracts async functions from the given node.\"\"\"\n    return self._visit_Function(node, is_async=True)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ClassDef","title":"<code>visit_ClassDef(node)</code>","text":"<p>Extracts classes from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ClassDef(self, node) -&gt; None:\n    \"\"\"Extracts classes from the given node.\"\"\"\n    class_info = ClassInfo(\n        name=node.name,\n        bases=[ast.unparse(base) for base in node.bases],\n        body=\"\",\n        decorators=[ast.unparse(decorator) for decorator in node.decorator_list],\n        docstring=None,\n    )\n\n    # Extract docstring if present\n    docstring = ast.get_docstring(node, False)\n    if docstring:\n        class_info.docstring = docstring\n\n    # Handle the rest of the class body\n    body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n    class_info.body = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n    self.classes.append(class_info)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_FunctionDef","title":"<code>visit_FunctionDef(node)</code>","text":"<p>Extracts functions from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_FunctionDef(self, node):\n    \"\"\"Extracts functions from the given node.\"\"\"\n    return self._visit_Function(node, is_async=False)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Import","title":"<code>visit_Import(node)</code>","text":"<p>Extracts imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Import(self, node) -&gt; None:\n    \"\"\"Extracts imports from the given node.\"\"\"\n    for alias in node.names:\n        self.imports.append((alias.name, alias.asname))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ImportFrom","title":"<code>visit_ImportFrom(node)</code>","text":"<p>Extracts from imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ImportFrom(self, node) -&gt; None:\n    \"\"\"Extracts from imports from the given node.\"\"\"\n    for alias in node.names:\n        self.from_imports.append((node.module, alias.name, alias.asname))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Module","title":"<code>visit_Module(node)</code>","text":"<p>Extracts comments from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Module(self, node) -&gt; None:\n    \"\"\"Extracts comments from the given node.\"\"\"\n    comments = ast.get_docstring(node, False)\n    self.comments = \"\" if comments is None else comments\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_prompt_changed","title":"<code>check_prompt_changed(file1_path, file2_path)</code>","text":"<p>Compare two prompts to check if the given prompts have changed.</p> <p>Parameters:</p> Name Type Description Default <code>file1_path</code> <code>Optional[str]</code> <p>The path to the first prompt.</p> required <code>file2_path</code> <code>Optional[str]</code> <p>The path to the second prompt.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether there are any differences between the two prompts.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_prompt_changed(file1_path: Optional[str], file2_path: Optional[str]) -&gt; bool:\n    \"\"\"Compare two prompts to check if the given prompts have changed.\n\n    Args:\n        file1_path: The path to the first prompt.\n        file2_path: The path to the second prompt.\n\n    Returns:\n        Whether there are any differences between the two prompts.\n    \"\"\"\n    if file1_path is None or file2_path is None:\n        raise FileNotFoundError(\"Prompt or version file is missing.\")\n    # Parse the first file\n    try:\n        with open(file1_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file1_path} was not found.\") from e\n    analyzer1 = PromptAnalyzer()\n    tree1 = ast.parse(content)\n    analyzer1.visit(tree1)\n\n    # Parse the second file\n    try:\n        with open(file2_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file2_path} was not found.\") from e\n    analyzer2 = PromptAnalyzer()\n    tree2 = ast.parse(content)\n    analyzer2.visit(tree2)\n    # Compare the contents of the two files\n    differences = {\n        \"comments\": analyzer1.comments != analyzer2.comments,\n        \"imports_diff\": bool(set(analyzer1.imports) ^ set(analyzer2.imports)),\n        \"from_imports_diff\": bool(\n            set(analyzer1.from_imports) ^ set(analyzer2.from_imports)\n        ),\n        \"functions_diff\": analyzer1.check_function_changed(analyzer2),\n        \"variables_diff\": set(analyzer1.variables.keys()) - ignore_variables\n        ^ set(analyzer2.variables.keys()) - ignore_variables,\n        \"classes_diff\": analyzer1.check_class_changed(analyzer2),\n        # Add other comparisons as needed\n    }\n    return any(differences.values())\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_status","title":"<code>check_status(mirascope_settings, directory)</code>","text":"<p>Checks the status of the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>mirascope_settings</code> <code>MirascopeSettings</code> <p>The user's mirascope settings.</p> required <code>directory</code> <code>str</code> <p>The name of the prompt file (excluding the .py extension).</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The path to the prompt if the prompt has changed, otherwise <code>None</code>.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_status(\n    mirascope_settings: MirascopeSettings, directory: str\n) -&gt; Optional[str]:\n    \"\"\"Checks the status of the given directory.\n\n    Args:\n        mirascope_settings: The user's mirascope settings.\n        directory: The name of the prompt file (excluding the .py extension).\n\n    Returns:\n        The path to the prompt if the prompt has changed, otherwise `None`.\n    \"\"\"\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n    prompt_directory = os.path.join(version_directory_path, directory)\n    used_prompt_path = f\"{prompt_directory_path}/{directory}.py\"\n\n    # Get the currently used prompt version\n    versions = get_prompt_versions(f\"{prompt_directory}/{version_file_name}\")\n    current_head = versions.current_revision\n    if current_head is None:\n        return used_prompt_path\n    current_version_prompt_path = find_prompt_path(prompt_directory, current_head)\n    # Check if users prompt matches the current prompt version\n    has_file_changed = check_prompt_changed(\n        current_version_prompt_path, used_prompt_path\n    )\n    if has_file_changed:\n        return used_prompt_path\n    return None\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_file_names","title":"<code>find_file_names(directory, prefix='')</code>","text":"<p>Finds all files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to search for the prompt.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the prompt to search for.</p> <code>''</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of file names found.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_file_names(directory: str, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"Finds all files in a directory.\n\n    Args:\n        directory: The directory to search for the prompt.\n        prefix: The prefix of the prompt to search for.\n\n    Returns:\n        A list of file names found.\n    \"\"\"\n    pattern = os.path.join(directory, f\"[!_]{prefix}*.py\")  # ignores private files\n    matching_files_with_dir = glob.glob(pattern)\n\n    # Removing the directory part from each path\n    return [os.path.basename(file) for file in matching_files_with_dir]\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_prompt_path","title":"<code>find_prompt_path(directory, prefix)</code>","text":"<p>Finds and opens the first found prompt with the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[Path, str]</code> <p>The directory to search for the prompt.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the prompt to search for.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The path to the prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_prompt_path(directory: Union[Path, str], prefix: str) -&gt; Optional[str]:\n    \"\"\"Finds and opens the first found prompt with the given directory.\n\n    Args:\n        directory: The directory to search for the prompt.\n        prefix: The prefix of the prompt to search for.\n\n    Returns:\n        The path to the prompt.\n    \"\"\"\n    prompt_files = find_prompt_paths(directory, prefix)\n    if prompt_files:\n        return prompt_files[0]\n    return None\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_prompt_paths","title":"<code>find_prompt_paths(directory, prefix)</code>","text":"<p>Finds and opens all prompts with the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[Path, str]</code> <p>The directory to search for the prompt.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the prompt to search for.</p> required <p>Returns:</p> Type Description <code>Optional[list[str]]</code> <p>A list of paths to the prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_prompt_paths(directory: Union[Path, str], prefix: str) -&gt; Optional[list[str]]:\n    \"\"\"Finds and opens all prompts with the given directory.\n\n    Args:\n        directory: The directory to search for the prompt.\n        prefix: The prefix of the prompt to search for.\n\n    Returns:\n        A list of paths to the prompt.\n    \"\"\"\n    pattern = os.path.join(directory, prefix + \"*.py\")\n    prompt_files = glob.glob(pattern)\n\n    if not prompt_files:\n        return None  # No files found\n\n    # Return first file found\n    return prompt_files\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_prompt_analyzer","title":"<code>get_prompt_analyzer(file)</code>","text":"<p>Gets an instance of PromptAnalyzer for a file</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file to analyze</p> required <p>Returns:</p> Type Description <code>PromptAnalyzer</code> <p>An instance of PromptAnalyzer</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_prompt_analyzer(file: str) -&gt; PromptAnalyzer:\n    \"\"\"Gets an instance of PromptAnalyzer for a file\n\n    Args:\n        file: The file to analyze\n\n    Returns:\n        An instance of PromptAnalyzer\n    \"\"\"\n    analyzer = PromptAnalyzer()\n    tree = ast.parse(file)\n    analyzer.visit(tree)\n    return analyzer\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_prompt_versions","title":"<code>get_prompt_versions(version_file_path)</code>","text":"<p>Returns the versions of the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>version_file_path</code> <code>str</code> <p>The path to the prompt.</p> required <p>Returns:</p> Type Description <code>VersionTextFile</code> <p>A <code>VersionTextFile</code> instance with the versions of current and latest revisions.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_prompt_versions(version_file_path: str) -&gt; VersionTextFile:\n    \"\"\"Returns the versions of the given prompt.\n\n    Args:\n        version_file_path: The path to the prompt.\n\n    Returns:\n        A `VersionTextFile` instance with the versions of current and latest revisions.\n    \"\"\"\n    versions = VersionTextFile()\n    try:\n        with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n            file.seek(0)\n            for line in file:\n                # Check if the current line contains the key\n                if line.startswith(CURRENT_REVISION_KEY + \"=\"):\n                    versions.current_revision = line.split(\"=\")[1].strip()\n                elif line.startswith(LATEST_REVISION_KEY + \"=\"):\n                    versions.latest_revision = line.split(\"=\")[1].strip()\n            return versions\n    except FileNotFoundError:\n        return versions\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_user_mirascope_settings","title":"<code>get_user_mirascope_settings(ini_file_path='mirascope.ini')</code>","text":"<p>Returns the user's mirascope settings.</p> <p>Parameters:</p> Name Type Description Default <code>ini_file_path</code> <code>str</code> <p>The path to the mirascope.ini file.</p> <code>'mirascope.ini'</code> <p>Returns:</p> Type Description <code>MirascopeSettings</code> <p>The user's mirascope settings as a <code>MirascopeSettings</code> instance.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the mirascope.ini file is not found.</p> <code>KeyError</code> <p>If the [mirascope] section is missing from the mirascope.ini file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_user_mirascope_settings(\n    ini_file_path: str = \"mirascope.ini\",\n) -&gt; MirascopeSettings:\n    \"\"\"Returns the user's mirascope settings.\n\n    Args:\n        ini_file_path: The path to the mirascope.ini file.\n\n    Returns:\n        The user's mirascope settings as a `MirascopeSettings` instance.\n\n    Raises:\n        FileNotFoundError: If the mirascope.ini file is not found.\n        KeyError: If the [mirascope] section is missing from the mirascope.ini file.\n    \"\"\"\n    config = ConfigParser(allow_no_value=True)\n    try:\n        read_ok = config.read(ini_file_path)\n        if not read_ok:\n            raise FileNotFoundError(\n                \"The mirascope.ini file was not found. Please run \"\n                \"`mirascope init` to create one or run the mirascope CLI from the \"\n                \"same directory as the mirascope.ini file.\"\n            )\n        mirascope_config = config[\"mirascope\"]\n        return MirascopeSettings(**mirascope_config)\n    except MissingSectionHeaderError as e:\n        raise MissingSectionHeaderError(ini_file_path, e.lineno, e.source) from e\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.parse_prompt_file_name","title":"<code>parse_prompt_file_name(prompt_file_name)</code>","text":"<p>Returns the file name without the .py extension.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def parse_prompt_file_name(prompt_file_name: str) -&gt; str:\n    \"\"\"Returns the file name without the .py extension.\"\"\"\n    if prompt_file_name.endswith(\".py\"):\n        return prompt_file_name[:-3]\n    return prompt_file_name\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.prompts_directory_files","title":"<code>prompts_directory_files()</code>","text":"<p>Returns a list of files in the user's prompts directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def prompts_directory_files() -&gt; list[str]:\n    \"\"\"Returns a list of files in the user's prompts directory.\"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    prompt_file_names = find_file_names(mirascope_settings.prompts_location)\n    return [f\"{name[:-3]}\" for name in prompt_file_names]  # remove .py extension\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.run_format_command","title":"<code>run_format_command(file)</code>","text":"<p>Runs the format command on the given file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file to format</p> required Source code in <code>mirascope/cli/utils.py</code> <pre><code>def run_format_command(file: str) -&gt; None:\n    \"\"\"Runs the format command on the given file.\n\n    Args:\n        file: The file to format\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    if mirascope_settings.format_command:\n        format_commands: list[list[str]] = [\n            command.split() for command in mirascope_settings.format_command.split(\";\")\n        ]\n        # assuming the final command takes filename as argument, and as final argument\n        format_commands[-1].append(file)\n        for command in format_commands:\n            subprocess.run(\n                [sys.executable, \"-m\"] + command,\n                check=True,\n                capture_output=True,\n                shell=False,\n            )\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.update_version_text_file","title":"<code>update_version_text_file(version_file, updates)</code>","text":"<p>Updates the version text file.</p> <p>Depending on the contents of <code>updates</code>, updates the ids of the current and latest revisions of the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>version_file</code> <code>str</code> <p>The path to the version text file.</p> required <code>updates</code> <code>dict[str, str]</code> <p>A dictionary containing updates to the current revision id and/or the latest revision id.</p> required Source code in <code>mirascope/cli/utils.py</code> <pre><code>def update_version_text_file(\n    version_file: str,\n    updates: dict[str, str],\n) -&gt; None:\n    \"\"\"Updates the version text file.\n\n    Depending on the contents of `updates`, updates the ids of the current and latest\n    revisions of the prompt.\n\n    Args:\n        version_file: The path to the version text file.\n        updates: A dictionary containing updates to the current revision id and/or\n            the latest revision id.\n    \"\"\"\n    modified_lines = []\n    edits_made = {\n        key: False for key in updates\n    }  # Track which keys already exist in the file\n    version_file_path: Path = Path(version_file)\n    if not version_file_path.is_file():\n        version_file_path.touch()\n    # Read the file and apply updates\n    with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            # Check if the current line contains any of the keys\n            for key, value in updates.items():\n                if line.startswith(key + \"=\"):\n                    modified_lines.append(f\"{key}={value}\\n\")\n                    edits_made[key] = True\n                    break\n            else:\n                # No key found, so keep the line as is\n                modified_lines.append(line)\n\n        # Add any keys that were not found at the end of the file\n        for key, value in updates.items():\n            if not edits_made[key]:\n                modified_lines.append(f\"{key}={value}\\n\")\n\n    # Write the modified content back to the file\n    with open(version_file_path, \"w\", encoding=\"utf-8\") as file:\n        file.writelines(modified_lines)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.write_prompt_to_template","title":"<code>write_prompt_to_template(file, command, variables=None)</code>","text":"<p>Writes the given prompt to the template.</p> <p>Deconstructs a prompt with ast and reconstructs it using the Jinja2 template, adding revision history into the prompt when the command is <code>MirascopeCommand.ADD</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the prompt.</p> required <code>command</code> <code>Literal[ADD, USE]</code> <p>The CLI command to execute.</p> required <code>variables</code> <code>Optional[MirascopeCliVariables]</code> <p>A dictionary of revision ids which are rendered together with variable assignments that are not inside any class. Only relevant when <code>command</code> is <code>MirascopeCommand.ADD</code> - if <code>command</code> is <code>MirascopeCommand.USE</code>, <code>variables</code> should be <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The reconstructed prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def write_prompt_to_template(\n    file: str,\n    command: Literal[MirascopeCommand.ADD, MirascopeCommand.USE],\n    variables: Optional[MirascopeCliVariables] = None,\n) -&gt; str:\n    \"\"\"Writes the given prompt to the template.\n\n    Deconstructs a prompt with ast and reconstructs it using the Jinja2 template, adding\n    revision history into the prompt when the command is `MirascopeCommand.ADD`.\n\n    Args:\n        file: The path to the prompt.\n        command: The CLI command to execute.\n        variables: A dictionary of revision ids which are rendered together with\n            variable assignments that are not inside any class. Only relevant when\n            `command` is `MirascopeCommand.ADD` - if `command` is\n            `MirascopeCommand.USE`, `variables` should be `None`.\n\n    Returns:\n        The reconstructed prompt.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    mirascope_directory = mirascope_settings.mirascope_location\n    auto_tag = mirascope_settings.auto_tag\n    template_loader = FileSystemLoader(searchpath=mirascope_directory)\n    template_env = Environment(loader=template_loader)\n    template = template_env.get_template(\"prompt_template.j2\")\n    analyzer = get_prompt_analyzer(file)\n    if variables is None:\n        variables = MirascopeCliVariables()\n\n    if command == MirascopeCommand.ADD:\n        # double quote revision ids to match how `ast.unparse()` formats strings\n        new_variables = {\n            k: f\"'{v}'\" if isinstance(v, str) else None\n            for k, v in variables.__dict__.items()\n        } | analyzer.variables\n    else:  # command == MirascopeCommand.USE\n        ignore_variable_keys = dict.fromkeys(ignore_variables, None)\n        new_variables = {\n            k: analyzer.variables[k]\n            for k in analyzer.variables\n            if k not in ignore_variable_keys\n        }\n\n    if auto_tag:\n        import_tag_name: Optional[str] = None\n        mirascope_alias = \"mirascope\"\n        for name, alias in analyzer.imports:\n            if name == \"mirascope\" and alias is not None:\n                mirascope_alias = alias\n                break\n        for module, name, alias in analyzer.from_imports:\n            if module == \"mirascope\" and name == \"tags\" and alias is not None:\n                mirascope_alias = alias\n                break\n\n        for python_class in analyzer.classes:\n            decorators = python_class.decorators\n            if python_class.bases and python_class.bases[0] in mirascope_prompt_bases:\n                import_tag_name = _update_tag_decorator_with_version(\n                    decorators, variables, mirascope_alias\n                )\n\n        if import_tag_name == \"tags\":\n            _update_mirascope_from_imports(import_tag_name, analyzer.from_imports)\n        elif import_tag_name == f\"{mirascope_alias}.tags\":\n            _update_mirascope_imports(analyzer.imports)\n\n    data = {\n        \"comments\": analyzer.comments,\n        \"variables\": new_variables,\n        \"imports\": analyzer.imports,\n        \"from_imports\": analyzer.from_imports,\n        \"classes\": analyzer.classes,\n    }\n    return template.render(**data)\n</code></pre>"},{"location":"api/gemini/","title":"gemini","text":"<p>A module for interacting with Google's Gemini models.</p>"},{"location":"api/gemini/prompt/","title":"gemini.prompt","text":"<p>A module for prompting Google's Gemini Chat API.</p>"},{"location":"api/gemini/prompt/#mirascope.gemini.prompt.GeminiPrompt","title":"<code>GeminiPrompt</code>","text":"<p>             Bases: <code>BasePrompt</code></p> <p>A class for prompting Google's Gemini Chat API.</p> <p>Example:</p> <pre><code>from google.generativeai import configure  # type: ignore\nfrom mirascope.gemini import GeminiPrompt\n\nconfigure(api_key=\"YOUR_API_KEY\")\n\n\nclass BookRecommendation(GeminiPrompt):\n    \"\"\"\n    USER:\n    You're the world's greatest librarian.\n\n    MODEL:\n    Ok, I understand that I'm the world's greatest librarian. How can I help you?\n\n    USER:\n    Please recommend some {genre} books.\n    \"\"\"\n\n    genre: str\n\n\nprompt = BookRecommendation(genre=\"fantasy\")\nprint(prompt.create())\n#&gt; As the world's greatest librarian, I am delighted to recommend...\n</code></pre> Source code in <code>mirascope/gemini/prompt.py</code> <pre><code>class GeminiPrompt(BasePrompt):\n    '''A class for prompting Google's Gemini Chat API.\n\n    Example:\n\n    ```python\n    from google.generativeai import configure  # type: ignore\n    from mirascope.gemini import GeminiPrompt\n\n    configure(api_key=\"YOUR_API_KEY\")\n\n\n    class BookRecommendation(GeminiPrompt):\n        \"\"\"\n        USER:\n        You're the world's greatest librarian.\n\n        MODEL:\n        Ok, I understand that I'm the world's greatest librarian. How can I help you?\n\n        USER:\n        Please recommend some {genre} books.\n        \"\"\"\n\n        genre: str\n\n\n    prompt = BookRecommendation(genre=\"fantasy\")\n    print(prompt.create())\n    #&gt; As the world's greatest librarian, I am delighted to recommend...\n    ```\n    '''\n\n    api_key: Annotated[\n        Optional[str],\n        AfterValidator(lambda key: configure(api_key=key) if key is not None else None),\n    ] = None\n\n    call_params: ClassVar[GeminiCallParams] = GeminiCallParams(\n        model=\"gemini-1.0-pro\",\n        generation_config={\"candidate_count\": 1},\n    )\n\n    @property\n    def messages(self) -&gt; ContentsType:\n        \"\"\"Returns the `ContentsType` messages for Gemini `generate_content`.\"\"\"\n        messages = []\n        for match in re.finditer(\n            r\"(MODEL|USER|TOOL): \" r\"((.|\\n)+?)(?=\\n(MODEL|USER|TOOL):|\\Z)\",\n            self.template(),\n        ):\n            role = match.group(1).lower()\n            content = format_template(self, match.group(2))\n            messages.append({\"role\": role, \"parts\": [content]})\n        if len(messages) == 0:\n            messages.append({\"role\": \"user\", \"parts\": [str(self)]})\n        return messages\n\n    def create(self, **kwargs: Any) -&gt; GeminiCompletion:\n        \"\"\"Makes a call to the model using this `GeminiPrompt`.\n\n        Returns:\n            A `GeminiCompletion` instance.\n        \"\"\"\n        gemini_pro_model = GenerativeModel(self.call_params.model)\n        tools = kwargs.pop(\"tools\") if \"tools\" in kwargs else []\n        if self.call_params.tools:\n            tools.extend(self.call_params.tools)\n        converted_tools = [\n            tool if isclass(tool) else tool_fn(tool)(GeminiTool.from_fn(tool))\n            for tool in tools\n        ]\n        completion_start_time = datetime.datetime.now().timestamp() * 1000\n        completion = gemini_pro_model.generate_content(\n            self.messages,\n            stream=False,\n            tools=[tool.tool_schema() for tool in converted_tools]\n            if converted_tools\n            else None,\n            generation_config=self.call_params.generation_config,\n            safety_settings=self.call_params.safety_settings,\n            request_options=self.call_params.request_options,\n        )\n        return GeminiCompletion(\n            completion=completion,\n            tool_types=converted_tools,\n            start_time=completion_start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    def stream(self) -&gt; Generator[GeminiCompletionChunk, None, None]:\n        \"\"\"Streams the response for a call to the model using `prompt`.\n\n        Yields:\n            A `GeminiCompletionChunk` for each chunk of the response.\n        \"\"\"\n        gemini_pro_model = GenerativeModel(self.call_params.model)\n        completion = gemini_pro_model.generate_content(\n            self.messages,\n            stream=True,\n            generation_config=self.call_params.generation_config,\n            safety_settings=self.call_params.safety_settings,\n            request_options=self.call_params.request_options,\n        )\n        for chunk in completion:\n            yield GeminiCompletionChunk(chunk=chunk)\n\n    @overload\n    def extract(self, schema: Type[BaseTypeT], retries: int = 0) -&gt; BaseTypeT:\n        ...  # pragma: no cover\n\n    @overload\n    def extract(self, schema: Type[BaseModelT], retries: int = 0) -&gt; BaseModelT:\n        ...  # pragma: no cover\n\n    @overload\n    def extract(self, schema: Callable, retries: int = 0) -&gt; GeminiTool:\n        ...  # pragma: no cover\n\n    def extract(self, schema, retries=0):\n        \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n        The given schema is converted into an `GeminiTool`, complete with a description\n        of the tool, all of the fields, and their types. This allows us to take\n        advantage of Gemini's tool/function calling functionality to extract information\n        from a prompt according to the context provided by the `BaseModel` schema.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            retries: The maximum number of times to retry the query on validation error.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n        \"\"\"\n        return_tool = True\n        gemini_tool = schema\n        if is_base_type(schema):\n            gemini_tool = GeminiTool.from_base_type(schema)\n            return_tool = False\n        elif not isclass(schema):\n            gemini_tool = GeminiTool.from_fn(schema)\n        elif not issubclass(schema, GeminiTool):\n            gemini_tool = GeminiTool.from_model(schema)\n            return_tool = False\n\n        completion = self.create(tools=[gemini_tool])\n        try:\n            tool = completion.tool\n            if tool is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            if return_tool:\n                return tool\n            if is_base_type(schema):\n                return tool.value\n            model = schema(**completion.tool.model_dump())  # type: ignore\n            model._completion = completion\n            return model\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return self.extract(schema, retries - 1)\n            raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/gemini/prompt/#mirascope.gemini.prompt.GeminiPrompt.messages","title":"<code>messages: ContentsType</code>  <code>property</code>","text":"<p>Returns the <code>ContentsType</code> messages for Gemini <code>generate_content</code>.</p>"},{"location":"api/gemini/prompt/#mirascope.gemini.prompt.GeminiPrompt.create","title":"<code>create(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>GeminiPrompt</code>.</p> <p>Returns:</p> Type Description <code>GeminiCompletion</code> <p>A <code>GeminiCompletion</code> instance.</p> Source code in <code>mirascope/gemini/prompt.py</code> <pre><code>def create(self, **kwargs: Any) -&gt; GeminiCompletion:\n    \"\"\"Makes a call to the model using this `GeminiPrompt`.\n\n    Returns:\n        A `GeminiCompletion` instance.\n    \"\"\"\n    gemini_pro_model = GenerativeModel(self.call_params.model)\n    tools = kwargs.pop(\"tools\") if \"tools\" in kwargs else []\n    if self.call_params.tools:\n        tools.extend(self.call_params.tools)\n    converted_tools = [\n        tool if isclass(tool) else tool_fn(tool)(GeminiTool.from_fn(tool))\n        for tool in tools\n    ]\n    completion_start_time = datetime.datetime.now().timestamp() * 1000\n    completion = gemini_pro_model.generate_content(\n        self.messages,\n        stream=False,\n        tools=[tool.tool_schema() for tool in converted_tools]\n        if converted_tools\n        else None,\n        generation_config=self.call_params.generation_config,\n        safety_settings=self.call_params.safety_settings,\n        request_options=self.call_params.request_options,\n    )\n    return GeminiCompletion(\n        completion=completion,\n        tool_types=converted_tools,\n        start_time=completion_start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/gemini/prompt/#mirascope.gemini.prompt.GeminiPrompt.extract","title":"<code>extract(schema, retries=0)</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call.</p> <p>The given schema is converted into an <code>GeminiTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Gemini's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>retries</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <p>Returns:</p> Type Description <p>The <code>Schema</code> instance extracted from the completion.</p> Source code in <code>mirascope/gemini/prompt.py</code> <pre><code>def extract(self, schema, retries=0):\n    \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n    The given schema is converted into an `GeminiTool`, complete with a description\n    of the tool, all of the fields, and their types. This allows us to take\n    advantage of Gemini's tool/function calling functionality to extract information\n    from a prompt according to the context provided by the `BaseModel` schema.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        retries: The maximum number of times to retry the query on validation error.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n    \"\"\"\n    return_tool = True\n    gemini_tool = schema\n    if is_base_type(schema):\n        gemini_tool = GeminiTool.from_base_type(schema)\n        return_tool = False\n    elif not isclass(schema):\n        gemini_tool = GeminiTool.from_fn(schema)\n    elif not issubclass(schema, GeminiTool):\n        gemini_tool = GeminiTool.from_model(schema)\n        return_tool = False\n\n    completion = self.create(tools=[gemini_tool])\n    try:\n        tool = completion.tool\n        if tool is None:\n            raise AttributeError(\"No tool found in the completion.\")\n        if return_tool:\n            return tool\n        if is_base_type(schema):\n            return tool.value\n        model = schema(**completion.tool.model_dump())  # type: ignore\n        model._completion = completion\n        return model\n    except (AttributeError, ValueError, ValidationError) as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: include failure in retry prompt.\n            return self.extract(schema, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/gemini/prompt/#mirascope.gemini.prompt.GeminiPrompt.stream","title":"<code>stream()</code>","text":"<p>Streams the response for a call to the model using <code>prompt</code>.</p> <p>Yields:</p> Type Description <code>GeminiCompletionChunk</code> <p>A <code>GeminiCompletionChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/gemini/prompt.py</code> <pre><code>def stream(self) -&gt; Generator[GeminiCompletionChunk, None, None]:\n    \"\"\"Streams the response for a call to the model using `prompt`.\n\n    Yields:\n        A `GeminiCompletionChunk` for each chunk of the response.\n    \"\"\"\n    gemini_pro_model = GenerativeModel(self.call_params.model)\n    completion = gemini_pro_model.generate_content(\n        self.messages,\n        stream=True,\n        generation_config=self.call_params.generation_config,\n        safety_settings=self.call_params.safety_settings,\n        request_options=self.call_params.request_options,\n    )\n    for chunk in completion:\n        yield GeminiCompletionChunk(chunk=chunk)\n</code></pre>"},{"location":"api/gemini/tools/","title":"gemini.tools","text":"<p>Classes for using tools with Google's Gemini Chat APIs.</p>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool","title":"<code>GeminiTool</code>","text":"<p>             Bases: <code>BaseTool</code></p> <p>A base class for easy use of tools with the Gemini Chat client.</p> <p><code>GeminiTool</code> internally handles the logic that allows you to use tools with simple calls such as <code>GeminiCompletion.tool</code> or <code>GeminiTool.fn</code>, as seen in the examples below.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCallParams, GeminiPrompt, GeminiTool\n\n\nclass CurrentWeather(GeminiTool):\n    \"\"\"A tool for getting the current weather in a location.\"\"\"\n\n    location: str\n\n\nclass WeatherPrompt(GeminiPrompt):\n    \"\"\"What is the current weather in Tokyo?\"\"\"\n\n    call_params = GeminiCallParams(\n        model=\"gemini-pro\",\n        tools=[CurrentWeather],\n    )\n\n\nprompt = WeatherPrompt()\ncurrent_weather = prompt.create().tool\nprint(current_weather.location)\n#&gt; Tokyo\n</code></pre> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>class GeminiTool(BaseTool):\n    '''A base class for easy use of tools with the Gemini Chat client.\n\n    `GeminiTool` internally handles the logic that allows you to use tools with simple\n    calls such as `GeminiCompletion.tool` or `GeminiTool.fn`, as seen in the\n    examples below.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCallParams, GeminiPrompt, GeminiTool\n\n\n    class CurrentWeather(GeminiTool):\n        \"\"\"A tool for getting the current weather in a location.\"\"\"\n\n        location: str\n\n\n    class WeatherPrompt(GeminiPrompt):\n        \"\"\"What is the current weather in Tokyo?\"\"\"\n\n        call_params = GeminiCallParams(\n            model=\"gemini-pro\",\n            tools=[CurrentWeather],\n        )\n\n\n    prompt = WeatherPrompt()\n    current_weather = prompt.create().tool\n    print(current_weather.location)\n    #&gt; Tokyo\n    ```\n    '''\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n        A Mirascope `GeminiTool` is deconstructed into a `Tool` schema for use with the\n        Gemini Chat client.\n\n        Returns:\n            The constructed `Tool` schema.\n\n        Raises:\n            ValueError: if the class doesn't have a docstring description.\n        \"\"\"\n        model_schema = cls.model_json_schema()\n        if \"description\" not in model_schema:\n            raise ValueError(\"Tool must have a docstring description.\")\n\n        tool_schema = super().tool_schema()\n        if \"parameters\" in tool_schema:\n            tool_schema[\"parameters\"].pop(\"$defs\")\n            tool_schema[\"parameters\"][\"properties\"] = {\n                prop: {\n                    key: value for key, value in prop_schema.items() if key != \"title\"\n                }\n                for prop, prop_schema in tool_schema[\"parameters\"][\"properties\"].items()\n            }\n        return Tool(function_declarations=[FunctionDeclaration(**tool_schema)])\n\n    @classmethod\n    def from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given a `GenerateContentResponse` from a Gemini chat completion response, this\n        method extracts the tool call and constructs an instance of the tool.\n\n        Args:\n            tool_call: The `GenerateContentResponse` from which to extract the tool.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValueError: if the tool call doesn't have any arguments.\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        if not tool_call.args:\n            raise ValueError(\"Tool call doesn't have any arguments.\")\n        model_json = {key: value for key, value in tool_call.args.items()}\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, GeminiTool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, GeminiTool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a function.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given a <code>GenerateContentResponse</code> from a Gemini chat completion response, this method extracts the tool call and constructs an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>FunctionCall</code> <p>The <code>GenerateContentResponse</code> from which to extract the tool.</p> required <p>Returns:</p> Type Description <code>GeminiTool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the tool call doesn't have any arguments.</p> <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given a `GenerateContentResponse` from a Gemini chat completion response, this\n    method extracts the tool call and constructs an instance of the tool.\n\n    Args:\n        tool_call: The `GenerateContentResponse` from which to extract the tool.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValueError: if the tool call doesn't have any arguments.\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json = {key: value for key, value in tool_call.args.items()}\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the OpenAI Chat client.</p> <p>A Mirascope <code>GeminiTool</code> is deconstructed into a <code>Tool</code> schema for use with the Gemini Chat client.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>The constructed <code>Tool</code> schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the class doesn't have a docstring description.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n    A Mirascope `GeminiTool` is deconstructed into a `Tool` schema for use with the\n    Gemini Chat client.\n\n    Returns:\n        The constructed `Tool` schema.\n\n    Raises:\n        ValueError: if the class doesn't have a docstring description.\n    \"\"\"\n    model_schema = cls.model_json_schema()\n    if \"description\" not in model_schema:\n        raise ValueError(\"Tool must have a docstring description.\")\n\n    tool_schema = super().tool_schema()\n    if \"parameters\" in tool_schema:\n        tool_schema[\"parameters\"].pop(\"$defs\")\n        tool_schema[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value for key, value in prop_schema.items() if key != \"title\"\n            }\n            for prop, prop_schema in tool_schema[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**tool_schema)])\n</code></pre>"},{"location":"api/gemini/types/","title":"gemini.types","text":"<p>Type classes for interacting with the OpenAI Chat API.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallParams","title":"<code>GeminiCallParams</code>","text":"<p>             Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Gemini sChat API with a prompt.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiPrompt, GeminiCallParams\n\n\nclass BookRecommendation(GeminiPrompt):\n    \"\"\"Please recommend some books.\"\"\"\n\n    call_params = GeminiCallParams(\n        model=\"gemini-1.0-pro-001\",\n        generation_config={\"candidate_count\": 2},\n    )\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallParams(BaseCallParams):\n    '''The parameters to use when calling the Gemini sChat API with a prompt.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiPrompt, GeminiCallParams\n\n\n    class BookRecommendation(GeminiPrompt):\n        \"\"\"Please recommend some books.\"\"\"\n\n        call_params = GeminiCallParams(\n            model=\"gemini-1.0-pro-001\",\n            generation_config={\"candidate_count\": 2},\n        )\n    ```\n    '''\n\n    model: str = \"gemini-pro\"\n    tools: Optional[list[Union[Callable, Type[GeminiTool]]]] = None\n    generation_config: Optional[dict[str, Any]] = None\n    safety_settings: Optional[Any] = None\n    request_options: Optional[dict[str, Any]] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCompletion","title":"<code>GeminiCompletion</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around Gemini chat completions.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiChat</code>, responses using <code>GeminiChat.create()</code> will return a <code>GeminiCompletion</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiPrompt\n\n\nclass BookRecommendation(GeminiPrompt):\n    \"\"\"Please recommend some books.\"\"\"\n\n\nprint(BookRecommendation().create())\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCompletion(BaseModel):\n    '''Convenience wrapper around Gemini chat completions.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiChat`, responses using `GeminiChat.create()` will return a\n    `GeminiCompletion`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiPrompt\n\n\n    class BookRecommendation(GeminiPrompt):\n        \"\"\"Please recommend some books.\"\"\"\n\n\n    print(BookRecommendation().create())\n    ```\n    '''\n\n    completion: GenerateContentResponse  # The completion response from the model\n    tool_types: Optional[list[Type[GeminiTool]]] = None\n    start_time: float  # The start time of the completion in ms\n    end_time: float  # The end time of the completion in ms\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def tool(self) -&gt; Optional[GeminiTool]:\n        \"\"\"Returns the 0th tool for the 0th candidate's 0th content part.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if self.tool_types is None:\n            return None\n\n        tool_call = self.completion.candidates[0].content.parts[0].function_call\n        for tool_type in self.tool_types:\n            if tool_call.name == tool_type.__name__:\n                return tool_type.from_tool_call(tool_call)\n\n        return None\n\n    def __str__(self):\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.completion.candidates[0].content.parts[0].text\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCompletion.tool","title":"<code>tool: Optional[GeminiTool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCompletion.__str__","title":"<code>__str__()</code>","text":"<p>Returns the contained string content for the 0th choice.</p> Source code in <code>mirascope/gemini/types.py</code> <pre><code>def __str__(self):\n    \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n    return self.completion.candidates[0].content.parts[0].text\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCompletionChunk","title":"<code>GeminiCompletionChunk</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiChat</code>, responses using <code>GeminiChat.stream()</code> will return a <code>GeminiCompletionChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiPrompt\n\n\nclass BookRecommendation(GeminiPrompt):\n    \"\"\"Please recommend some books.\"\"\"\"\"\"\n\n\nfor chunk in BookRecommendation().stream():\n    print(chunk, end=\"\")\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCompletionChunk(BaseModel):\n    '''Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiChat`, responses using `GeminiChat.stream()` will return a\n    `GeminiCompletionChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiPrompt\n\n\n    class BookRecommendation(GeminiPrompt):\n        \"\"\"Please recommend some books.\"\"\"\"\"\"\n\n\n    for chunk in BookRecommendation().stream():\n        print(chunk, end=\"\")\n    ```\n    '''\n\n    chunk: GenerateContentResponse\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.chunk.candidates[0].content.parts[0].text\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCompletionChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the chunk content for the 0th choice.</p> Source code in <code>mirascope/gemini/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n    return self.chunk.candidates[0].content.parts[0].text\n</code></pre>"},{"location":"api/openai/","title":"openai","text":"<p>A module for interacting with OpenAI models.</p>"},{"location":"api/openai/prompt/","title":"openai.prompt","text":"<p>A module for prompting OpenAI's Chat API.</p>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt","title":"<code>OpenAIPrompt</code>","text":"<p>             Bases: <code>BasePrompt</code></p> <p>A class for prompting OpenAI's Chat API.</p> <p>Example:</p> <pre><code>import os\n\nfrom mirascope.openai import OpenAIPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"\n    SYSTEM:\n    You're the world's greatest librarian.\n\n    USER:\n    Please recommend some {genre} books.\n    \"\"\"\n\n    genre: str\n\n\nprompt = BookRecommendation(genre=\"fantasy\")\nprint(prompt.create())\n#&gt; There are many great books to read, it ultimately depends...\n</code></pre> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>class OpenAIPrompt(BasePrompt):\n    '''A class for prompting OpenAI's Chat API.\n\n    Example:\n\n    ```python\n    import os\n\n    from mirascope.openai import OpenAIPrompt\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\n    class BookRecommendation(OpenAIPrompt):\n        \"\"\"\n        SYSTEM:\n        You're the world's greatest librarian.\n\n        USER:\n        Please recommend some {genre} books.\n        \"\"\"\n\n        genre: str\n\n\n    prompt = BookRecommendation(genre=\"fantasy\")\n    print(prompt.create())\n    #&gt; There are many great books to read, it ultimately depends...\n    ```\n    '''\n\n    api_key: Annotated[\n        Optional[str],\n        AfterValidator(lambda key: _set_api_key(key) if key is not None else None),\n    ] = None\n\n    call_params: ClassVar[OpenAICallParams] = OpenAICallParams(\n        model=\"gpt-3.5-turbo-0125\",\n    )\n\n    @property\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"Returns this prompt's list of `ChatCompletionMessageParam` instances.\"\"\"\n        messages = super().messages\n        return [cast(ChatCompletionMessageParam, message) for message in messages]\n\n    def create(self, **kwargs: Any) -&gt; OpenAIChatCompletion:\n        \"\"\"Makes a call to the model using this `OpenAIPrompt` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments to pass to the `create` call.\n\n        Returns:\n            A `OpenAIChatCompletion` instance.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        client = OpenAI(base_url=self.call_params.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        tools = kwargs.pop(\"tools\") if \"tools\" in kwargs else []\n        if self.call_params.tools:\n            tools.extend(self.call_params.tools)\n        converted_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, self, converted_tools)\n        completion_start_time = datetime.datetime.now().timestamp() * 1000\n        completion = client.chat.completions.create(\n            stream=False,\n            **kwargs,\n        )\n        return OpenAIChatCompletion(\n            completion=completion,\n            tool_types=converted_tools,\n            start_time=completion_start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def async_create(self, **kwargs: Any) -&gt; OpenAIChatCompletion:\n        \"\"\"Makes an asynchronous call to the model using this `OpenAIPrompt`.\n\n        Args:\n            **kwargs: Additional keyword arguments to pass to the `async_create` call.\n\n        Returns:\n            An `OpenAIChatCompletion` instance.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        client = AsyncOpenAI(base_url=self.call_params.base_url)\n        if self.call_params.async_wrapper is not None:\n            client = self.call_params.async_wrapper(client)\n        tools = kwargs.pop(\"tools\") if \"tools\" in kwargs else []\n        if self.call_params.tools:\n            tools.extend(self.call_params.tools)\n        converted_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, self, converted_tools)\n        completion_start_time = datetime.datetime.now().timestamp() * 1000\n        completion = await client.chat.completions.create(\n            stream=False,\n            **kwargs,\n        )\n        return OpenAIChatCompletion(\n            completion=completion,\n            tool_types=converted_tools,\n            start_time=completion_start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    def stream(self, **kwargs: Any) -&gt; Generator[OpenAIChatCompletionChunk, None, None]:\n        \"\"\"Streams the response for a call to the model using `prompt`.\n\n        Args:\n            **kwargs: Additional keyword arguments to pass to the `stream` call.\n\n        Yields:\n            A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        client = OpenAI(base_url=self.call_params.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        tools = convert_tools_list_to_openai_tools(self.call_params.tools)\n        patch_openai_kwargs(kwargs, self, tools)\n        stream = client.chat.completions.create(\n            stream=True,\n            **kwargs,\n        )\n        for chunk in stream:\n            yield OpenAIChatCompletionChunk(chunk=chunk, tool_types=tools)\n\n    async def async_stream(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[OpenAIChatCompletionChunk, None]:\n        \"\"\"Streams the response for an asynchronous call to the model using `prompt`.\n\n        Args:\n            **kwargs: Additional keyword arguments to pass to the `async_stream` call.\n\n        Yields:\n            A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        client = AsyncOpenAI(base_url=self.call_params.base_url)\n        if self.call_params.async_wrapper is not None:\n            client = self.call_params.async_wrapper(client)\n        tools = convert_tools_list_to_openai_tools(self.call_params.tools)\n        patch_openai_kwargs(kwargs, self, tools)\n        stream = await client.chat.completions.create(\n            stream=True,\n            **kwargs,\n        )\n\n        async for chunk in stream:\n            yield OpenAIChatCompletionChunk(\n                chunk=chunk, tool_types=tools if tools else None\n            )\n\n    @overload\n    def extract(self, schema: Type[BaseTypeT], retries: int = 0) -&gt; BaseTypeT:\n        ...  # pragma: no cover\n\n    @overload\n    def extract(self, schema: Type[BaseModelT], retries: int = 0) -&gt; BaseModelT:\n        ...  # pragma: no cover\n\n    @overload\n    def extract(self, schema: Callable, retries: int = 0) -&gt; OpenAITool:\n        ...  # pragma: no cover\n\n    def extract(self, schema, retries=0):\n        \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n        The given schema is converted into an `OpenAITool`, complete with a description\n        of the tool, all of the fields, and their types. This allows us to take\n        advantage of OpenAI's tool/function calling functionality to extract information\n        from a prompt according to the context provided by the `BaseModel` schema.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            retries: The maximum number of times to retry the query on validation error.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        return_tool = True\n        openai_tool = schema\n        if is_base_type(schema):\n            openai_tool = OpenAITool.from_base_type(schema)  # type: ignore\n            return_tool = False\n        elif not isclass(schema):\n            openai_tool = OpenAITool.from_fn(schema)\n        elif not issubclass(schema, OpenAITool):\n            openai_tool = OpenAITool.from_model(schema)\n            return_tool = False\n\n        completion = self.create(tools=[openai_tool])\n        try:\n            tool = completion.tool\n            if tool is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            if return_tool:\n                return tool\n            if is_base_type(schema):\n                return tool.value  # type: ignore\n            model = schema(**completion.tool.model_dump())  # type: ignore\n            model._completion = completion\n            return model\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return self.extract(schema, retries - 1)\n            raise  # re-raise if we have no retries left\n\n    @overload\n    def async_extract(self, schema: Type[BaseTypeT], retries: int = 0) -&gt; BaseTypeT:\n        ...  # pragma: no cover\n\n    @overload\n    async def async_extract(\n        self, schema: Type[BaseModelT], retries: int = 0\n    ) -&gt; BaseModelT:\n        ...  # pragma: no cover\n\n    @overload\n    async def async_extract(self, schema: Callable, retries: int = 0) -&gt; OpenAITool:\n        ...  # pragma: no cover\n\n    async def async_extract(self, schema, retries=0):\n        \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n        The given schema is converted into an `OpenAITool`, complete with a description\n        of the tool, all of the fields, and their types. This allows us to take\n        advantage of OpenAI's tool/function calling functionality to extract information\n        from a prompt according to the context provided by the `BaseModel` schema.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            retries: The maximum number of times to retry the query on validation error.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        return_tool = True\n        openai_tool = schema\n        if is_base_type(schema):\n            openai_tool = OpenAITool.from_base_type(schema)  # type: ignore\n            return_tool = False\n        elif not isclass(schema):\n            openai_tool = OpenAITool.from_fn(schema)\n        elif not issubclass(schema, OpenAITool):\n            openai_tool = OpenAITool.from_model(schema)\n            return_tool = False\n\n        completion = await self.async_create(tools=[openai_tool])\n        try:\n            tool = completion.tool\n            if tool is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            if return_tool:\n                return tool\n            if is_base_type(schema):\n                return tool.value  # type: ignore\n            model = schema(**completion.tool.model_dump())  # type: ignore\n            model._completion = completion\n            return model\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return await self.async_extract(schema, retries - 1)\n            raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.messages","title":"<code>messages: list[ChatCompletionMessageParam]</code>  <code>property</code>","text":"<p>Returns this prompt's list of <code>ChatCompletionMessageParam</code> instances.</p>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.async_create","title":"<code>async_create(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>OpenAIPrompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>async_create</code> call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIChatCompletion</code> <p>An <code>OpenAIChatCompletion</code> instance.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>async def async_create(self, **kwargs: Any) -&gt; OpenAIChatCompletion:\n    \"\"\"Makes an asynchronous call to the model using this `OpenAIPrompt`.\n\n    Args:\n        **kwargs: Additional keyword arguments to pass to the `async_create` call.\n\n    Returns:\n        An `OpenAIChatCompletion` instance.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    client = AsyncOpenAI(base_url=self.call_params.base_url)\n    if self.call_params.async_wrapper is not None:\n        client = self.call_params.async_wrapper(client)\n    tools = kwargs.pop(\"tools\") if \"tools\" in kwargs else []\n    if self.call_params.tools:\n        tools.extend(self.call_params.tools)\n    converted_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, self, converted_tools)\n    completion_start_time = datetime.datetime.now().timestamp() * 1000\n    completion = await client.chat.completions.create(\n        stream=False,\n        **kwargs,\n    )\n    return OpenAIChatCompletion(\n        completion=completion,\n        tool_types=converted_tools,\n        start_time=completion_start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.async_extract","title":"<code>async_extract(schema, retries=0)</code>  <code>async</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call.</p> <p>The given schema is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>retries</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <p>Returns:</p> Type Description <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>async def async_extract(self, schema, retries=0):\n    \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n    The given schema is converted into an `OpenAITool`, complete with a description\n    of the tool, all of the fields, and their types. This allows us to take\n    advantage of OpenAI's tool/function calling functionality to extract information\n    from a prompt according to the context provided by the `BaseModel` schema.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        retries: The maximum number of times to retry the query on validation error.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    return_tool = True\n    openai_tool = schema\n    if is_base_type(schema):\n        openai_tool = OpenAITool.from_base_type(schema)  # type: ignore\n        return_tool = False\n    elif not isclass(schema):\n        openai_tool = OpenAITool.from_fn(schema)\n    elif not issubclass(schema, OpenAITool):\n        openai_tool = OpenAITool.from_model(schema)\n        return_tool = False\n\n    completion = await self.async_create(tools=[openai_tool])\n    try:\n        tool = completion.tool\n        if tool is None:\n            raise AttributeError(\"No tool found in the completion.\")\n        if return_tool:\n            return tool\n        if is_base_type(schema):\n            return tool.value  # type: ignore\n        model = schema(**completion.tool.model_dump())  # type: ignore\n        model._completion = completion\n        return model\n    except (AttributeError, ValueError, ValidationError) as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: include failure in retry prompt.\n            return await self.async_extract(schema, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.async_stream","title":"<code>async_stream(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response for an asynchronous call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>async_stream</code> call.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[OpenAIChatCompletionChunk, None]</code> <p>A <code>OpenAIChatCompletionChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>async def async_stream(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[OpenAIChatCompletionChunk, None]:\n    \"\"\"Streams the response for an asynchronous call to the model using `prompt`.\n\n    Args:\n        **kwargs: Additional keyword arguments to pass to the `async_stream` call.\n\n    Yields:\n        A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    client = AsyncOpenAI(base_url=self.call_params.base_url)\n    if self.call_params.async_wrapper is not None:\n        client = self.call_params.async_wrapper(client)\n    tools = convert_tools_list_to_openai_tools(self.call_params.tools)\n    patch_openai_kwargs(kwargs, self, tools)\n    stream = await client.chat.completions.create(\n        stream=True,\n        **kwargs,\n    )\n\n    async for chunk in stream:\n        yield OpenAIChatCompletionChunk(\n            chunk=chunk, tool_types=tools if tools else None\n        )\n</code></pre>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.create","title":"<code>create(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>OpenAIPrompt</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>create</code> call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIChatCompletion</code> <p>A <code>OpenAIChatCompletion</code> instance.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>def create(self, **kwargs: Any) -&gt; OpenAIChatCompletion:\n    \"\"\"Makes a call to the model using this `OpenAIPrompt` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments to pass to the `create` call.\n\n    Returns:\n        A `OpenAIChatCompletion` instance.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    client = OpenAI(base_url=self.call_params.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    tools = kwargs.pop(\"tools\") if \"tools\" in kwargs else []\n    if self.call_params.tools:\n        tools.extend(self.call_params.tools)\n    converted_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, self, converted_tools)\n    completion_start_time = datetime.datetime.now().timestamp() * 1000\n    completion = client.chat.completions.create(\n        stream=False,\n        **kwargs,\n    )\n    return OpenAIChatCompletion(\n        completion=completion,\n        tool_types=converted_tools,\n        start_time=completion_start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.extract","title":"<code>extract(schema, retries=0)</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call.</p> <p>The given schema is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>retries</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <p>Returns:</p> Type Description <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>def extract(self, schema, retries=0):\n    \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n    The given schema is converted into an `OpenAITool`, complete with a description\n    of the tool, all of the fields, and their types. This allows us to take\n    advantage of OpenAI's tool/function calling functionality to extract information\n    from a prompt according to the context provided by the `BaseModel` schema.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        retries: The maximum number of times to retry the query on validation error.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    return_tool = True\n    openai_tool = schema\n    if is_base_type(schema):\n        openai_tool = OpenAITool.from_base_type(schema)  # type: ignore\n        return_tool = False\n    elif not isclass(schema):\n        openai_tool = OpenAITool.from_fn(schema)\n    elif not issubclass(schema, OpenAITool):\n        openai_tool = OpenAITool.from_model(schema)\n        return_tool = False\n\n    completion = self.create(tools=[openai_tool])\n    try:\n        tool = completion.tool\n        if tool is None:\n            raise AttributeError(\"No tool found in the completion.\")\n        if return_tool:\n            return tool\n        if is_base_type(schema):\n            return tool.value  # type: ignore\n        model = schema(**completion.tool.model_dump())  # type: ignore\n        model._completion = completion\n        return model\n    except (AttributeError, ValueError, ValidationError) as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: include failure in retry prompt.\n            return self.extract(schema, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/prompt/#mirascope.openai.prompt.OpenAIPrompt.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>stream</code> call.</p> <code>{}</code> <p>Yields:</p> Type Description <code>OpenAIChatCompletionChunk</code> <p>A <code>OpenAIChatCompletionChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/prompt.py</code> <pre><code>def stream(self, **kwargs: Any) -&gt; Generator[OpenAIChatCompletionChunk, None, None]:\n    \"\"\"Streams the response for a call to the model using `prompt`.\n\n    Args:\n        **kwargs: Additional keyword arguments to pass to the `stream` call.\n\n    Yields:\n        A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    client = OpenAI(base_url=self.call_params.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    tools = convert_tools_list_to_openai_tools(self.call_params.tools)\n    patch_openai_kwargs(kwargs, self, tools)\n    stream = client.chat.completions.create(\n        stream=True,\n        **kwargs,\n    )\n    for chunk in stream:\n        yield OpenAIChatCompletionChunk(chunk=chunk, tool_types=tools)\n</code></pre>"},{"location":"api/openai/tools/","title":"openai.tools","text":"<p>Classes for using tools with OpenAI Chat APIs.</p>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>             Bases: <code>BaseTool</code></p> <p>A base class for easy use of tools with the OpenAI Chat client.</p> <p><code>OpenAITool</code> internally handles the logic that allows you to use tools with simple calls such as <code>OpenAIChatCompletion.tool</code> or <code>OpenAITool.fn</code>, as seen in the examples below.</p> <p>Example:</p> <pre><code>from mirascope import OpenAICallParams, BasePrompt, OpenAIChat, OpenAIToolStreamParser\n\n\ndef animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n    \"\"\"Tells you your most likely favorite animal from personality traits.\n\n    Args:\n        fav_food: your favorite food.\n        fav_color: your favorite color.\n\n    Returns:\n        The animal most likely to be your favorite based on traits.\n    \"\"\"\n    return \"Your favorite animal is the best one, a frog.\"\n\n\nclass AnimalPrompt(BasePrompt):\n    \"\"\"\n    Tell me my favorite animal if my favorite food is {food} and my\n    favorite color is {color}.\n    \"\"\"\n\n    food: str\n    color: str\n\n    call_params = OpenAICallParams(tools=[animal_matcher])\n\n\nprompt = AnimalPrompt(food=\"pizza\", color=\"red\")\nchat = OpenAIChat()\n\nresponse = chat.create(prompt)\ntool = response.tool\n\nprint(tool.fn(**tool.model_dump(exclude={\"tool_call\"})))\n#&gt; Your favorite animal is the best one, a frog.\n\nstream = chat.stream(prompt)\nparser = OpenAIToolStreamParser(tools=prompt.call_params.tools)\n\nfor tool in parser.from_stream(stream):\n    print(tool.fn(**tool.model_dump(exclude={\"tool_call\"})))\n#&gt; Your favorite animal is the best one, a frog.\n</code></pre> Source code in <code>mirascope/openai/tools.py</code> <pre><code>class OpenAITool(BaseTool):\n    '''A base class for easy use of tools with the OpenAI Chat client.\n\n    `OpenAITool` internally handles the logic that allows you to use tools with simple\n    calls such as `OpenAIChatCompletion.tool` or `OpenAITool.fn`, as seen in the\n    examples below.\n\n    Example:\n\n    ```python\n    from mirascope import OpenAICallParams, BasePrompt, OpenAIChat, OpenAIToolStreamParser\n\n\n    def animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n        \"\"\"Tells you your most likely favorite animal from personality traits.\n\n        Args:\n            fav_food: your favorite food.\n            fav_color: your favorite color.\n\n        Returns:\n            The animal most likely to be your favorite based on traits.\n        \"\"\"\n        return \"Your favorite animal is the best one, a frog.\"\n\n\n    class AnimalPrompt(BasePrompt):\n        \"\"\"\n        Tell me my favorite animal if my favorite food is {food} and my\n        favorite color is {color}.\n        \"\"\"\n\n        food: str\n        color: str\n\n        call_params = OpenAICallParams(tools=[animal_matcher])\n\n\n    prompt = AnimalPrompt(food=\"pizza\", color=\"red\")\n    chat = OpenAIChat()\n\n    response = chat.create(prompt)\n    tool = response.tool\n\n    print(tool.fn(**tool.model_dump(exclude={\"tool_call\"})))\n    #&gt; Your favorite animal is the best one, a frog.\n\n    stream = chat.stream(prompt)\n    parser = OpenAIToolStreamParser(tools=prompt.call_params.tools)\n\n    for tool in parser.from_stream(stream):\n        print(tool.fn(**tool.model_dump(exclude={\"tool_call\"})))\n    #&gt; Your favorite animal is the best one, a frog.\n    ```\n    '''\n\n    tool_call: ChatCompletionMessageToolCall\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n        A Mirascope `OpenAITool` is deconstructed into a JSON schema, and relevant keys\n        are renamed to match the OpenAI `ChatCompletionToolParam` schema used to make\n        function/tool calls in OpenAI API.\n\n        Returns:\n            The constructed `ChatCompletionToolParam` schema.\n\n        Raises:\n            ValueError: if the class doesn't have a docstring description.\n        \"\"\"\n        fn = super().tool_schema()\n        return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given `ChatCompletionMessageToolCall` from an OpenAI chat completion response,\n        takes its function arguments and creates an `OpenAITool` instance from it.\n\n        Args:\n            tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        try:\n            model_json = json.loads(tool_call.function.arguments)\n        except json.JSONDecodeError as e:\n            raise ValueError() from e\n\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, OpenAITool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, OpenAITool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseTypeT]) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a function.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given <code>ChatCompletionMessageToolCall</code> from an OpenAI chat completion response, takes its function arguments and creates an <code>OpenAITool</code> instance from it.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The <code>ChatCompletionMessageToolCall</code> to extract the tool from.</p> required <p>Returns:</p> Type Description <code>OpenAITool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given `ChatCompletionMessageToolCall` from an OpenAI chat completion response,\n    takes its function arguments and creates an `OpenAITool` instance from it.\n\n    Args:\n        tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    try:\n        model_json = json.loads(tool_call.function.arguments)\n    except json.JSONDecodeError as e:\n        raise ValueError() from e\n\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the OpenAI Chat client.</p> <p>A Mirascope <code>OpenAITool</code> is deconstructed into a JSON schema, and relevant keys are renamed to match the OpenAI <code>ChatCompletionToolParam</code> schema used to make function/tool calls in OpenAI API.</p> <p>Returns:</p> Type Description <code>ChatCompletionToolParam</code> <p>The constructed <code>ChatCompletionToolParam</code> schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the class doesn't have a docstring description.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n    A Mirascope `OpenAITool` is deconstructed into a JSON schema, and relevant keys\n    are renamed to match the OpenAI `ChatCompletionToolParam` schema used to make\n    function/tool calls in OpenAI API.\n\n    Returns:\n        The constructed `ChatCompletionToolParam` schema.\n\n    Raises:\n        ValueError: if the class doesn't have a docstring description.\n    \"\"\"\n    fn = super().tool_schema()\n    return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n</code></pre>"},{"location":"api/openai/types/","title":"openai.types","text":"<p>Type classes for interacting with the OpenAI Chat API.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>             Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the OpenAI Chat API with a prompt.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the OpenAI Chat API with a prompt.\"\"\"\n\n    model: str = \"gpt-3.5-turbo-0125\"\n    base_url: Optional[str] = None\n    wrapper: Optional[Callable[[OpenAI], OpenAI]] = None\n    async_wrapper: Optional[Callable[[AsyncOpenAI], AsyncOpenAI]] = None\n    frequency_penalty: Optional[float] = None\n    logit_bias: Optional[dict[str, int]] = None\n    logprobs: Optional[bool] = None\n    max_tokens: Optional[int] = None\n    n: Optional[int] = None\n    presence_penalty: Optional[float] = None\n    response_format: Optional[ResponseFormat] = None\n    seed: Optional[int] = None\n    stop: Union[Optional[str], list[str]] = None\n    temperature: Optional[float] = None\n    tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None\n    top_logprobs: Optional[int] = None\n    top_p: Optional[float] = None\n    user: Optional[str] = None\n    # Values defined below take precedence over values defined elsewhere. Use these\n    # params to pass additional parameters to the API if necessary that aren't already\n    # available as params.\n    extra_headers: Optional[Headers] = None\n    extra_query: Optional[Query] = None\n    extra_body: Optional[Body] = None\n    timeout: Optional[Union[float, Timeout]] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the keyword argument call parameters.\"\"\"\n        return {\n            key: value\n            for key, value in self.model_dump(\n                exclude={\"tools\", \"base_url\", \"wrapper\", \"async_wrapper\"}\n            ).items()\n            if value is not None\n        }\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallParams.kwargs","title":"<code>kwargs: dict[str, Any]</code>  <code>property</code>","text":"<p>Returns the keyword argument call parameters.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion","title":"<code>OpenAIChatCompletion</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around OpenAI chat completions.</p> <p>When using Mirascope's convenience wrappers to interact with OpenAI models via <code>OpenAIChat</code>, responses using <code>OpenAIChat.create()</code> will return a <code>OpenAIChatCompletion</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope import OpenAIChat\n\nchat = OpenAIChat()\nresponse = chat.create(\"What is 1 + 2?\")\n\nprint(response.choices)\n#&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n#  message=ChatCompletionMessage(content='1 + 2 equals 3.', role='assistant',\n#  function_call=None, tool_calls=None))]\n\nprint(response.message)\n#&gt; ChatCompletionMessage(content='1 + 2 equals 3.', role='assistant',\n#  function_call=None, tool_calls=None)\n\nprint(response.content)\n#&gt; 1 + 2 equals 3.\n</code></pre> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAIChatCompletion(BaseModel):\n    \"\"\"Convenience wrapper around OpenAI chat completions.\n\n    When using Mirascope's convenience wrappers to interact with OpenAI models via\n    `OpenAIChat`, responses using `OpenAIChat.create()` will return a\n    `OpenAIChatCompletion`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope import OpenAIChat\n\n    chat = OpenAIChat()\n    response = chat.create(\"What is 1 + 2?\")\n\n    print(response.choices)\n    #&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n    #  message=ChatCompletionMessage(content='1 + 2 equals 3.', role='assistant',\n    #  function_call=None, tool_calls=None))]\n\n    print(response.message)\n    #&gt; ChatCompletionMessage(content='1 + 2 equals 3.', role='assistant',\n    #  function_call=None, tool_calls=None)\n\n    print(response.content)\n    #&gt; 1 + 2 equals 3.\n    ```\n\n    \"\"\"\n\n    completion: ChatCompletion\n    tool_types: Optional[list[Type[OpenAITool]]] = None\n    start_time: float  # The start time of the completion in ms\n    end_time: float  # The end time of the completion in ms\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.completion.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.completion.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.completion.choices[0].message\n\n    @property\n    def content(self) -&gt; Optional[str]:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.completion.choices[0].message.content\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChatCompletionMessageToolCall]]:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @property\n    def tools(self) -&gt; Optional[list[OpenAITool]]:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[OpenAITool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls or len(self.tool_calls) == 0:\n            return None\n\n        tool_call = self.tool_calls[0]\n        for tool_type in self.tool_types:\n            if self.tool_calls[0].function.name == tool_type.__name__:\n                return tool_type.from_tool_call(tool_call)\n\n        return None\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the chat completion to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": self.completion.model_dump(),\n        }\n\n    def __str__(self):\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.content","title":"<code>content: Optional[str]</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.tool","title":"<code>tool: Optional[OpenAITool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.tool_calls","title":"<code>tool_calls: Optional[list[ChatCompletionMessageToolCall]]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.tools","title":"<code>tools: Optional[list[OpenAITool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.__str__","title":"<code>__str__()</code>","text":"<p>Returns the contained string content for the 0th choice.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def __str__(self):\n    \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n    return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletion.dump","title":"<code>dump()</code>","text":"<p>Dumps the chat completion to a dictionary.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the chat completion to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": self.completion.model_dump(),\n    }\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk","title":"<code>OpenAIChatCompletionChunk</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with OpenAI models via <code>OpenAIChat</code>, responses using <code>OpenAIChat.stream()</code> will return a <code>OpenAIChatCompletionChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope import OpenAIChat\n\nchat = OpenAIChat()\nstream = chat.stream(\"What is 1 + 2?\")\n\nfor chunk in stream:\n    print(chunk.content)\n#&gt; 1\n#  +\n#  2\n#   equals\n#\n#  3\n#  .\n</code></pre> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAIChatCompletionChunk(BaseModel):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with OpenAI models via\n    `OpenAIChat`, responses using `OpenAIChat.stream()` will return a\n    `OpenAIChatCompletionChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope import OpenAIChat\n\n    chat = OpenAIChat()\n    stream = chat.stream(\"What is 1 + 2?\")\n\n    for chunk in stream:\n        print(chunk.content)\n    #&gt; 1\n    #  +\n    #  2\n    #   equals\n    #\n    #  3\n    #  .\n    ```\n    \"\"\"\n\n    chunk: ChatCompletionChunk\n    tool_types: Optional[list[Type[OpenAITool]]] = None\n\n    @property\n    def choices(self) -&gt; list[ChunkChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChunkChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        return self.choices[0].delta\n\n    @property\n    def content(self) -&gt; Optional[str]:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return self.delta.content\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChoiceDeltaToolCall]]:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        return self.delta.tool_calls\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk.choice","title":"<code>choice: ChunkChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk.choices","title":"<code>choices: list[ChunkChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk.content","title":"<code>content: Optional[str]</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk.delta","title":"<code>delta: ChoiceDelta</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk.tool_calls","title":"<code>tool_calls: Optional[list[ChoiceDeltaToolCall]]</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAIChatCompletionChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the chunk content for the 0th choice.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n    return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/openai/utils/","title":"openai.utils","text":"<p>Utility functions for OpenAIChat.</p>"},{"location":"api/openai/utils/#mirascope.openai.utils.convert_tools_list_to_openai_tools","title":"<code>convert_tools_list_to_openai_tools(tools)</code>","text":"<p>Converts a list of <code>Callable</code> or <code>OpenAITool</code> instances to an <code>OpenAITool</code> list.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of functions or <code>OpenAITool</code>s.</p> required <p>Returns:</p> Type Description <code>Optional[list[Type[OpenAITool]]]</code> <p>A list of all items converted to <code>OpenAITool</code> instances.</p> Source code in <code>mirascope/openai/utils.py</code> <pre><code>def convert_tools_list_to_openai_tools(\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]],\n) -&gt; Optional[list[Type[OpenAITool]]]:\n    \"\"\"Converts a list of `Callable` or `OpenAITool` instances to an `OpenAITool` list.\n\n    Args:\n        tools: A list of functions or `OpenAITool`s.\n\n    Returns:\n        A list of all items converted to `OpenAITool` instances.\n    \"\"\"\n    if not tools:\n        return None\n    return [\n        tool if isclass(tool) else convert_function_to_tool(tool, OpenAITool)\n        for tool in tools\n    ]\n</code></pre>"},{"location":"api/openai/utils/#mirascope.openai.utils.patch_openai_kwargs","title":"<code>patch_openai_kwargs(kwargs, prompt, tools)</code>","text":"<p>Sets up the kwargs for an OpenAI API call.</p> <p>Kwargs are parsed as such: messages are formatted to have the required <code>role</code> and <code>content</code> items; tools (if any exist) are parsed into JSON schemas in order to fit the OpenAI API; tool choice, if not provided, is set to \"auto\". Other kwargs are left unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict[str, Any]</code> <p>The kwargs to patch.</p> required <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The prompt to use.</p> required <code>tools</code> <code>Optional[list[Type[OpenAITool]]]</code> <p>The tools to use, if any.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> Source code in <code>mirascope/openai/utils.py</code> <pre><code>def patch_openai_kwargs(\n    kwargs: dict[str, Any],\n    prompt: Optional[Union[BasePrompt, str]],\n    tools: Optional[list[Type[OpenAITool]]],\n) -&gt; None:\n    \"\"\"Sets up the kwargs for an OpenAI API call.\n\n    Kwargs are parsed as such: messages are formatted to have the required `role` and\n    `content` items; tools (if any exist) are parsed into JSON schemas in order to fit\n    the OpenAI API; tool choice, if not provided, is set to \"auto\". Other kwargs are\n    left unchanged.\n\n    Args:\n        kwargs: The kwargs to patch.\n        prompt: The prompt to use.\n        tools: The tools to use, if any.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n    \"\"\"\n    if prompt is None:\n        if \"messages\" not in kwargs:\n            raise ValueError(\"Either `prompt` or `messages` must be provided.\")\n    elif isinstance(prompt, str):\n        kwargs[\"messages\"] = [\n            ChatCompletionUserMessageParam(role=\"user\", content=prompt)\n        ]\n    else:\n        kwargs[\"messages\"] = prompt.messages\n        kwargs.update(prompt.call_params.kwargs)\n\n    if tools:\n        kwargs[\"tools\"] = [tool.tool_schema() for tool in tools]\n        if \"tool_choice\" not in kwargs:\n            kwargs[\"tool_choice\"] = \"auto\"\n</code></pre>"},{"location":"api/openai/models/","title":"openai.models","text":"<p>Classes for interactings with LLMs through Chat APIs.</p>"},{"location":"api/openai/models/openai_chat/","title":"openai.models.OpenAIChat","text":"<p>A convenience wrapper for the OpenAI Chat client.</p> <p>The Mirascope convenience wrapper for OpenAI provides a more user-friendly interface for interacting with their API. For more usage details, check out our examples.</p> <p>Example:</p> <pre><code>import os\n\nfrom mirascope import OpenAIChat, BasePrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"how to bake a cake\")\nmodel = OpenAIChat()\ncompletion = model.create(prompt)\n\nprint(completion)\n#&gt; Certinly! Here are some books on how to bake a cake:\n#  1. \"The Cake Bible\" by Rose Levy Beranbaum\n#  2. \"Joy of Baking\" by Irma S Rombauer and Marion Rombauer Becker\n#  ...\n</code></pre> Source code in <code>mirascope/openai/models/openai_chat.py</code> <pre><code>class OpenAIChat:\n    '''A convenience wrapper for the OpenAI Chat client.\n\n    The Mirascope convenience wrapper for OpenAI provides a more user-friendly interface\n    for interacting with their API. For more usage details, check out our examples.\n\n    Example:\n\n    ```python\n    import os\n\n    from mirascope import OpenAIChat, BasePrompt\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\n    class BookRecommendationPrompt(BasePrompt):\n        \"\"\"\n        Can you recommend some books on {topic}?\n        \"\"\"\n\n        topic: str\n\n\n    prompt = BookRecommendationPrompt(topic=\"how to bake a cake\")\n    model = OpenAIChat()\n    completion = model.create(prompt)\n\n    print(completion)\n    #&gt; Certinly! Here are some books on how to bake a cake:\n    #  1. \"The Cake Bible\" by Rose Levy Beranbaum\n    #  2. \"Joy of Baking\" by Irma S Rombauer and Marion Rombauer Becker\n    #  ...\n    ```\n\n    '''\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        client_wrapper: Optional[Callable] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initializes an instance of `OpenAIChat.\"\"\"\n        warnings.warn(\n            \"`OpenAIChat` is deprecated. Use `OpenAIPrompt` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if \"model\" in kwargs:\n            self.model = kwargs.pop(\"model\")\n            self.model_is_set = True\n        else:\n            self.model = \"gpt-3.5-turbo\"\n            self.model_is_set = False\n        self.client = OpenAI(api_key=api_key, base_url=base_url, **kwargs)\n        if client_wrapper is not None:\n            self.client = client_wrapper(self.client)\n\n    def create(\n        self,\n        prompt: Optional[Union[BasePrompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs: Any,\n    ) -&gt; OpenAIChatCompletion:\n        \"\"\"Makes a call to the model using `prompt`.\n\n        Args:\n            prompt: The prompt to use for the call. This can either be a `BasePrompt`\n                instance, a raw string, or `None`. If `prompt` is a `BasePrompt` instance,\n                then the call will use the `OpenAICallParams` in the prompt before\n                anything else. If `prompt` is `None`, then the call will attempt to use\n                the `messages` keyword argument.\n            tools: (Deprecated) A list of `OpenAITool` types or `Callable` functions\n                that the creation call can decide to use. If `tools` is provided,\n                `tool_choice` will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            A `OpenAIChatCompletion` instance.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        extract = kwargs.pop(\"extract\") if \"extract\" in kwargs else False\n        if isinstance(prompt, BasePrompt):\n            if self.model_is_set:\n                warnings.warn(\n                    \"The `model` parameter will be ignored when `prompt` is of type \"\n                    \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                    \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                    \"`BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            self.model = prompt.call_params.model\n\n            if tools is not None and not extract:\n                warnings.warn(\n                    \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                    \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            if prompt.call_params.tools is not None:\n                tools = prompt.call_params.tools\n\n        start_time = datetime.datetime.now().timestamp() * 1000\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n        kwargs[\"model\"] = self.model\n        return OpenAIChatCompletion(\n            completion=self.client.chat.completions.create(\n                stream=False,\n                **kwargs,\n            ),\n            tool_types=openai_tools if tools else None,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    def stream(\n        self,\n        prompt: Optional[Union[BasePrompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs: Any,\n    ) -&gt; Generator[OpenAIChatCompletionChunk, None, None]:\n        \"\"\"Streams the response for a call to the model using `prompt`.\n\n        Args:\n            prompt: The `BasePrompt` to use for the call.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Yields:\n            A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        if isinstance(prompt, BasePrompt):\n            if self.model_is_set:\n                warnings.warn(\n                    \"The `model` parameter will be ignored when `prompt` is of type \"\n                    \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                    \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                    \"`BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            self.model = prompt.call_params.model\n\n            if tools is not None:\n                warnings.warn(\n                    \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                    \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            if prompt.call_params.tools is not None:\n                tools = prompt.call_params.tools\n\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n        kwargs[\"model\"] = self.model\n        completion_stream = self.client.chat.completions.create(\n            stream=True,\n            **kwargs,\n        )\n\n        for chunk in completion_stream:\n            yield OpenAIChatCompletionChunk(\n                chunk=chunk,\n                tool_types=openai_tools if tools else None,\n            )\n\n    def extract(\n        self,\n        schema: Type[BaseModelT],\n        prompt: Optional[Union[BasePrompt, str]] = None,\n        retries: int = 0,\n        **kwargs: Any,\n    ) -&gt; BaseModelT:\n        \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n        The given schema is converted into an `OpenAITool`, complete with a description\n        of the tool, all of the fields, and their types. This allows us to take\n        advantage of OpenAI's tool/function calling functionality to extract information\n        from a prompt according to the context provided by the `BaseModel` schema.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            prompt: The prompt from which the schema will be extracted. If `prompt` is\n                a `BasePrompt` instance, then the call will use the `OpenAICallParams` in\n                the prompt.\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        tool = convert_base_model_to_tool(schema, OpenAITool)\n        completion = self.create(\n            prompt,\n            tools=[tool],\n            tool_choice={\n                \"type\": \"function\",\n                \"function\": {\"name\": tool.__name__},\n            },\n            extract=True,\n            **kwargs,\n        )\n\n        try:\n            model = schema(**completion.tool.model_dump())  # type: ignore\n            model._completion = completion\n            return model\n        except (AttributeError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: update this to include failure history once prompts can handle\n                # chat history properly.\n                return self.extract(schema, prompt, retries - 1)\n            raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/models/openai_chat/#mirascope.openai.models.OpenAIChat.__init__","title":"<code>__init__(api_key=None, base_url=None, client_wrapper=None, **kwargs)</code>","text":"<p>Initializes an instance of `OpenAIChat.</p> Source code in <code>mirascope/openai/models/openai_chat.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    client_wrapper: Optional[Callable] = None,\n    **kwargs: Any,\n):\n    \"\"\"Initializes an instance of `OpenAIChat.\"\"\"\n    warnings.warn(\n        \"`OpenAIChat` is deprecated. Use `OpenAIPrompt` instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if \"model\" in kwargs:\n        self.model = kwargs.pop(\"model\")\n        self.model_is_set = True\n    else:\n        self.model = \"gpt-3.5-turbo\"\n        self.model_is_set = False\n    self.client = OpenAI(api_key=api_key, base_url=base_url, **kwargs)\n    if client_wrapper is not None:\n        self.client = client_wrapper(self.client)\n</code></pre>"},{"location":"api/openai/models/openai_chat/#mirascope.openai.models.OpenAIChat.create","title":"<code>create(prompt=None, tools=None, **kwargs)</code>","text":"<p>Makes a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The prompt to use for the call. This can either be a <code>BasePrompt</code> instance, a raw string, or <code>None</code>. If <code>prompt</code> is a <code>BasePrompt</code> instance, then the call will use the <code>OpenAICallParams</code> in the prompt before anything else. If <code>prompt</code> is <code>None</code>, then the call will attempt to use the <code>messages</code> keyword argument.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>(Deprecated) A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIChatCompletion</code> <p>A <code>OpenAIChatCompletion</code> instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/models/openai_chat.py</code> <pre><code>def create(\n    self,\n    prompt: Optional[Union[BasePrompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs: Any,\n) -&gt; OpenAIChatCompletion:\n    \"\"\"Makes a call to the model using `prompt`.\n\n    Args:\n        prompt: The prompt to use for the call. This can either be a `BasePrompt`\n            instance, a raw string, or `None`. If `prompt` is a `BasePrompt` instance,\n            then the call will use the `OpenAICallParams` in the prompt before\n            anything else. If `prompt` is `None`, then the call will attempt to use\n            the `messages` keyword argument.\n        tools: (Deprecated) A list of `OpenAITool` types or `Callable` functions\n            that the creation call can decide to use. If `tools` is provided,\n            `tool_choice` will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        A `OpenAIChatCompletion` instance.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    extract = kwargs.pop(\"extract\") if \"extract\" in kwargs else False\n    if isinstance(prompt, BasePrompt):\n        if self.model_is_set:\n            warnings.warn(\n                \"The `model` parameter will be ignored when `prompt` is of type \"\n                \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                \"`BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self.model = prompt.call_params.model\n\n        if tools is not None and not extract:\n            warnings.warn(\n                \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        if prompt.call_params.tools is not None:\n            tools = prompt.call_params.tools\n\n    start_time = datetime.datetime.now().timestamp() * 1000\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n    kwargs[\"model\"] = self.model\n    return OpenAIChatCompletion(\n        completion=self.client.chat.completions.create(\n            stream=False,\n            **kwargs,\n        ),\n        tool_types=openai_tools if tools else None,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/openai/models/openai_chat/#mirascope.openai.models.OpenAIChat.extract","title":"<code>extract(schema, prompt=None, retries=0, **kwargs)</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call.</p> <p>The given schema is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModelT]</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The prompt from which the schema will be extracted. If <code>prompt</code> is a <code>BasePrompt</code> instance, then the call will use the <code>OpenAICallParams</code> in the prompt.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModelT</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/models/openai_chat.py</code> <pre><code>def extract(\n    self,\n    schema: Type[BaseModelT],\n    prompt: Optional[Union[BasePrompt, str]] = None,\n    retries: int = 0,\n    **kwargs: Any,\n) -&gt; BaseModelT:\n    \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n    The given schema is converted into an `OpenAITool`, complete with a description\n    of the tool, all of the fields, and their types. This allows us to take\n    advantage of OpenAI's tool/function calling functionality to extract information\n    from a prompt according to the context provided by the `BaseModel` schema.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        prompt: The prompt from which the schema will be extracted. If `prompt` is\n            a `BasePrompt` instance, then the call will use the `OpenAICallParams` in\n            the prompt.\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    tool = convert_base_model_to_tool(schema, OpenAITool)\n    completion = self.create(\n        prompt,\n        tools=[tool],\n        tool_choice={\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__},\n        },\n        extract=True,\n        **kwargs,\n    )\n\n    try:\n        model = schema(**completion.tool.model_dump())  # type: ignore\n        model._completion = completion\n        return model\n    except (AttributeError, ValidationError) as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: update this to include failure history once prompts can handle\n            # chat history properly.\n            return self.extract(schema, prompt, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/models/openai_chat/#mirascope.openai.models.OpenAIChat.stream","title":"<code>stream(prompt=None, tools=None, **kwargs)</code>","text":"<p>Streams the response for a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The <code>BasePrompt</code> to use for the call.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Yields:</p> Type Description <code>OpenAIChatCompletionChunk</code> <p>A <code>OpenAIChatCompletionChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/models/openai_chat.py</code> <pre><code>def stream(\n    self,\n    prompt: Optional[Union[BasePrompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs: Any,\n) -&gt; Generator[OpenAIChatCompletionChunk, None, None]:\n    \"\"\"Streams the response for a call to the model using `prompt`.\n\n    Args:\n        prompt: The `BasePrompt` to use for the call.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Yields:\n        A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    if isinstance(prompt, BasePrompt):\n        if self.model_is_set:\n            warnings.warn(\n                \"The `model` parameter will be ignored when `prompt` is of type \"\n                \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                \"`BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self.model = prompt.call_params.model\n\n        if tools is not None:\n            warnings.warn(\n                \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        if prompt.call_params.tools is not None:\n            tools = prompt.call_params.tools\n\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n    kwargs[\"model\"] = self.model\n    completion_stream = self.client.chat.completions.create(\n        stream=True,\n        **kwargs,\n    )\n\n    for chunk in completion_stream:\n        yield OpenAIChatCompletionChunk(\n            chunk=chunk,\n            tool_types=openai_tools if tools else None,\n        )\n</code></pre>"},{"location":"api/openai/models/openai_chat_async/","title":"openai.models.AsyncOpenAIChat","text":"<p>A convenience wrapper for the AsyncOpenAI Chat client.</p> <p>The Mirascope convenience wrapper for OpenAI provides a more user-friendly interface for interacting with their API. For more usage details, check out our examples.</p> <p>Example:</p> <pre><code>import asyncio\nimport os\n\nfrom mirascope import AsyncOpenAIChat, BasePrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"how to bake a cake\")\n\nmodel = AsyncOpenAIChat()\n\n\nasync def create_book_recommendation():\n    \"\"\"Asynchronously creates the response for a call to the model using `prompt`.\"\"\"\n    return await model.create(prompt)\n\n\nprint(asyncio.run(create_book_recommendation()))\n#&gt; Certinly! Here are some books on how to bake a cake:\n#  1. \"The Cake Bible\" by Rose Levy Beranbaum\n#  2. \"Joy of Baking\" by Irma S Rombauer and Marion Rombauer Becker\n#  ...\n</code></pre> Source code in <code>mirascope/openai/models/async_openai_chat.py</code> <pre><code>class AsyncOpenAIChat:\n    '''A convenience wrapper for the AsyncOpenAI Chat client.\n\n    The Mirascope convenience wrapper for OpenAI provides a more user-friendly interface\n    for interacting with their API. For more usage details, check out our examples.\n\n    Example:\n\n    ```python\n    import asyncio\n    import os\n\n    from mirascope import AsyncOpenAIChat, BasePrompt\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n    class BookRecommendationPrompt(BasePrompt):\n        \"\"\"\n        Can you recommend some books on {topic}?\n        \"\"\"\n\n        topic: str\n\n\n    prompt = BookRecommendationPrompt(topic=\"how to bake a cake\")\n\n    model = AsyncOpenAIChat()\n\n\n    async def create_book_recommendation():\n        \"\"\"Asynchronously creates the response for a call to the model using `prompt`.\"\"\"\n        return await model.create(prompt)\n\n\n    print(asyncio.run(create_book_recommendation()))\n    #&gt; Certinly! Here are some books on how to bake a cake:\n    #  1. \"The Cake Bible\" by Rose Levy Beranbaum\n    #  2. \"Joy of Baking\" by Irma S Rombauer and Marion Rombauer Becker\n    #  ...\n    ```\n    '''\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        client_wrapper: Optional[Callable] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Initializes an instance of `AsyncOpenAIChat.\"\"\"\n        warnings.warn(\n            \"`AsyncOpenAIChat` is deprecated. Use `OpenAIPrompt` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if \"model\" in kwargs:\n            self.model = kwargs.pop(\"model\")\n            self.model_is_set = True\n        else:\n            self.model = \"gpt-3.5-turbo\"\n            self.model_is_set = False\n        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url, **kwargs)\n        if client_wrapper is not None:\n            self.client = client_wrapper(self.client)\n\n    async def create(\n        self,\n        prompt: Optional[Union[BasePrompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs: Any,\n    ) -&gt; OpenAIChatCompletion:\n        \"\"\"Asynchronously makes a call to the model using `prompt`.\n\n        Args:\n            prompt: The prompt to use for the call. This can either be a `BasePrompt`\n                instance, a raw string, or `None`. If `prompt` is `None`, then the call\n                will attempt to use the `messages` keyword argument.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            A `OpenAIChatCompletion` instance.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        extract = kwargs.pop(\"extract\") if \"extract\" in kwargs else False\n        if isinstance(prompt, BasePrompt):\n            if self.model_is_set:\n                warnings.warn(\n                    \"The `model` parameter will be ignored when `prompt` is of type \"\n                    \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                    \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                    \"`BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            self.model = prompt.call_params.model\n\n            if tools is not None and not extract:\n                warnings.warn(\n                    \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                    \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            if prompt.call_params.tools is not None:\n                tools = prompt.call_params.tools\n\n        start_time = datetime.datetime.now().timestamp() * 1000\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n        kwargs[\"model\"] = self.model\n        return OpenAIChatCompletion(\n            completion=await self.client.chat.completions.create(\n                stream=False,\n                **kwargs,\n            ),\n            tool_types=openai_tools if tools else None,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def stream(\n        self,\n        prompt: Optional[Union[BasePrompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncGenerator[OpenAIChatCompletionChunk, None]:\n        \"\"\"Asynchronously streams the response for a call to the model using `prompt`.\n\n        Args:\n            prompt: The `BasePrompt` to use for the call.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/prompts/create\n\n        Yields:\n            A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        if isinstance(prompt, BasePrompt):\n            if self.model_is_set:\n                warnings.warn(\n                    \"The `model` parameter will be ignored when `prompt` is of type \"\n                    \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                    \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                    \"`BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            self.model = prompt.call_params.model\n\n            if tools is not None:\n                warnings.warn(\n                    \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                    \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            if prompt.call_params.tools is not None:\n                tools = prompt.call_params.tools\n\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n        kwargs[\"model\"] = self.model\n        completion_stream = await self.client.chat.completions.create(\n            stream=True,\n            **kwargs,\n        )\n\n        async for chunk in completion_stream:\n            yield OpenAIChatCompletionChunk(\n                chunk=chunk, tool_types=openai_tools if tools else None\n            )\n\n    async def extract(\n        self,\n        schema: Type[BaseModelT],\n        prompt: Optional[Union[BasePrompt, str]] = None,\n        retries: int = 0,\n        **kwargs: Any,\n    ) -&gt; BaseModelT:\n        \"\"\"Extracts the given schema from the response of a chat `create` call async.\n\n        The given schema is converted into an `OpenAITool`, complete with a description\n        of the tool, all of the fields, and their types. This allows us to take\n        advantage of OpenAI's tool/function calling functionality to extract information\n        from a prompt according to the context provided by the `BaseModel` schema.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            prompt: The prompt from which the schema will be extracted.\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        tool = convert_base_model_to_tool(schema, OpenAITool)\n        completion = await self.create(\n            prompt,\n            tools=[tool],\n            tool_choice={\n                \"type\": \"function\",\n                \"function\": {\"name\": tool.__name__},\n            },\n            extract=True,\n            **kwargs,\n        )\n\n        try:\n            model = schema(**completion.tool.model_dump())  # type: ignore\n            model._completion = completion\n            return model\n        except ValidationError as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: update this to include failure history once prompts can handle\n                # chat history properly.\n                return await self.extract(schema, prompt, retries - 1)\n            raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/models/openai_chat_async/#mirascope.openai.models.AsyncOpenAIChat.__init__","title":"<code>__init__(api_key=None, base_url=None, client_wrapper=None, **kwargs)</code>","text":"<p>Initializes an instance of `AsyncOpenAIChat.</p> Source code in <code>mirascope/openai/models/async_openai_chat.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    client_wrapper: Optional[Callable] = None,\n    **kwargs: Any,\n):\n    \"\"\"Initializes an instance of `AsyncOpenAIChat.\"\"\"\n    warnings.warn(\n        \"`AsyncOpenAIChat` is deprecated. Use `OpenAIPrompt` instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if \"model\" in kwargs:\n        self.model = kwargs.pop(\"model\")\n        self.model_is_set = True\n    else:\n        self.model = \"gpt-3.5-turbo\"\n        self.model_is_set = False\n    self.client = AsyncOpenAI(api_key=api_key, base_url=base_url, **kwargs)\n    if client_wrapper is not None:\n        self.client = client_wrapper(self.client)\n</code></pre>"},{"location":"api/openai/models/openai_chat_async/#mirascope.openai.models.AsyncOpenAIChat.create","title":"<code>create(prompt=None, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously makes a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The prompt to use for the call. This can either be a <code>BasePrompt</code> instance, a raw string, or <code>None</code>. If <code>prompt</code> is <code>None</code>, then the call will attempt to use the <code>messages</code> keyword argument.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIChatCompletion</code> <p>A <code>OpenAIChatCompletion</code> instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/models/async_openai_chat.py</code> <pre><code>async def create(\n    self,\n    prompt: Optional[Union[BasePrompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs: Any,\n) -&gt; OpenAIChatCompletion:\n    \"\"\"Asynchronously makes a call to the model using `prompt`.\n\n    Args:\n        prompt: The prompt to use for the call. This can either be a `BasePrompt`\n            instance, a raw string, or `None`. If `prompt` is `None`, then the call\n            will attempt to use the `messages` keyword argument.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        A `OpenAIChatCompletion` instance.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    extract = kwargs.pop(\"extract\") if \"extract\" in kwargs else False\n    if isinstance(prompt, BasePrompt):\n        if self.model_is_set:\n            warnings.warn(\n                \"The `model` parameter will be ignored when `prompt` is of type \"\n                \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                \"`BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self.model = prompt.call_params.model\n\n        if tools is not None and not extract:\n            warnings.warn(\n                \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        if prompt.call_params.tools is not None:\n            tools = prompt.call_params.tools\n\n    start_time = datetime.datetime.now().timestamp() * 1000\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n    kwargs[\"model\"] = self.model\n    return OpenAIChatCompletion(\n        completion=await self.client.chat.completions.create(\n            stream=False,\n            **kwargs,\n        ),\n        tool_types=openai_tools if tools else None,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/openai/models/openai_chat_async/#mirascope.openai.models.AsyncOpenAIChat.extract","title":"<code>extract(schema, prompt=None, retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call async.</p> <p>The given schema is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModelT]</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The prompt from which the schema will be extracted.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModelT</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/models/async_openai_chat.py</code> <pre><code>async def extract(\n    self,\n    schema: Type[BaseModelT],\n    prompt: Optional[Union[BasePrompt, str]] = None,\n    retries: int = 0,\n    **kwargs: Any,\n) -&gt; BaseModelT:\n    \"\"\"Extracts the given schema from the response of a chat `create` call async.\n\n    The given schema is converted into an `OpenAITool`, complete with a description\n    of the tool, all of the fields, and their types. This allows us to take\n    advantage of OpenAI's tool/function calling functionality to extract information\n    from a prompt according to the context provided by the `BaseModel` schema.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        prompt: The prompt from which the schema will be extracted.\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    tool = convert_base_model_to_tool(schema, OpenAITool)\n    completion = await self.create(\n        prompt,\n        tools=[tool],\n        tool_choice={\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__},\n        },\n        extract=True,\n        **kwargs,\n    )\n\n    try:\n        model = schema(**completion.tool.model_dump())  # type: ignore\n        model._completion = completion\n        return model\n    except ValidationError as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: update this to include failure history once prompts can handle\n            # chat history properly.\n            return await self.extract(schema, prompt, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/openai/models/openai_chat_async/#mirascope.openai.models.AsyncOpenAIChat.stream","title":"<code>stream(prompt=None, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously streams the response for a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[BasePrompt, str]]</code> <p>The <code>BasePrompt</code> to use for the call.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/prompts/create</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[OpenAIChatCompletionChunk, None]</code> <p>A <code>OpenAIChatCompletionChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/models/async_openai_chat.py</code> <pre><code>async def stream(\n    self,\n    prompt: Optional[Union[BasePrompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs: Any,\n) -&gt; AsyncGenerator[OpenAIChatCompletionChunk, None]:\n    \"\"\"Asynchronously streams the response for a call to the model using `prompt`.\n\n    Args:\n        prompt: The `BasePrompt` to use for the call.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/prompts/create\n\n    Yields:\n        A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    if isinstance(prompt, BasePrompt):\n        if self.model_is_set:\n            warnings.warn(\n                \"The `model` parameter will be ignored when `prompt` is of type \"\n                \"`BasePrompt` in favor of `OpenAICallParams.model` field inside of \"\n                \"`prompt`; version&gt;=0.3.0. Use `OpenAICallParams` inside of your \"\n                \"`BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        self.model = prompt.call_params.model\n\n        if tools is not None:\n            warnings.warn(\n                \"The `tools` parameter is deprecated; version&gt;=0.3.0. \"\n                \"Use `OpenAICallParams` inside of your `BasePrompt` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        if prompt.call_params.tools is not None:\n            tools = prompt.call_params.tools\n\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n    kwargs[\"model\"] = self.model\n    completion_stream = await self.client.chat.completions.create(\n        stream=True,\n        **kwargs,\n    )\n\n    async for chunk in completion_stream:\n        yield OpenAIChatCompletionChunk(\n            chunk=chunk, tool_types=openai_tools if tools else None\n        )\n</code></pre>"},{"location":"api/openai/parsers/","title":"openai.parsers","text":"<p>A module for interacting with Tool Parsers.</p>"},{"location":"api/openai/parsers/openai_tool_stream_parser/","title":"openai.parsers.OpenAIToolStreamParser","text":"<p>             Bases: <code>BaseModel</code></p> <p>A utility class to parse <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code>.</p> <p>Example:</p> <p>```python import os from typing import Callable, Literal, Type, Union</p> <p>from pydantic import Field</p> <p>from mirascope import (     OpenAICallParams,     OpenAIChat,     OpenAITool,     OpenAIToolStreamParser,     BasePrompt, )</p> <p>os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"</p> <p>def get_current_weather(     location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\" ) -&gt; str:     \"\"\"Returns the current weather in a given location.\"\"\"     return f\"{location} is 65 degrees {unit}.\"</p> <p>tools: list[Union[Callable, Type[OpenAITool]]] = [get_current_weather]</p> <p>class CurrentWeatherPrompt(BasePrompt):     \"\"\"What's the weather like in San Francisco, Tokyo, and Paris?\"\"\"</p> <pre><code>call_params = OpenAICallParams(\n    model=\"gpt-3.5-turbo-1106\",\n    tools=tools,  # pass in function itself for automatic conversion\n)\n</code></pre> <p>chat = OpenAIChat() prompt = CurrentWeatherPrompt() stream_completion = chat.stream(prompt) parser = OpenAIToolStreamParser(tools=tools) for tool in parser.from_stream(stream_completion):     print(tool)</p> Source code in <code>mirascope/openai/parsers/openai_parser.py</code> <pre><code>class OpenAIToolStreamParser(BaseModel):\n    '''A utility class to parse `OpenAIChatCompletionChunk`s into `OpenAITools`.\n\n    Example:\n\n    ```python\n    import os\n    from typing import Callable, Literal, Type, Union\n\n    from pydantic import Field\n\n    from mirascope import (\n        OpenAICallParams,\n        OpenAIChat,\n        OpenAITool,\n        OpenAIToolStreamParser,\n        BasePrompt,\n    )\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\n    def get_current_weather(\n        location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n    ) -&gt; str:\n        \"\"\"Returns the current weather in a given location.\"\"\"\n        return f\"{location} is 65 degrees {unit}.\"\n\n    tools: list[Union[Callable, Type[OpenAITool]]] = [get_current_weather]\n\n\n    class CurrentWeatherPrompt(BasePrompt):\n        \"\"\"What's the weather like in San Francisco, Tokyo, and Paris?\"\"\"\n\n        call_params = OpenAICallParams(\n            model=\"gpt-3.5-turbo-1106\",\n            tools=tools,  # pass in function itself for automatic conversion\n        )\n\n\n    chat = OpenAIChat()\n    prompt = CurrentWeatherPrompt()\n    stream_completion = chat.stream(prompt)\n    parser = OpenAIToolStreamParser(tools=tools)\n    for tool in parser.from_stream(stream_completion):\n        print(tool)\n    '''\n\n    tool_calls: list[ChatCompletionMessageToolCall] = []\n    tools: list[Union[Callable, Type[OpenAITool]]] = []\n\n    def from_stream(\n        self, stream: Generator[OpenAIChatCompletionChunk, None, None]\n    ) -&gt; Generator[OpenAITool, None, None]:\n        \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools`.\"\"\"\n        current_tool_type: Optional[Type[OpenAITool]] = None\n        for chunk in stream:\n            # Chunks start and end with None so we skip\n            if not chunk.tool_calls:\n                continue\n            # We are making what we think is a reasonable assumption here that\n            # tool_calls is never longer than 1. If it is, this will be updated.\n            tool_call_chunk = chunk.tool_calls[0]\n\n            if created_new_tool_call(self.tool_calls, tool_call_chunk):\n                current_tool_type = None\n\n            tool_call = self.tool_calls[tool_call_chunk.index]\n            if tool_call_chunk.id:\n                tool_call.id = tool_call_chunk.id\n\n            if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n                tool_class = find_tool_class(\n                    self.tool_calls, tool_call_chunk, self.tools\n                )\n                if tool_class:\n                    current_tool_type = tool_class\n\n            append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n            try:\n                if current_tool_type:\n                    tool_call = self.tool_calls[tool_call_chunk.index]\n                    yield current_tool_type.from_tool_call(tool_call)\n            except ValueError:\n                continue\n</code></pre>"},{"location":"api/openai/parsers/openai_tool_stream_parser/#mirascope.openai.parsers.OpenAIToolStreamParser.from_stream","title":"<code>from_stream(stream)</code>","text":"<p>Parses a stream of <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code>.</p> Source code in <code>mirascope/openai/parsers/openai_parser.py</code> <pre><code>def from_stream(\n    self, stream: Generator[OpenAIChatCompletionChunk, None, None]\n) -&gt; Generator[OpenAITool, None, None]:\n    \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools`.\"\"\"\n    current_tool_type: Optional[Type[OpenAITool]] = None\n    for chunk in stream:\n        # Chunks start and end with None so we skip\n        if not chunk.tool_calls:\n            continue\n        # We are making what we think is a reasonable assumption here that\n        # tool_calls is never longer than 1. If it is, this will be updated.\n        tool_call_chunk = chunk.tool_calls[0]\n\n        if created_new_tool_call(self.tool_calls, tool_call_chunk):\n            current_tool_type = None\n\n        tool_call = self.tool_calls[tool_call_chunk.index]\n        if tool_call_chunk.id:\n            tool_call.id = tool_call_chunk.id\n\n        if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n            tool_class = find_tool_class(\n                self.tool_calls, tool_call_chunk, self.tools\n            )\n            if tool_class:\n                current_tool_type = tool_class\n\n        append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n        try:\n            if current_tool_type:\n                tool_call = self.tool_calls[tool_call_chunk.index]\n                yield current_tool_type.from_tool_call(tool_call)\n        except ValueError:\n            continue\n</code></pre>"},{"location":"api/openai/parsers/openai_tool_stream_parser_async/","title":"openai.parsers.AsyncOpenAIToolStreamParser","text":"<p>             Bases: <code>BaseModel</code></p> <p>A utility class to parse <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code>.</p> <pre><code>This is an async version of `OpenAIToolStreamParser`.\n\nExample:\n\n```python\nimport asyncio\nimport os\nfrom typing import Callable, Literal, Type, Union\n\nfrom pydantic import Field\n\nfrom mirascope import (\n    AsyncOpenAIChat,\n    AsyncOpenAIToolStreamParser,\n    OpenAICallParams,\n    OpenAITool,\n    BasePrompt,\n    openai_tool_fn,\n)\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Returns the current weather in a given location.\"\"\"\n    return f\"{location} is 65 degrees {unit}.\"\n\ntools: list[Union[Callable, Type[OpenAITool]]] = [get_current_weather]\n\nclass CurrentWeatherPrompt(BasePrompt):\n    \"\"\"What's the weather like in San Francisco, Tokyo, and Paris?\"\"\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\",\n        tools=tools,\n    )\n\nasync def stream_openai_tool():\n    chat = AsyncOpenAIChat()\n    prompt = CurrentWeatherPrompt()\n    stream_completion = chat.stream(prompt)\n    parser = AsyncOpenAIToolStreamParser(tools=prompt.call_params.tools)\n    async for partial_tool in parser.from_stream(stream_completion):\n        print(\"data: \", partial_tool.__dict__, \"\n</code></pre> <p>\")</p> <pre><code>asyncio.run(stream_openai_tool())\n```\n</code></pre> Source code in <code>mirascope/openai/parsers/openai_parser_async.py</code> <pre><code>class AsyncOpenAIToolStreamParser(BaseModel):\n    '''A utility class to parse `OpenAIChatCompletionChunk`s into `OpenAITools`.\n\n    This is an async version of `OpenAIToolStreamParser`.\n\n    Example:\n\n    ```python\n    import asyncio\n    import os\n    from typing import Callable, Literal, Type, Union\n\n    from pydantic import Field\n\n    from mirascope import (\n        AsyncOpenAIChat,\n        AsyncOpenAIToolStreamParser,\n        OpenAICallParams,\n        OpenAITool,\n        BasePrompt,\n        openai_tool_fn,\n    )\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\n    def get_current_weather(\n        location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n    ) -&gt; str:\n        \"\"\"Returns the current weather in a given location.\"\"\"\n        return f\"{location} is 65 degrees {unit}.\"\n\n    tools: list[Union[Callable, Type[OpenAITool]]] = [get_current_weather]\n\n    class CurrentWeatherPrompt(BasePrompt):\n        \"\"\"What's the weather like in San Francisco, Tokyo, and Paris?\"\"\"\n\n        call_params = OpenAICallParams(\n            model=\"gpt-3.5-turbo-1106\",\n            tools=tools,\n        )\n\n    async def stream_openai_tool():\n        chat = AsyncOpenAIChat()\n        prompt = CurrentWeatherPrompt()\n        stream_completion = chat.stream(prompt)\n        parser = AsyncOpenAIToolStreamParser(tools=prompt.call_params.tools)\n        async for partial_tool in parser.from_stream(stream_completion):\n            print(\"data: \", partial_tool.__dict__, \"\\n\\n\")\n\n\n    asyncio.run(stream_openai_tool())\n    ```\n    '''\n\n    tool_calls: list[ChatCompletionMessageToolCall] = []\n    tools: list[Union[Callable, Type[OpenAITool]]] = []\n\n    async def from_stream(\n        self, stream: AsyncGenerator[OpenAIChatCompletionChunk, None]\n    ) -&gt; AsyncGenerator[OpenAITool, None]:\n        \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools` async.\"\"\"\n        current_tool_type: Optional[Type[OpenAITool]] = None\n        async for chunk in stream:\n            # Chunks start and end with None so we skip\n            if not chunk.tool_calls:\n                continue\n            # We are making what we think is a reasonable assumption here that\n            # tool_calls is never longer than 1. If it is, this will be updated.\n            tool_call_chunk = chunk.tool_calls[0]\n\n            if created_new_tool_call(self.tool_calls, tool_call_chunk):\n                current_tool_type = None\n\n            tool_call = self.tool_calls[tool_call_chunk.index]\n            if tool_call_chunk.id:\n                tool_call.id = tool_call_chunk.id\n\n            if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n                tool_class = find_tool_class(\n                    self.tool_calls, tool_call_chunk, self.tools\n                )\n                if tool_class:\n                    current_tool_type = tool_class\n\n            append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n            try:\n                if current_tool_type:\n                    tool_call = self.tool_calls[tool_call_chunk.index]\n                    yield current_tool_type.from_tool_call(tool_call)\n            except ValueError:\n                continue\n</code></pre>"},{"location":"api/openai/parsers/openai_tool_stream_parser_async/#mirascope.openai.parsers.AsyncOpenAIToolStreamParser.from_stream","title":"<code>from_stream(stream)</code>  <code>async</code>","text":"<p>Parses a stream of <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code> async.</p> Source code in <code>mirascope/openai/parsers/openai_parser_async.py</code> <pre><code>async def from_stream(\n    self, stream: AsyncGenerator[OpenAIChatCompletionChunk, None]\n) -&gt; AsyncGenerator[OpenAITool, None]:\n    \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools` async.\"\"\"\n    current_tool_type: Optional[Type[OpenAITool]] = None\n    async for chunk in stream:\n        # Chunks start and end with None so we skip\n        if not chunk.tool_calls:\n            continue\n        # We are making what we think is a reasonable assumption here that\n        # tool_calls is never longer than 1. If it is, this will be updated.\n        tool_call_chunk = chunk.tool_calls[0]\n\n        if created_new_tool_call(self.tool_calls, tool_call_chunk):\n            current_tool_type = None\n\n        tool_call = self.tool_calls[tool_call_chunk.index]\n        if tool_call_chunk.id:\n            tool_call.id = tool_call_chunk.id\n\n        if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n            tool_class = find_tool_class(\n                self.tool_calls, tool_call_chunk, self.tools\n            )\n            if tool_class:\n                current_tool_type = tool_class\n\n        append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n        try:\n            if current_tool_type:\n                tool_call = self.tool_calls[tool_call_chunk.index]\n                yield current_tool_type.from_tool_call(tool_call)\n        except ValueError:\n            continue\n</code></pre>"},{"location":"api/wandb/","title":"wandb","text":"<p>Weights and Biases integrated prompt for built-in logging functionality</p>"},{"location":"api/wandb/prompt/","title":"wandb.prompt","text":"<p>Prompts with WandB and OpenAI integration to support logging functionality.</p>"},{"location":"api/wandb/prompt/#mirascope.wandb.prompt.WandbPrompt","title":"<code>WandbPrompt</code>","text":"<p>             Bases: <code>OpenAIPrompt</code></p> <p>Parent class for inherited WandB functionality.</p> <p>Use this class's built in <code>trace</code> and <code>trace_error</code> methods to log traces to WandB.</p> <p>Example:</p> <pre><code>import os\nimport wandb\nfrom mirascope.wandb import WandbPrompt\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nclass HiPrompt(WandbPrompt):\n\"\"\"{greeting}.\"\"\"\n\ngreeting: str\n\nprompt = HiPrompt(span_type=\"llm\", greeting=\"Hello\")\ncompletion, span = prompt.create_with_trace()\n\nroot_span.log(name=\"mirascope_trace\")\n</code></pre> Source code in <code>mirascope/wandb/prompt.py</code> <pre><code>class WandbPrompt(OpenAIPrompt):\n    '''Parent class for inherited WandB functionality.\n\n    Use this class's built in `trace` and `trace_error` methods to log traces to WandB.\n\n    Example:\n\n    ```python\n    import os\n    import wandb\n    from mirascope.wandb import WandbPrompt\n\n    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n    wandb.init(project=\"wandb_logged_chain\")\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n    class HiPrompt(WandbPrompt):\n    \"\"\"{greeting}.\"\"\"\n\n    greeting: str\n\n    prompt = HiPrompt(span_type=\"llm\", greeting=\"Hello\")\n    completion, span = prompt.create_with_trace()\n\n    root_span.log(name=\"mirascope_trace\")\n    ```\n    '''\n\n    span_type: Literal[\"tool\", \"llm\", \"chain\", \"agent\"]\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n    def create_with_trace(\n        self, parent: Optional[Trace] = None\n    ) -&gt; tuple[Optional[OpenAIChatCompletion], Trace]:\n        \"\"\"Creates an OpenAI chat completion and logs it via a W&amp;B `Trace`.\n\n        Args:\n            parent: The parent trace to connect to.\n\n        Returns:\n            A tuple containing the completion and its trace (which has been connected\n                to the parent).\n        \"\"\"\n        try:\n            start_time = datetime.datetime.now().timestamp() * 1000\n            completion = super().create()\n            return completion, self._trace(completion, parent)\n        except Exception as e:\n            return None, self._trace_error(e, parent, start_time)\n\n    @overload\n    def extract_with_trace(\n        self, schema: Type[BaseTypeT], parent: Optional[Trace] = None, retries: int = 0\n    ) -&gt; tuple[BaseTypeT, Trace]:\n        ...  # pragma: no cover\n\n    @overload\n    def extract_with_trace(\n        self, schema: Type[BaseModelT], parent: Optional[Trace] = None, retries: int = 0\n    ) -&gt; tuple[BaseModelT, Trace]:\n        ...  # pragma: no cover\n\n    @overload\n    def extract_with_trace(\n        self, schema: Callable, parent: Optional[Trace] = None, retries: int = 0\n    ) -&gt; tuple[OpenAITool, Trace]:\n        ...  # pragma: no cover\n\n    def extract_with_trace(self, schema, parent=None, retries=0):\n        \"\"\"Calls an OpenAI extraction then logs the result via a W&amp;B `Trace`.\n\n        Args:\n            schema: The schema to extract.\n            parent: The parent trace to connect to.\n            retries: The number of times to retry the extraction.\n\n        Returns:\n            A tuple containing the completion and its trace (which has been connected\n                to the parent).\n        \"\"\"\n        try:\n            start_time = datetime.datetime.now().timestamp() * 1000\n            completion = super().extract(schema, retries)\n            return completion, self._trace(completion, parent)\n        except Exception as e:\n            return None, self._trace_error(e, parent, start_time)\n\n    def _trace(\n        self,\n        completion: Union[OpenAIChatCompletion, BaseModel],\n        parent: Optional[Trace],\n    ) -&gt; Trace:\n        \"\"\"Returns a trace connected to parent.\n\n        Args:\n            completion: The completion to trace. Handles `OpenAIChatCompletion` output\n                from both standard OpenAI chat completions, and `BaseModel` for\n                extractions.\n            parent: The parent trace to connect to.\n\n        Returns:\n            The created trace, connected to the parent.\n        \"\"\"\n        if isinstance(completion, OpenAIChatCompletion):\n            if completion.tool:\n                output = {\n                    \"assistant\": completion.tool.model_dump(),\n                    \"tool_output\": completion.tool.fn(**completion.tool.args),\n                }\n            else:\n                output = {\"assistant\": str(completion)}\n            open_ai_chat_completion = completion\n        elif isinstance(completion, BaseModel):\n            output = {\"assistant\": str(completion.model_dump())}\n            if not hasattr(completion, \"_completion\"):\n                raise ValueError(\n                    \"Completion of type `BaseModel` was not created using the `extract`\"\n                    \" function and does not contain the necessary `_completion` private\"\n                    \" attribute.\"\n                )\n            else:\n                open_ai_chat_completion = completion._completion\n        span = Trace(\n            name=self.__class__.__name__,\n            kind=self.span_type,\n            status_code=\"success\",\n            status_message=None,\n            metadata={\n                \"call_params\": dict(self.call_params),\n                \"usage\": dict(open_ai_chat_completion.completion.usage),  # type: ignore\n            },\n            start_time_ms=round(open_ai_chat_completion.start_time),\n            end_time_ms=round(open_ai_chat_completion.end_time),\n            inputs={message[\"role\"]: message[\"content\"] for message in self.messages},\n            outputs=output,\n        )\n        if parent:\n            parent.add_child(span)\n        return span\n\n    def _trace_error(\n        self, error: Exception, parent: Optional[Trace], start_time: float\n    ) -&gt; Trace:\n        \"\"\"Returns an error trace connected to parent.\n\n        Start time is set to time of prompt creation, and end time is set to the time\n        function is called.\n\n        Args:\n            error: The error to trace.\n            parent: The parent trace to connect to.\n            start_time: The time the call to OpenAI was started.\n\n        Returns:\n            The created error trace, connected to the parent.\n        \"\"\"\n        span = Trace(\n            name=self.__class__.__name__,\n            kind=self.span_type,\n            status_code=\"error\",\n            status_message=str(error),\n            metadata={\"call_params\": dict(self.call_params)},\n            start_time_ms=round(start_time),\n            end_time_ms=round(datetime.datetime.now().timestamp() * 1000),\n            inputs={message[\"role\"]: message[\"content\"] for message in self.messages},\n            outputs=None,\n        )\n        if parent:\n            parent.add_child(span)\n        return span\n</code></pre>"},{"location":"api/wandb/prompt/#mirascope.wandb.prompt.WandbPrompt.create_with_trace","title":"<code>create_with_trace(parent=None)</code>","text":"<p>Creates an OpenAI chat completion and logs it via a W&amp;B <code>Trace</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Optional[OpenAIChatCompletion], Trace]</code> <p>A tuple containing the completion and its trace (which has been connected to the parent).</p> Source code in <code>mirascope/wandb/prompt.py</code> <pre><code>def create_with_trace(\n    self, parent: Optional[Trace] = None\n) -&gt; tuple[Optional[OpenAIChatCompletion], Trace]:\n    \"\"\"Creates an OpenAI chat completion and logs it via a W&amp;B `Trace`.\n\n    Args:\n        parent: The parent trace to connect to.\n\n    Returns:\n        A tuple containing the completion and its trace (which has been connected\n            to the parent).\n    \"\"\"\n    try:\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = super().create()\n        return completion, self._trace(completion, parent)\n    except Exception as e:\n        return None, self._trace_error(e, parent, start_time)\n</code></pre>"},{"location":"api/wandb/prompt/#mirascope.wandb.prompt.WandbPrompt.extract_with_trace","title":"<code>extract_with_trace(schema, parent=None, retries=0)</code>","text":"<p>Calls an OpenAI extraction then logs the result via a W&amp;B <code>Trace</code>.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <p>The schema to extract.</p> required <code>parent</code> <p>The parent trace to connect to.</p> <code>None</code> <code>retries</code> <p>The number of times to retry the extraction.</p> <code>0</code> <p>Returns:</p> Type Description <p>A tuple containing the completion and its trace (which has been connected to the parent).</p> Source code in <code>mirascope/wandb/prompt.py</code> <pre><code>def extract_with_trace(self, schema, parent=None, retries=0):\n    \"\"\"Calls an OpenAI extraction then logs the result via a W&amp;B `Trace`.\n\n    Args:\n        schema: The schema to extract.\n        parent: The parent trace to connect to.\n        retries: The number of times to retry the extraction.\n\n    Returns:\n        A tuple containing the completion and its trace (which has been connected\n            to the parent).\n    \"\"\"\n    try:\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = super().extract(schema, retries)\n        return completion, self._trace(completion, parent)\n    except Exception as e:\n        return None, self._trace_error(e, parent, start_time)\n</code></pre>"},{"location":"concepts/attaching_and_calling_tool_functions/","title":"Attaching tool functions to Mirascope Prompts","text":""},{"location":"concepts/attaching_and_calling_tool_functions/#using-mirascope-openai-tool","title":"Using Mirascope OpenAI Tool","text":"<p>Create your prompt and pass in your <code>OpenAITool</code>:</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\n\nfrom mirascope import tool_fn\nfrom mirascope.openai import OpenAIPrompt, OpenAITool\n\n@tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nclass CurrentWeather(OpenAIPrompt):\n    \"\"\"What's the weather like in San Francisco, Tokyo, and Paris?\"\"\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\", tools=[GetCurrentWeather]\n    )\n</code></pre> <p>The tools are attached to the <code>call_params</code> attribute in a Mirascope Prompt. For more information check out Learn why colocation is so important and how combining it with the Mirascope CLI makes engineering better prompts easy.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#using-a-function-properly-documented-with-a-docstring","title":"Using a function properly documented with a docstring","text":"<p>Create your prompt and pass in your function:</p> <pre><code>import json\n\nfrom typing import Literal\n\nfrom mirascope.openai import OpenAIPrompt\n\n\ndef get_current_weather(\n        location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON object containing the location, temperature, and unit.\n    \"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\nclass CurrentWeather(OpenAIPrompt):\n    \"\"\"What's the weather like in San Francisco, Tokyo, and Paris?\"\"\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\", tools=[get_current_weather]\n    )\n</code></pre>"},{"location":"concepts/attaching_and_calling_tool_functions/#calling-tools","title":"Calling Tools","text":"<p>Generate content by calling the <code>create</code> method:</p> <pre><code># using same code as above\ncurrent_weather = CurrentWeather()\ncompletion = current_weather.create()\nif tools := completion.tools:\n    for tool in tools:\n        print(tool.fn(**tool.args))\n\n#&gt; {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n</code></pre> <p>The\u00a0<code>completion.tools</code>\u00a0property returns an actual instance of the tool.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#streaming-tools","title":"Streaming Tools","text":"<p>We also support streaming of tools using our <code>OpenAIToolStreamParser</code> class. Simply replace <code>create</code> with <code>stream</code> and call <code>from_stream</code>, like so:</p> <pre><code>from mirascope.openai import OpenAIToolStreamParser\n\n# using same code as above\ncurrent_weather = CurrentWeather()\ncompletion = current_weather.stream()\nparser = OpenAIToolStreamParser(tools=current_weather.call_params.tools)  # pass in the same tools\nfor tool in parser.from_stream(completion):\n    print(tool.fn(**tool.args))\n\n#&gt; {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n</code></pre> <p>Note, this will stream complete tools, not partial tools.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#async","title":"Async","text":"<p>All of the examples above also work with\u00a0<code>async</code>\u00a0by replacing <code>create</code> with <code>async_create</code> or <code>stream</code> with <code>async_stream</code> .</p> <p>If streaming, you will also need to replace <code>OpenAIToolStreamParser</code> with <code>AsyncOpenAIToolStreamParser</code> and change the <code>Generator</code> to an <code>AsyncGenerator</code></p> <pre><code>from mirascope.openai import AsyncOpenAIToolStreamParser\n\ncompletion = current_weather.async_stream()\nparser = AsyncOpenAIToolStreamParser(tools=current_weather.call_params.tools)\nasync for tool in parser.from_stream(completion):\n    print(tool.fn(**tool.args))\n</code></pre>"},{"location":"concepts/defining_and_extracting_schemas/","title":"Defining and extracting schemas","text":"<p>Mirascope is built on top of Pydantic. We will walk through the high-level concepts you need to know to get started extracting structured information with LLMs. We recommend reading their docs for more detailed explanations of everything that you can do with Pydantic.</p>"},{"location":"concepts/defining_and_extracting_schemas/#model","title":"Model","text":"<p>Defining the schema for extraction is done via models, which are classes that inherit from <code>pydantic.BaseModel</code>. We can then use a prompt to extract this schema:</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"The Name of the Wind by Patrick Rothfuss.\"\"\"\n\n\nbook = BookRecommendation().extract(Book)\nprint(book)\n#&gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>You can use tool classes like <code>OpenAITool</code> directly if you want to extract a single tool instead of just a schema (which is useful for calling attached functions).</p>"},{"location":"concepts/defining_and_extracting_schemas/#field","title":"Field","text":"<p>You can also use <code>pydantic.Fields</code> to add additional information for each field in your schema. Again, this information will be included in the prompt, and we can take advantage of that:</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel, Field\n\n\nclass Book(BaseModel):\n    title: str\n    author: str = Field(..., description=\"Last, First\")\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"The Name of the Wind by Patrick Rothfuss.\"\"\"\n\n\nbook = BookRecommendation().extract(Book)\nprint(book)\n#&gt; title='The Name of the Wind' author='Rothfuss, Patrick'\n</code></pre> <p>Notice how instead of \u201cPatrick Rothfuss\u201d the extracted author is \u201cRothfuss, Patrick\u201d as desired.</p>"},{"location":"concepts/defining_and_extracting_schemas/#retries","title":"Retries","text":"<p>Sometimes the model will fail to extract the schema. This can often be a result of the prompt; however, sometimes it\u2019s simply a failure of the model. If you want to retry the extraction some number of times, you can set <code>retries</code> equal to however many retries you want to run (defaults to 0).</p> <pre><code>book = BookRecommendation().extract(Book, retries=3)  # will retry up to 3 times \n</code></pre>"},{"location":"concepts/defining_and_extracting_schemas/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<p>In the above examples, we\u2019re extracting information present in the prompt text into structured form. We can also use <code>extract</code> to generate structured information from a prompt:</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"A fantasy book.\"\"\"\n\n    title: str\n    author: str\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"Please recommend a book.\"\"\"\n\nbook = BookRecommendation().extract(Book)\nprint(book)\n#&gt; title='Dune' author='Frank Herbert'\n</code></pre> <p>Notice that the docstring for the <code>Book</code> schema specified a science fiction book, which resulted in the model recommending a science fiction book. The docstring gets included with the prompt as part of the schema definition, and you can use this to your advantage for better prompting.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/","title":"Defining tools (function calls)","text":"<p>Tools are extremely useful when you want the model to intelligently choose to output the arguments to call one or more functions. With Mirascope it is extremely easy to use tools.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/#using-tools-in-mirascope","title":"Using tools in Mirascope","text":"<p>Mirascope will automatically convert any function properly documented with a docstring into a tool. This means that you can use any such function as a tool with no additional work. Create a function call, this one is taken from OpenAI documentation with Google style python docstrings:</p> <pre><code>import json\n\nfrom typing import Literal\n\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON object containing the location, temperature, and unit.\n    \"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n</code></pre> <p>You can also define your own\u00a0<code>OpenAITool</code>\u00a0class. This is necessary when the function you want to use as a tool does not have a docstring. Additionally, the\u00a0<code>OpenAITool</code>\u00a0class makes it easy to further update the descriptions, which is useful when you want to further engineer your prompt:</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\n\nfrom mirascope import tool_fn\nfrom mirascope.openai import OpenAITool\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n        # Assume this function does not have a docstring\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\n@tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n</code></pre> <p>Using the\u00a0tool_fn\u00a0decorator will attach the function defined by the tool to the tool for easier calling of the function. This happens automatically when using the function directly.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/#tools-with-openai-api-only","title":"Tools with OpenAI API only","text":"<p>Using the same OpenAI docs, the function call is defined as such:</p> <pre><code>import json\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n</code></pre> <p>OpenAI uses JSON Schema to define the tool call:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n</code></pre> <p>You can quickly see how bloated OpenAI tools become when defining multiple tools:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                },\n                \"required\": [\"location\", \"format\"],\n            },\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_n_day_weather_forecast\",\n            \"description\": \"Get an N-day weather forecast\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                    \"num_days\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The number of days to forecast\",\n                    }\n                },\n                \"required\": [\"location\", \"format\", \"num_days\"]\n            },\n        }\n    },\n]\n</code></pre> <p>With Mirascope, it will look like this:</p> <pre><code>class GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\n\nclass GetNDayWeatherForecast(GetCurrentWeather):\n    \"\"\"Get an N-day weather forecast\"\"\"\n\n    num_days: int = Field(..., description=\"The number of days to forecast\")\n</code></pre> <p>We can take advantage of class inheritance and reduce repetition. </p>"},{"location":"concepts/defining_tools_%28function_calls%29/#other-providers","title":"Other Providers","text":"<p>If you are using a function property documented with a docstring, you do not need to make any code changes when using other providers. Mirascope will automatically convert these functions to their proper format for you under the hood.</p> <p>For classes, simply replace <code>OpenAITool</code> with your provider of choice e.g. <code>GeminiTool</code> to match your choice of prompt.</p>"},{"location":"concepts/dumping_prompts/","title":"Dumping prompts","text":"<p>The <code>.dump()</code> function can be called from both prompts and completions to output a dictionary of associated data. </p>"},{"location":"concepts/dumping_prompts/#dumping-from-the-prompt","title":"Dumping from the Prompt","text":"<p>When called from <code>BasePrompt</code> or any of its subclasses, <code>.dump()</code> will give you:</p> <ul> <li>the prompt template</li> <li>inputs used to construct the prompt</li> <li>the prompt\u2019s tags</li> <li>any parameters specific to the model provider\u2019s API call, if they are not None:</li> <li>start and end times of its affiliated completion, if it has happened</li> </ul> <pre><code>import os\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAIPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n@tags([\"recommendation_project\", \"version:0001\"])\nclass BookRecommendationPrompt(OpenAIPrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"how to bake a cake\")\nprint(prompt.dump())\n\n\"\"\"\nOutput:\n{\n    \"template\": \"Can you recommend some books on {topic}?\",\n    \"inputs\": {\"api_key\": None, \"topic\": \"how to bake a cake\"},\n    \"tags\": [\"recommendation_project\", \"version:0001\"],\n    \"call_params\": {\"model\": \"gpt-3.5-turbo-0125\"},\n    \"start_time_ms\": None,\n    \"end_time_ms\": None,\n}\n\"\"\"\n\nprompt.create()\nprint(prompt.dump())\n\n\"\"\"\nOutput:\n{\n    # ... same as above\n    \"start_time_ms\": 1709847166609.473,\n    \"end_time_ms\": 1709847169424.146,\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts/#dumping-from-the-completion","title":"Dumping from the Completion","text":"<p>(Support for Gemini and Mistral completions coming soon\u2026)</p> <p>You can also call <code>.dump()</code> on chat completions themselves, which will contain:</p> <ul> <li>start and end times of the completion</li> <li>parameters of the call to the OpenAI API associated with the completion, within the key \u201coutput\u201d</li> </ul> <pre><code>completion = prompt.create() # prompt is an OpenAIPrompt, continued from above\nprint(completion.dump())\n\n\"\"\"\nOutput:\n{\n    \"output\": {\n        \"id\": \"chatcmpl-8zuVFGO2zgRsyckc9iW8CTSOgiNQm\",\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"index\": 0,\n                \"logprobs\": None,\n                \"message\": {\n                    \"content\": '1. \"The Cake Bible\" by Rose Levy Beranbaum...\n                    \"role\": \"assistant\",\n                    \"function_call\": None,\n                    \"tool_calls\": None,\n                },\n            }\n        ],\n        \"created\": 1709765897,\n        \"model\": \"gpt-3.5-turbo-0125\",\n        \"object\": \"chat.completion\",\n        \"system_fingerprint\": \"fp_2b778c6b35\",\n        \"usage\": {\"completion_tokens\": 210, \"prompt_tokens\": 19, \"total_tokens\": 229},\n    },\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts/#combining-both","title":"Combining Both","text":"<p>We also give you an option to see everything at once by calling <code>BasePrompt.dump(completion.dump())</code> , which will append the two dictionaries and display them in one. Note that the <code>.dump()</code> function will take any dictionary and append it to the data of the prompt, so feel free to use it flexibly to suit your needs.</p> <pre><code>print(prompt.dump(completion.dump()))\n\n\"\"\"\nOutput:\n{\n    \"template\": \"Can you recommend some books on {topic}?\",\n    \"inputs\": {\"api_key\": None, \"topic\": \"how to bake a cake\"},\n    \"tags\": [\"recommendation_project\", \"version:0001\"],\n    \"call_params\": {\"model\": \"gpt-3.5-turbo-0125\"},\n    \"start_time\": 1709837824962.49,\n    \"end_time\": 1709837825585.0588,\n    \"output\": {\n        \"id\": \"chatcmpl-8zuVFGO2zgRsyckc9iW8CTSOgiNQm\",\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"index\": 0,\n                \"logprobs\": None,\n                \"message\": {\n                    \"content\": '1. \"The Cake Bible\" by Rose Levy Beranbaum...\n                    \"role\": \"assistant\",\n                    \"function_call\": None,\n                    \"tool_calls\": None,\n                },\n            }\n        ],\n        \"created\": 1709765897,\n        \"model\": \"gpt-3.5-turbo-0125\",\n        \"object\": \"chat.completion\",\n        \"system_fingerprint\": \"fp_2b778c6b35\",\n        \"usage\": {\"completion_tokens\": 210, \"prompt_tokens\": 19, \"total_tokens\": 229},\n    },\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts/#logging","title":"Logging","text":"<p>Now that you have the JSON dump, it can be useful to log your completions:</p> <pre><code>\"\"\"A basic example on how to log the data from a prompt and a chat completion.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Optional\n\nimport pandas as pd\nfrom sqlalchemy import JSON, Float, Integer, MetaData, String, create_engine\nfrom sqlalchemy.dialects.postgresql import JSONB\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, sessionmaker\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAIPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\nlogger = logging.getLogger(\"mirascope\")\nTABLE_NAME = \"openai_chat_completions\"\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass OpenAIChatCompletionTable(Base):\n    __tablename__ = TABLE_NAME\n    id: Mapped[int] = mapped_column(\n        Integer(), primary_key=True, autoincrement=True, nullable=False\n    )\n    template: Mapped[str] = mapped_column(String(), nullable=False)\n    inputs: Mapped[Optional[dict]] = mapped_column(JSONB)\n    tags: Mapped[Optional[list[str]]] = mapped_column(JSON)\n    call_params: Mapped[Optional[dict]] = mapped_column(JSONB)\n    start_time: Mapped[Optional[float]] = mapped_column(Float(), nullable=False)\n    end_time: Mapped[Optional[float]] = mapped_column(Float(), nullable=False)\n    output: Mapped[Optional[dict]] = mapped_column(JSONB)\n\n\n@tags([\"recommendation_project\"])\nclass BookRecommendationPrompt(OpenAIPrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nUSERNAME = \"root\"\nPASSWORD = \"\"\nHOST = \"localhost\"\nPORT = \"5432\"\nDB_NAME = \"mirascope\"\nengine = create_engine(f\"postgresql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DB_NAME}\")\n\n\ndef create_database():\n    \"\"\"Create the database and table for the OpenAI chat completions.\"\"\"\n    metadata = MetaData()\n    table_objects = [Base.metadata.tables[TABLE_NAME]]\n    metadata.create_all(engine, tables=table_objects)\n\n\ndef log_to_database(prompt_completion: dict[str, Any]):\n    \"\"\"Create a prompt completion and log it to the database.\"\"\"\n    create_database()\n    Session = sessionmaker(engine)\n    with Session() as session:\n        openai_completion_db = OpenAIChatCompletionTable(**prompt_completion)\n        session.add(openai_completion_db)\n        session.commit()\n\n\ndef log_to_csv(prompt_completion: dict[str, Any]):\n    \"\"\"Log the prompt completion to a CSV file.\"\"\"\n    df = pd.DataFrame([prompt_completion])\n    with open(\"log.csv\", \"w\") as f:\n        df.to_csv(f, index=False)\n\n\ndef log_to_logger(prompt_completion: dict[str, Any]):\n    \"\"\"Log the prompt completion to the logger.\"\"\"\n    logger.info(prompt_completion)\n\n\nif __name__ == \"__main__\":\n    prompt = BookRecommendationPrompt(topic=\"how to bake a cake\")\n    completion = prompt.create()\n    prompt_completion = prompt.dump(completion.dump())\n    log_to_database(prompt_completion)\n    log_to_csv(prompt_completion)\n    log_to_logger(prompt_completion)\n</code></pre>"},{"location":"concepts/extracting_base_types/","title":"Extracting base types","text":"<p>Mirascope also makes it possible to extract base types without defining a <code>pydantic.BaseModel</code> with the same exact format for extraction:</p> <pre><code>from mirascope.openai import OpenAIPrompt\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"Please recommend some science fiction books.\"\"\"\n\n\nbooks = BookRecommendation().extract(list[str])\nprint(books)\n#&gt; ['Dune', 'Neuromancer', \"Ender's Game\", \"The Hitchhiker's Guide to the Galaxy\", 'Foundation', 'Snow Crash']\n</code></pre> <p>We currently support: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>list</code>, <code>set</code>, <code>tuple</code>, and <code>Enum</code>.</p> <p>We also support using <code>Union</code>, <code>Literal</code>, and <code>Annotated</code> </p> <p>Note</p> <p>If you\u2019re using <code>mypy</code> you\u2019ll need to add <code>#  type: ignore</code> due to how these types are handled differently by Python.</p>"},{"location":"concepts/extracting_base_types/#using-enum-or-literal-for-classification","title":"Using <code>Enum</code> or <code>Literal</code> for classification","text":"<p>One nice feature of extracting base types is that we can easily use <code>Enum</code> or <code>Literal</code> to define a set of labels that the model should use to classify the prompt. For example, let\u2019s classify whether or not some email text is spam:</p> <pre><code>from enum import Enum\n# from typing import Literal\n\nfrom mirascope.openai import OpenAIPrompt\n\n# Label = Literal[\"is spam\", \"is not spam\"]\n\n\nclass Label(Enum):\n    NOT_SPAM = \"not_spam\"\n    SPAM = \"spam\"\n\n\nclass NotSpam(OpenAIPrompt):\n    \"\"\"Your car insurance payment has been processed. Thank you for your business.\"\"\"\n\n\nclass Spam(OpenAIPrompt):\n    \"\"\"I can make you $1000 in just an hour. Interested?\"\"\"\n\n\n# assert NotSpam().extract(Label) == \"is not spam\"\n# assert Spam().extract(Label) == \"is spam\"\nassert NotSpam().extract(Label) == Label.NOT_SPAM\nassert Spam().extract(Label) == Label.SPAM\n</code></pre>"},{"location":"concepts/extracting_structured_information_using_llms/","title":"Extracting structured information with LLMs","text":"<p>Large Language Models (LLMs) are powerful at generating human-like text, but their outputs are inherently unstructured. Many real-world applications require structured data to function properly, such as extracting due dates, priorities, and task descriptions from user inputs for a task management application, or extracting tabular data from unstructured text sources for data analysis pipelines.</p> <p>Mirascope provides tools and techniques to address this challenge, allowing you to extract structured information from LLM outputs reliably.</p>"},{"location":"concepts/extracting_structured_information_using_llms/#challenges-in-extracting-structured-information","title":"Challenges in Extracting Structured Information","text":"<p>The key challenges in extracting structured information from LLMs include:</p> <ol> <li>Unstructured Outputs: LLMs are trained on vast amounts of unstructured text data, causing their outputs to be unstructured as well.</li> <li>Hallucinations and Inaccuracies: LLMs can sometimes generate factually incorrect information, complicating the extraction of accurate structured data.</li> </ol>"},{"location":"concepts/extracting_structured_information_using_llms/#mirascopes-approach","title":"Mirascope's Approach","text":"<p>Mirascope offers a convenient <code>extract</code> method on prompt classes to extract structured information from LLM outputs. This method leverages a combination of natural language processing techniques and heuristics to reliably extract the required structured data. While you can find more details in the following pages, let's consider a simple example where we want to extract task details like due date, priority, and description from a user's natural language input:</p> <pre><code>from typing import Literal\n\nfrom mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    description: str\n\n\nclass TaskExtractor(OpenAIPrompt):\n    \"\"\"\n    Extract the task details from the following task:\n\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract(TaskDetails)\nassert isinstance(task_details, TaskDetails)\nprint(TaskDetails)\n#&gt; due_date='next Friday' priority='high' description='Submit quarterly report'\n</code></pre> <p>As you can see, Mirascope makes this extremely simple. Under the hood, Mirascope uses the provided schema to extract the generated content and validate it (see Validation for more details).</p>"},{"location":"concepts/generating_content/","title":"Generating content","text":"<p>Now that you have your prompt, you can combine it with a model to generate content. Mirascope provides high-level wrappers around common providers so you can focus on prompt engineering instead of learning the interface for providers. Our high-level wrappers are not required to use our prompts but simply provide convenience if you wish to use it.</p> <p>Note</p> <p>This doc uses OpenAI. See using different model providers for how to generate content with other model providers like Anyscale, Together, Gemini, and more.</p>"},{"location":"concepts/generating_content/#openaiprompt","title":"OpenAIPrompt","text":"<p><code>OpenAIPrompt</code> extends <code>BasePrompt</code> by adding methods the OpenAI SDK provides such as <code>create</code>.</p>"},{"location":"concepts/generating_content/#create","title":"Create","text":"<p>You can initialize an <code>OpenAIPrompt</code> instance and call the <code>create</code> method to generate an <code>OpenAIChatCompletion</code>:</p> <pre><code>import os\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nrecipe = Recipe(ingredient=\"apples\")\ncompletion = recipe.create()\nprint(completion)  # prints the string content of the completion\n</code></pre> <p>The <code>call_params</code> of the OpenAI client is tied to the prompt. Refer to Engineering better prompts [Add link] for more information.</p>"},{"location":"concepts/generating_content/#async","title":"Async","text":"<p>If you are want concurrency, you can use the <code>async</code> function instead.</p> <pre><code>import asyncio\nimport os\n\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nasync def create_recipe_recommendation():\n    \"\"\"Asynchronously creates the response for a call to the model using `OpenAIPrompt`.\"\"\"\n    recipe = Recipe(ingredient=\"apples\")\n    return await recipe.create()\n\n\nprint(asyncio.run(create_recipe_recommendation())) \n</code></pre>"},{"location":"concepts/generating_content/#completion","title":"Completion","text":"<p>The <code>create</code> method returns an <code>OpenAIChatCompletion</code> class instance, which is a simple wrapper around the <code>ChatCompletion</code> class in <code>openai</code>. In fact, you can access everything from the original chunk as desired. The primary purpose of the class is to provide convenience.</p> <pre><code>from mirascope.openai.types import OpenAIChatCompletion\n\ncompletion = OpenAIChatCompletion(...)\n\ncompletion.completion  # ChatCompletion(...)\nstr(completion)        # original.choices[0].delta.content\ncompletion.choices     # original.choices\ncompletion.choice      # original.choices[0]\ncompletion.message     # original.choices[0].message\ncompletion.content     # original.choices[0].message.content\ncompletion.tool_calls  # original.choices[0].message.tool_calls\n</code></pre>"},{"location":"concepts/generating_content/#chaining","title":"Chaining","text":"<p>Adding a chain of calls is as simple as writing a function:</p> <pre><code>import os\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Chef(OpenAIPrompt):\n    \"\"\"\n    Name the best chef in the world at cooking {food_type} food\n    \"\"\"\n\n    food_type: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-1106\")\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend a recipe that uses {ingredient} as an ingredient\n    that chef {chef} would serve in their restuarant\n    \"\"\"\n\n    ingredient: str\n    chef: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-1106\")\n\n\ndef recipe_by_chef_using(ingredient: str, food_type: str) -&gt; str:\n    \"\"\"Returns a recipe using `ingredient`.\n\n    The recipe will be generated based on what dish using `ingredient`\n    the best chef in the world at cooking `food_type` might serve in\n    their restaurant\n    \"\"\"\n    chef_client = Chef(food_type=food_type)\n    recipe_client = Recipe(ingredient=ingredient, chef=chef_client.create())\n    return recipe_client.create()\n\n\nrecipe = recipe_by_chef_using(\"apples\", \"japanese\")\n</code></pre>"},{"location":"concepts/integrations/","title":"Integrations","text":""},{"location":"concepts/integrations/#client-wrappers","title":"Client Wrappers","text":"<p>If you want to use Mirascope in conjunction with another library which implements an OpenAI wrapper (such as LangSmith), you can do so easily by setting the <code>wrapper</code> parameter within <code>OpenAICallParams</code>. Setting this parameter will internally wrap the <code>OpenAI</code> client within an <code>OpenAIPrompt</code>, giving you access to both sets of functionalities.</p> <pre><code>from some_library import some_wrapper\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\",\n        wrapper=some_wrapper\n    )\n</code></pre> <p>Now, every call to <code>create()</code>, <code>async_create()</code>, <code>stream()</code>, and <code>async_stream()</code> will be executed on top of the wrapped <code>OpenAI</code> client.</p>"},{"location":"concepts/integrations/#weights-biases","title":"Weights &amp; Biases","text":"<p>If you want to seamlessly use Weights &amp; Biases\u2019 logging functionality, we\u2019ve got you covered -  <code>WandbPrompt</code> is an <code>OpenAIPrompt</code> with unique creation methods that internally call W&amp;B\u2019s <code>Trace()</code> function and log your runs. For standard chat completions, you can use <code>WandbPrompt.create_with_trace()</code>, and for extractions, you can use <code>WandbPrompt.extract_with_trace()</code>.</p>"},{"location":"concepts/integrations/#generating-content-with-a-wb-trace","title":"Generating Content with a W&amp;B Trace","text":"<p>The <code>create_with_trace()</code> function internally calls both <code>OpenAIPrompt.create()</code> and <code>wandb.Trace()</code> and is configured to properly log both successful completions and errors. </p> <p>Note that unlike a standard <code>OpenAIPrompt</code>, it requires the argument <code>type</code> to specify the type of <code>Trace</code> it initializes.  Once called, it will return a tuple of the <code>OpenAIChatCompletion</code>  and the created <code>Trace</code>. </p> <pre><code>import os\nfrom mirascope.wandb import WandbPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass ExplainPrompt(WandbPrompt):\n    \"\"\"Tell me more about {topic} in detail.\"\"\"\n    topic: str\n\n\nprompt = ExplainPrompt(type=\"llm\",topic=\"the Roman Empire\")\ncompletion, span = prompt.create_with_trace()\n\nspan.log(name=\"my_trace\")\n</code></pre> <p>In addition, <code>create_with_trace</code> can take an argument  <code>parent</code> for chained completions, and the initialized <code>Trace</code> will be linked with its parent within W&amp;B logs. </p> <pre><code>import os\nfrom mirascope.wandb import WandbPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass ExplainPrompt(WandbPrompt):\n    \"\"\"Tell me more about {topic} in detail.\"\"\"\n    topic: str\n\n\nclass SummaryPrompt(WandbPrompt):\n    \"\"\"Summarize the following: {text}\"\"\"\n    text: str\n\n\nexplain_prompt = ExplainPrompt(type=\"llm\",topic=\"the Roman Empire\")\nexplanation, explain_span = explain_prompt.create_with_trace()\n\nsummary_prompt = SummaryPrompt(type=\"llm\", text=explanation)\nsummary, summary_span = summary_prompt.create_with_trace(explain_span)\n\nexplain_span.log(name=\"my_trace\")\n</code></pre> <p>Since <code>WandbPrompt</code> inherits from <code>OpenAIPrompt</code>, it will support function calling the same way you would a standard <code>OpenAIPrompt</code>, as seen here</p>"},{"location":"concepts/integrations/#extracting-with-a-wb-trace","title":"Extracting with a W&amp;B Trace","text":"<p>When working with longer chains, it is often useful to use extractions so that data is passed along in a structured format. Just like <code>create_with_trace()</code> , you will need to pass in a <code>type</code> argument to the prompt and a <code>parent</code> to the extraction.</p> <pre><code>import os\nfrom mirascope.wandb import WandbPrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass InfoPrompt(WandbPrompt):\n    \"\"\"There are 7 oceans on earth.\"\"\"\n\n\ninfo_prompt = InfoPrompt(type=\"tool\")\ncompletion, span = info_prompt.extract_with_trace(\n    schema=int,\n    parent=root,\n)\n\nroot_span.log(name=\"mirascope_trace\")\n</code></pre> <p>To see <code>WandbPrompt</code> in further action, feel free to check out the cookbook example here.</p>"},{"location":"concepts/integrations/#langchain-and-langsmith","title":"LangChain and LangSmith","text":""},{"location":"concepts/integrations/#logging-a-langsmith-trace","title":"Logging a LangSmith trace","text":"<p>You can use client wrappers (as mentioned in the first section of this doc) to integrate Mirascope with LangSmith. When using a wrapper, you can generate content as you would with a normal <code>OpenAIPrompt</code>:</p> <pre><code>import os\nfrom langsmith import wrappers\n\nfrom mirascope.openai import OpenAIPrompt\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass BookRecommendationPrompt(OpenAIPrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\",\n        wrapper=wrappers.wrap_openai\n    )\n\ncompletion = BookRecommendationPrompt(topic=\"sci-fi\").create()\n</code></pre> <p>Now, if you log into LangSmith , you will be see your results have been traced. Of course, this integration works not just for <code>create()</code>, but also for <code>stream()</code> and <code>extract()</code>. For more details, check out the cookbook example here.</p>"},{"location":"concepts/integrations/#using-mirascope-prompts-with-langchain","title":"Using Mirascope prompts with LangChain","text":"<p>You may also want to use LangChain given it\u2019s tight integration with LangSmith. For us, one issue we had when we first started using LangChain was that their <code>invoke</code> function had no type-safety or lint help. This means that calling <code>invoke({\"foox\": \"foo\"})</code> was a difficult bug to catch. There\u2019s so much functionality in LangChain, and we wanted to make using it more pleasant.</p> <p>With Mirascope prompts, you can instantiate a <code>ChatPromptTemplate</code> from a Mirascope prompt template, and you can use the prompt\u2019s <code>model_dump</code> method so you don\u2019t have to worry about the invocation dictionary:</p> <pre><code>import os\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom mirascope import BasePrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass JokePrompt(BasePrompt):\n    \"\"\"\n    tell me a short joke about {topic}\n    \"\"\"\n\n    topic: str\n\n\njoke_prompt = JokePrompt(topic=\"ice cream\")\nprompt = ChatPromptTemplate.from_template(joke_prompt.template())\n# ^ instead of:\n# prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n\nmodel = ChatOpenAI(model=\"gpt-4\")\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\n\njoke = chain.invoke(joke_prompt.model_dump())\n# ^ instead of:\n# joke = chain.invoke({\"topic\": \"ice cream\"})\nprint(joke)\n</code></pre>"},{"location":"concepts/integrations/#want-more-integrations","title":"Want more integrations?","text":"<p>If there are features you\u2019d like that we haven\u2019t yet implemented, please submit a feature request to our GitHub Issues.</p> <p>We also welcome and greatly appreciate contributions if you\u2019re interested in helping us out!</p>"},{"location":"concepts/streaming_generated_content/","title":"Streaming generated content","text":"<p>Streaming generated content is similar to Generating Content so check that out if you haven\u2019t already.</p>"},{"location":"concepts/streaming_generated_content/#openaiprompt","title":"OpenAIPrompt","text":"<p>We will be using the same <code>OpenAIPrompt</code> in Generating Content. Feel free to swap it out with a different provider.</p>"},{"location":"concepts/streaming_generated_content/#streaming","title":"Streaming","text":"<p>You can use the <code>stream</code> method to stream a response. All this is doing is setting <code>stream=True</code> and providing the <code>OpenAIChatCompletionChunk</code> convenience wrappers around the response chunks.</p> <pre><code>import os\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nrecipe = Recipe(ingredient=\"apples\")\nstream = recipe.stream()\n\nfor chunk in stream:\n    print(str(chunk), end=\"\")\n</code></pre>"},{"location":"concepts/streaming_generated_content/#async","title":"Async","text":"<p>If you want concurrency, you can use the <code>async</code> function instead.</p> <pre><code>import os\n\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nasync def stream_recipe_recommendation():\n    \"\"\"Asynchronously streams the response for a call to the model using `OpenAIPrompt`.\"\"\"\n    recipe = Recipe(ingredient=\"apples\")\n    stream = recipe.async_stream()\n    async for chunk in astream:\n        print(chunk, end=\"\")\n\nasyncio.run(stream_recipe_recommendation())\n</code></pre>"},{"location":"concepts/streaming_generated_content/#openaichatcompletionchunk","title":"OpenAIChatCompletionChunk","text":"<p>The <code>stream</code> method returns an <code>OpenAIChatCompletionChunk</code> instance, which is a convenience wrapper around the <code>ChatCompletionChunk</code> class in <code>openai</code></p> <pre><code>from mirascope.openai.types import OpenAIChatCompletionChunk\n\nchunk = OpenAIChatCompletionChunk(...)\n\nchunk.chunk    # ChatCompletionChunk(...)\nstr(chunk)     # original.choices[0].delta.content\nchunk.choices  # original.choices\nchunk.choice   # original.choices[0]\nchunk.delta    # original.choices[0].delta\nchunk.content  # original.choices[0].delta.content\n</code></pre>"},{"location":"concepts/tools_%28function_calling%29/","title":"Tools (Function Calling)","text":"<p>Large Language Models (LLMs) are incredibly powerful at generating human-like text, but their capabilities extend far beyond mere text generation. With the help of tools (often called function calling), LLMs can perform a wide range of tasks, from mathematical calculations to code execution and information retrieval.</p>"},{"location":"concepts/tools_%28function_calling%29/#what-are-tools","title":"What are Tools?","text":"<p>Tools, in the context of LLMs, are essentially functions or APIs that the model can call upon to perform specific tasks. These tools can range from simple arithmetic operations to complex web APIs or custom-built functions. By leveraging tools, LLMs can augment their capabilities and provide more accurate and useful outputs.</p>"},{"location":"concepts/tools_%28function_calling%29/#why-are-tools-important","title":"Why are Tools Important?","text":"<p>Traditionally, LLMs have been limited to generating text based solely on their training data and the provided prompt. While this approach can produce impressive results, it also has inherent limitations. Tools allow LLMs to break free from these constraints by accessing external data sources, performing calculations, and executing code, among other capabilities.</p> <p>Incorporating tools into LLM workflows opens up a wide range of possibilities, including:</p> <ol> <li>Improved Accuracy: By leveraging external data sources and APIs, LLMs can provide more accurate and up-to-date information, reducing the risk of hallucinations or factual errors.</li> <li>Enhanced Capabilities: Tools allow LLMs to perform tasks that would be challenging or impossible with text generation alone, such as mathematical computations, code execution, and data manipulation.</li> <li>Contextualized Responses: By incorporating external data and contextual information, LLMs can provide more relevant and personalized responses, tailored to the user's specific needs and context.</li> </ol>"},{"location":"concepts/tools_%28function_calling%29/#tools-in-mirascope","title":"Tools in Mirascope","text":"<p>Mirascope provides a clean and intuitive way to incorporate tools into your LLM workflows. The simplest form-factor we offer is to extract a single tool automatically generated from a function. We can then call that function with the extracted arguments:</p> <pre><code>from mirascope.openai import OpenAIPrompt\n\n\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get's the weather for `location` and prints it.\n\n    Args:\n        location: The \"City, State\" or \"City, Country\" for which to get the weather.\n    \"\"\"\n    print(location)\n    if location == \"Tokyo, Japan\":\n        return f\"The weather in {location} is 72 degrees and sunny.\"\n    elif location == \"San Francisco, CA\":\n        return f\"The weather in {location} is 45 degrees and cloudy.\"\n    else:\n        return f\"I'm sorry, I don't have the weather for {location}.\"\n\n\nclass Weather(OpenAIPrompt):\n    \"\"\"What's the weather in Tokyo?\"\"\"\n\n\nweather_tool = Weather().extract(get_weather)\nprint(weather_tool.fn(**weather_tool.args))\n#&gt; The weather in Tokyo, Japan is 72 degrees and sunny.\n</code></pre> <p>Note</p> <p>While it may not be clear from the above example, <code>tool.fn</code> is an extremely powerful simplification. When using multiple tools, having the function attached to the tool makes it immediately accessible and callable with a single line of code.</p> <p>In the following pages, we\u2019ll go into greater detail around how to define and use tools effectively.</p>"},{"location":"concepts/using_different_model_providers/","title":"Using different model providers","text":"<p>Testing out various providers is a powerful way to boost the performance of your prompts. Mirascope makes it fast and simple to swap between various providers.</p> <p>We currently support the following providers:</p> <ul> <li>OpenAI</li> <li>Gemini</li> <li>Mistral (coming soon...)</li> <li>Claude (coming soon...)</li> </ul> <p>This also means that we support any providers that use these APIs.</p>"},{"location":"concepts/using_different_model_providers/#using-providers-that-support-the-openai-api","title":"Using providers that support the OpenAI API","text":"<p>If you want to use a provider that supports the OpenAI API, simply update the base_url. </p> <p>This works for endpoints such as:</p> <ul> <li>Ollama</li> <li>Anyscale</li> <li>Together</li> <li>Groq</li> <li>and more\u2026</li> </ul> <pre><code>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"OTHER_PROVIDER_API_KEY\"\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(base_url=\"BASE_URL\", model=...)\n</code></pre>"},{"location":"concepts/using_different_model_providers/#swapping-from-openai-to-gemini","title":"Swapping from OpenAI to Gemini","text":"<p>The generative-ai library and openai-python library are vastly different from each other, so swapping between them to attempt to gain better prompt responses is not worth the engineering effort and maintenance. </p> <p>This leads to people typically sticking with one provider even when providers release new features frequently. Take for example when Google announced Gemini 1.5, it would be very useful to implement prompts with the new context window. Thankfully, Mirascope makes this swap trivial.</p>"},{"location":"concepts/using_different_model_providers/#assuming-you-are-starting-with-openaiprompt","title":"Assuming you are starting with OpenAIPrompt","text":"<pre><code>import os\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nrecipe = Recipe(ingredient=\"apples\")\ncompletion = recipe.create()\nprint(completion)  # prints the string content of the completion\n</code></pre> <ol> <li>First install Mirascope\u2019s integration with gemini if you haven\u2019t already.</li> </ol> <pre><code>pip install mirascope[gemini]\n</code></pre> <ol> <li>Swap out OpenAI with Gemini:<ol> <li>Replace <code>OpenAIPrompt</code> and <code>OpenAICallParams</code> with <code>GeminiiPrompt</code> and <code>GeminiCallParams</code> respectively </li> <li>Configure your Gemini API Key</li> <li>Update <code>GeminiCallParams</code> with new model and other attributes</li> </ol> </li> </ol> <pre><code>from google.generativeai import configure\nfrom mirasope.gemini import GeminiPrompt, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass GeminiRecipe(GeminiPrompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n\n\nrecipe = GeminiRecipe(ingredient=\"apples\")\ncompletion = recipe.create()\nprint(completion)  # prints the string content of the completion\n</code></pre> <p>That\u2019s it for the basic example! Now you can evaluate the quality of your prompt with Gemini.</p>"},{"location":"concepts/using_different_model_providers/#something-a-bit-more-advanced","title":"Something a bit more advanced","text":"<p>What if you want to use a more complex message? The steps above are all the same except with one extra step.</p> <p>Consider this OpenAI example:</p> <pre><code>from mirascope import OpenAIPrompt, OpenAICallParams\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass Recipe(OpenAIPrompt):\n    \"\"\"A normal docstring\"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n    @property\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\"role\": \"system\", \"content\": \"You are the world's greatest chef.\"},\n            {\"role\": \"user\", \"content\": f\"Can you recommend some recipes that use {self.ingredient} as an ingredient?\"},\n        ]\n</code></pre> <p>The Gemini example will look like this:</p> <pre><code>from google.generativeai import configure\nfrom google.generativeai.types import ContentsType\nfrom mirasope.gemini import GeminiPrompt, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass Recipe(GeminiPrompt):\n    \"\"\"A normal docstring\"\"\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n\n    @property\n    def messages(self) -&gt; ContentsType:\n        return [\n            {\"role\": \"user\", \"content\": \"You are the world's greatest chef.\"},\n            {\"role\": \"model\", \"content\": \"I am the world's greatest chef.\"},\n            {\"role\": \"user\", \"content\": f\"Can you recommend some recipes that use {self.ingredient} as an ingredient?\"},\n        ]\n</code></pre> <p>Update the return type from <code>list[ChatCompletionMessageParam]</code> to <code>ContentsType</code> and the <code>messages</code> method. Gemini doesn\u2019t have a <code>system</code> role, so instead we need to simulate OpenAI\u2019s <code>system</code> message using a <code>user</code> \u2192 <code>model</code> pair. Refer to the providers documentation on how to format their messages array.</p>"},{"location":"concepts/using_different_model_providers/#swapping-to-another-provider","title":"Swapping to another provider","text":"<p>While the example above uses OpenAI to Gemini, the same applies to any provider that we support. If there\u2019s a provider you would like us to support request the feature on our GitHub Issues page or contribute a PR yourself.</p>"},{"location":"concepts/using_the_mirascope_cli/","title":"Using the Mirascope CLI","text":"<p>One of the main frustrations of dealing with prompts is keeping track of all the various revisions. Taking inspiration from alembic and git, the Mirascope CLI provides a couple of key commands to make managing prompts easier.</p>"},{"location":"concepts/using_the_mirascope_cli/#the-prompt-management-environment","title":"The prompt management environment","text":"<p>The first step to using the Mirascope CLI is to use the <code>init</code> command in your project's root directory.</p> <pre><code>mirascope init\n</code></pre> <p>This will create the directories and files to help manage prompts. Here is a sample structure created by the <code>init</code> function:</p> <pre><code>|\n|-- mirascope.ini\n|-- mirascope\n|   |-- prompt_template.j2\n|   |-- versions/\n|   |   |-- &lt;directory_name&gt;/\n|   |   |   |-- version.txt\n|   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n|-- prompts/\n</code></pre> <p>Here is a rundown of each directory and file:</p> <ul> <li><code>mirascope.ini</code> - The INI file that can be customized for your project</li> <li><code>mirascope</code> - The default name of the directory that is home to the prompt management environment</li> <li><code>prompt_template.j2</code> - The Jinja2 template file that is used to generate prompt versions</li> <li><code>versions</code> - The directory that holds the various prompt versions</li> <li><code>versions/&lt;directory_name</code> - The sub-directory that is created for each prompt file in the <code>prompts</code> directory</li> <li><code>version.txt</code> - A file system method of keeping track of current and latest revisions. Coming soon is revision tracking using a database instead</li> <li><code>&lt;revision_id&gt;_&lt;directory_name&gt;.py</code> - A prompt version that is created by the <code>mirascope add</code> command, more on this later.</li> <li><code>prompts</code> - The user's prompt directory that stores all prompt files</li> </ul> <p>The directory names can be changed anytime by modifying the <code>mirascope.ini</code> file or when running the <code>init</code> command.</p> <pre><code>mirascope init --mirascope_location my_mirascope --prompts_location my_prompts\n</code></pre>"},{"location":"concepts/using_the_mirascope_cli/#saving-your-first-prompt","title":"Saving your first prompt","text":"<p>After creating the prompt management directory, you are now ready to build and iterate on some prompts. Begin by adding a Mirascope Prompt to the prompts directory.</p> <pre><code># prompts/my_prompt.py\nfrom mirascope.openai import OpenAIPrompt, OpenAICallParams\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo\")\n</code></pre> <p>Once you are happy with the first iteration of this prompt, you can run:</p> <pre><code>mirascope add my_prompt\n</code></pre> <p>This will commit <code>my_prompt.py</code> to your <code>versions/</code> directory, creating a <code>my_prompt</code> sub-directory and a <code>0001_my_prompt.py</code>.</p> <p>Here is what <code>0001_my_prompt.py</code> will look like:</p> <pre><code># versions/my_prompt/0001_my_prompt.py\nfrom mirascope.openai import OpenAIPrompt, OpenAICallParams\n\nprev_revision_id = \"None\"\nrevision_id = \"0001\"\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo\")\n</code></pre> <p>The prompt inside the versions directory is almost identical to the prompt inside the prompts directory with a few differences.</p> <p>The variables <code>prev_revision_id</code> and <code>revision_id</code> will be used for features coming soon, so stay tuned for updates.</p>"},{"location":"concepts/using_the_mirascope_cli/#colocate","title":"Colocate","text":"<p>Everything that affects the quality of a prompt lives in the prompt. This is why <code>call_params</code> exists in Prompt and why <code>OpenAI</code> and other provider wrappers are combined with <code>BasePrompt</code>.</p>"},{"location":"concepts/using_the_mirascope_cli/#iterating-on-the-prompt","title":"Iterating on the prompt","text":"<p>Now that this version of <code>my_prompt</code> has been saved, you are now free to modify the original <code>my_prompt.py</code> and iterate. Maybe, you want to switch to a different provider and compare results.</p> <p>Here is what the next iteration of <code>my_prompt.py</code> will look like:</p> <pre><code># prompts/my_prompt.py\nfrom google.generativeai import configure\nfrom mirasope.gemini import GeminiPrompt, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass BookRecommendation(GeminiPrompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n</code></pre> <p>Before adding the next revision of <code>my_prompt</code>, you may want to check the status of your prompt.</p> <pre><code># You can specify a specific prompt\nmirascope status my_prompt\n\n# or, you can check the status of all prompts\nmirascope status\n</code></pre> <p>Note that status will also be checked before the <code>add</code> or <code>use</code> command is run. Now we can run the same <code>add</code> command in the previous section to commit another version <code>0002_my_prompt.py</code></p>"},{"location":"concepts/using_the_mirascope_cli/#switching-between-versions","title":"Switching between versions","text":"<p>You can now freely switch different providers or use the same provider with a different model to iterate to the best results.</p> <p>You can use the <code>use</code> command to quickly switch between the prompts:</p> <pre><code>mirascope use my_prompt 0001\n</code></pre> <p>Here you specify which prompt and also which version you want to use. This will update your <code>prompts/my_prompt.py</code> with the contents of <code>versions/0001_my_prompt.py</code> (minus the variables used internally).</p> <p>This will let you quickly swap prompts or providers with no code change, the exception being when prompts have different attributes. In that case, your linter will detect missing or additional attributes that need to be addressed.</p>"},{"location":"concepts/using_the_mirascope_cli/#removing-prompts","title":"Removing prompts","text":"<p>Often times when experimenting with prompts, a lot of experimental prompts will need to be cleaned up in your project.</p> <p>You can use the <code>remove</code> command to delete any version:</p> <pre><code>mirascope remove my_prompt 0001\n</code></pre> <p>Here you specify which prompt and version you want to remove. Removal will delete the file but also update any versions that have the deleted version in their <code>prev_revision_id</code> to <code>None</code>.</p> <p>Note</p> <p><code>mirascope remove</code> will not remove the prompt if <code>current_revision</code> is the same as the prompt you are trying to remove. You can use <code>mirascope add</code> if you have incoming changes or <code>mirascope use</code> to swap <code>current_revision</code>.</p>"},{"location":"concepts/using_the_mirascope_cli/#mirascope-ini","title":"Mirascope INI","text":"<p>The Mirascope INI provides some customization for you. Feel free to update any field.</p> <pre><code>[mirascope]\n\n# path to mirascope directory\nmirascope_location = .mirascope\n\n# path to versions directory\nversions_location = %(mirascope_location)s/versions\n\n# path to prompts directory\nprompts_location = prompts\n\n# name of versions text file\nversion_file_name = version.txt\n\n# formats the version file\n# leave blank to not format \nformat_command = ruff check --select I --fix; ruff format\n\n# auto tag prompts with version\nauto_tag = True\n</code></pre> <ul> <li><code>auto_tag</code> - Adds <code>@tags([\"version:0001\"])</code> to Mirascope Prompts. This will auto increment the version number if a new version is added.</li> </ul>"},{"location":"concepts/using_the_mirascope_cli/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to the Mirascope CLI. Here is a list in no order of things we are thinking about adding next:</p> <ul> <li>prompt comparison - A way to compare two different versions with a golden test</li> <li>history - View the revision history of a version</li> <li>testing - Adding input and outputs to the revision for CI testing</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"concepts/validation/","title":"Validation","text":"<p>When extracting structured information from LLMs, it\u2019s important that we validate the extracted information, especially the types. We want to make sure that if we\u2019re looking for an integer that we actual get an <code>int</code> back. One of the primary benefits of building on top of Pydantic is that it makes validation easy \u2014 in fact, we get validation on types out-of-the-box.</p> <p>We recommend you check out their thorough documentation for detailed information on everything you can do with their validators. This document will be brief and specifically related to LLM extraction to avoid duplication.</p>"},{"location":"concepts/validation/#validating-types","title":"Validating Types","text":"<p>When we extract information \u2014 for base types, <code>BaseModel</code>, or any of our tools \u2014 everything is powered by Pydantic. This means that we automatically get type validation and can handle it gracefully:</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Book(BaseModel):\n    title: str\n    price: float\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"Please recommend a book.\"\"\"\n\n\ntry:\n    book = BookRecommendation().extract(Book)\n    print(book)\n    #&gt; title='The Alchemist' price=12.99\nexcept ValidationError as e:\n    print(e)\n  #&gt; 1 validation error for Book\n  #  price\n  #    Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='standard', input_type=str]\n  #      For further information visit https://errors.pydantic.dev/2.6/v/float_parsing\n</code></pre> <p>Now we can proceed with our extracted information knowing that it will behave as the expected type.</p>"},{"location":"concepts/validation/#custom-validation","title":"Custom Validation","text":"<p>It\u2019s often useful to write custom validation when working with LLMs so that we can automatically handle things that are difficult to hard-code. For example, consider determining whether the generated content adheres to your company\u2019s guidelines. It\u2019s a difficult task to determine this, but an LLM is well-suited to do the task well.</p> <p>We can use an LLM to make the determination by adding an <code>AfterValidator</code> to our extracted output:</p> <pre><code>from enum import Enum\nfrom typing import Annotated\n\nfrom mirascope.openai import OpenAIPrompt\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\nclass Label(Enum):\n    HAPPY = \"happy story\"\n    SAD = \"sad story\"\n\n\nclass Sentiment(OpenAIPrompt):\n    \"\"\"Is the following happy or sad? {text}.\"\"\"\n\n    text: str\n\n\ndef validate_happy(story: str) -&gt; str:\n    \"\"\"Check if the content follows the guidelines.\"\"\"\n    label = Sentiment(text=story).extract(Label)\n    assert label == Label.HAPPY, \"Story wasn't happy.\"\n    return story\n\n\nclass StoryTeller(OpenAIPrompt):\n    \"\"\"Please tell me a story that's really sad.\"\"\"\n\n\nclass HappyStory(BaseModel):\n    story: Annotated[str, AfterValidator(validate_happy)]\n\n\ntry:\n    story = StoryTeller().extract(HappyStory)  # type: ignore\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for HappyStoryTool\n    #   story\n    #     Assertion failed, Story wasn't happy. [type=assertion_error, input_value=\"Once upon a time, there ...er every waking moment.\", input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n</code></pre>"},{"location":"concepts/why_were_building_mirascope/","title":"Why we\u2019re building Mirascope","text":"<p>When OpenAI first announced GPT-3.5 turbo model, we were excited to use it in our own backend API server. However, what we quickly realized was that we spent the majority of time building developer tooling around the OpenAI SDK rather than actually prompt engineering. When asking other developers how they are using the OpenAI SDK, we found that the answers were rather similar to our own situation. So we set out to build an Open Source Library such that others can prompt engineer with speed, robustness, and simplicity.</p>"},{"location":"concepts/why_were_building_mirascope/#pydantic-prompts","title":"Pydantic Prompts","text":"<p>Our internal tooling for working with GPT leveraged Pydantic which became the foundation for Mirascope.</p> <p>The <code>BasePrompt</code> class is the core of Mirascope, which extends Pydantic's <code>BaseModel</code>. The class leverages the power of python to make writing more complex prompts as easy and readable as possible. The docstring is automatically formatted as a prompt so that you can write prompts in the style of your codebase.</p>"},{"location":"concepts/why_were_building_mirascope/#why-use-mirascope","title":"Why use Mirascope?","text":""},{"location":"concepts/why_were_building_mirascope/#you-get-all-of-the-benefits-of-using-pydantic","title":"You get all of the benefits of using Pydantic","text":"<ul> <li>type hints, json schema, customization, ecosystem, production-grade</li> </ul>"},{"location":"concepts/why_were_building_mirascope/#speeds-up-development","title":"Speeds up development","text":"<ul> <li>Fewer bugs through validation</li> <li>Auto-complete, editor (and linter) support for errors</li> </ul>"},{"location":"concepts/why_were_building_mirascope/#easy-to-learn","title":"Easy to learn","text":"<ul> <li>You only need to learn Pydantic</li> </ul>"},{"location":"concepts/why_were_building_mirascope/#standardization-and-compatibility","title":"Standardization and compatibility","text":"<ul> <li>Integrations with other libraries that use JSON Schema such as OpenAPI and FastAPI means writing less code.</li> </ul>"},{"location":"concepts/why_were_building_mirascope/#customization","title":"Customization","text":"<ul> <li>Everything is Pydantic or basic python, so changing anything is as easy as overriding what you want to change</li> </ul> <p>All of the above helps lead to production ready code</p>"},{"location":"concepts/writing_prompts/","title":"Writing prompts","text":""},{"location":"concepts/writing_prompts/#the-baseprompt-class","title":"The <code>BasePrompt</code> Class","text":"<p>The docstring of the class acts as the prompt's template, and the attributes act as the template variables:</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nstr(prompt)\n</code></pre> <pre><code>Can you recommend some books on coding?\n</code></pre> <p>The <code>__str__</code> method, which formats all of the template variables, relies on the <code>template</code> function, which provides built-in string formatting so that you can write prettier docstrings. This means that longer prompts will still look well-formatted in your code base:</p> <pre><code>from mirascope import BasePrompt\n\n\nclass LongerPrompt(BasePrompt):\n    \"\"\"\n    Longer prompts can be edited in a more organized format that looks\n    better in your code base. Any unwanted characters such as newlines\n    or tabs that are purely for text alignment and structure will be\n    automatically removed.\n\n    For newlines, just add one extra (e.g. 2 newlines -&gt; 1 newline here)\n\n        - The same goes for things you want indented\n    \"\"\"\n</code></pre> <p>Note</p> <p>If you want custom docstring formatting or none at all, simply override the <code>template</code> method.</p>"},{"location":"concepts/writing_prompts/#editor-support","title":"Editor Support","text":"<ul> <li> <p>Inline Errors [Take new screenshots @Brendan Kao]</p> <p>https://github.com/Mirascope/mirascope/assets/15950811/4a1ebc7f-9f24-4106-a30c-87d68d88bf3c</p> </li> <li> <p>Autocomplete [Take new screenshots @Brendan Kao]</p> <p>https://github.com/Mirascope/mirascope/assets/15950811/a4c1e45b-d25e-438b-9d6d-b103c05ae054</p> </li> </ul>"},{"location":"concepts/writing_prompts/#template-variables","title":"Template Variables","text":"<p>When you call <code>str(prompt)</code> the template will be formatted using the properties of the class that match the template variables. This means that you can define more complex properties through code. This is particularly useful when you want to inject template variables with custom formatting or template variables that depend on multiple attributes.</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    Can you recommend some books on the following topic and genre pairs?\n\n        {topics_x_genres}\n    \"\"\"\n\n    topics: list[str]\n        genres: list[str]\n\n    @property\n    def topics_x_genres(self) -&gt; str:\n        \"\"\"Returns `topics` as a comma separated list.\"\"\"\n        return \"\\\\n\\\\t\".expandtabs(4).join(\n            [\n                f\"Topic: {topic}, Genre: {genre}\"\n                for topic in self.topics\n                for genre in self.genres\n            ]\n        )\n\n\nprompt = BookRecommendationPrompt(\n    topics=[\"coding\", \"music\"], genres=[\"fiction\", \"fantasy\"]\n)\nstr(prompt)\n</code></pre> <pre><code>Can you recommend some books on the following topic and genre pairs?\n    Topic: coding, Genre: fiction\n    Topic: coding, Genre: fantasy\n    Topic: music, Genre: fiction\n    Topic: music, Genre fantasy\n</code></pre>"},{"location":"concepts/writing_prompts/#messages","title":"Messages","text":"<p>By default, the <code>Prompt</code> class treats the prompt template as a single user message. If you want to specify a list of messages instead, use the message keywords SYSTEM, USER, ASSISTANT, or TOOL:</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages)\n</code></pre> <pre><code>[{\"role\": \"system\", \"content\": \"You are the world's greatest librarian\"}, {\"role\": \"user\", \"content\": \"Can you recommend some books on coding?\"}]\n</code></pre> <p>Note</p> <p>This example is using OpenAI messages, if you are using a different provider, refer to the provider's documentation on their message roles.</p>"},{"location":"concepts/writing_prompts/#magic-is-optional","title":"Magic is optional","text":"<p>We understand that there are users that do not want to use docstring magic. Mirascope allows the user to write the messages array manually, which has the added benefit of accessing functionality that is not yet supported by the docstring parser.</p> <pre><code>from mirascope import BasePrompt\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"A normal docstring\"\"\"\n\n    topic: str\n    history: list[Message] = []\n\n    @property\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\"role\": \"system\", \"content\": \"You are the world's greatest librarian.\"},\n            *self.history,\n            {\"role\": \"user\", \"content\": f\"Can you recommend some books on {self.topic}?\"},\n        ]\n</code></pre>"},{"location":"concepts/writing_prompts/#integrations-with-providers","title":"Integrations with Providers","text":"<p>The <code>BasePrompt</code> class should be used for providers that are not yet supported by Mirascope. Pass in the messages from the prompt into the LLM provider client messages array.</p> <pre><code>from mirascope import BasePrompt\nfrom some_llm_provider import LLMProvider\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nclient = LLMProvider(api_key=...)\nmessage = client.messages.create(\n    model=\"some-model\",\n    max_tokens=1000,\n    temperature=0.0,\n        stream=False,\n    messages=prompt.messages\n)\n</code></pre>"},{"location":"cookbook/langsmith/","title":"Integrating LangSmith with Mirascope","text":"<p>This recipe will walk you through how to use LangSmith\u2019s tracing functionality with Mirascope.</p>"},{"location":"cookbook/langsmith/#before-we-get-started","title":"Before we get started","text":"<p>You will need a LangSmith account and API key. Follow their docs on how to setup LangSmith. You will also need to have mirascope installed! Check out Getting Started if you have not installed mirascope yet. Finally, this example uses OpenAI so you will also need an OpenAI API Key.</p>"},{"location":"cookbook/langsmith/#quick-refresher-on-mirascope-llm-convenience-wrappers","title":"Quick Refresher on Mirascope LLM Convenience Wrappers","text":"<p>The <code>OpenAICallParams</code> class enable us to pass through parameters to the underlying client as keyword arguments. This means that Mirascope will have any support that LangSmith has with OpenAI.</p> <p>What this means is that Mirascope can take advantage of LangSmith\u2019s <code>wrappers</code> function around OpenAI.</p> <pre><code>from openai import OpenAI\nfrom langsmith import wrappers\n\nclient = wrappers.wrap_openai(OpenAI())\n</code></pre> <p>with Mirascope it looks like this:</p> <pre><code>from mirascope.openai import OpenAICallParams, OpenAIPrompt\nfrom langsmith import wrappers\n\nclass LangsmithPrompt(OpenAIPrompt):\n    \"\"\"Your prompt here\"\"\"\n\n    call_params = OpenAICallParams(wrapper=wrappers.wrap_openai)\n</code></pre> <p>\u2026and now you have all of LangSmith\u2019s integration with OpenAI with the power of Mirascope Prompts.</p>"},{"location":"cookbook/langsmith/#logging-a-langsmith-trace-using-mirascope","title":"Logging a LangSmith trace using Mirascope","text":""},{"location":"cookbook/langsmith/#setup-your-environment","title":"Setup your environment","text":"<p>Create a <code>.env</code> file with your secrets:</p> <pre><code>LANGCHAIN_API_KEY=ls__...\n\nOPENAI_API_KEY=...\n</code></pre> <p>We recommend leveraging Pydantic Settings when getting your secrets. Create a <code>config.py</code> file:</p> <pre><code>\"\"\"Global variables for LangSmith.\"\"\"\nfrom typing import Literal\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings):\n    openai_api_key: str = \"\"\n    langchain_tracing_v2: Literal[\"true\"] = \"true\"\n    langchain_api_key: str = \"\"\n\n    model_config = SettingsConfigDict(env_file=\".env\")\n</code></pre> <p>Note that we keep non-secret settings inside <code>config.py</code> such as <code>LANGCHAIN_TRACING_V2</code>.</p>"},{"location":"cookbook/langsmith/#create-prompt-and-chat","title":"Create prompt and chat","text":"<p>Now that the environment is setup, we will create <code>book_recommendation.py</code></p> <pre><code>import os\n\nfrom langsmith import wrappers\nfrom config import Settings\n\nfrom mirascope.openai import OpenAICallParams, OpenAIPrompt\n\nsettings = Settings()\n\nos.environ[\"LANGCHAIN_API_KEY\"] = settings.langchain_api_key\nos.environ[\"LANGCHAIN_TRACING_V2\"] = settings.langchain_tracing_v2\nos.environ[\"OPENAI_API_KEY\"] = settings.openai_api_key\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\", temperature=0.1, wrapper=wrappers.wrap_openai\n    )\n\n\nbook_recommendation = BookRecommendation(topic=\"how to bake a cake\")\ncompletion = book_recommendation.create()\nprint(completion)\n</code></pre>"},{"location":"cookbook/langsmith/#view-your-trace","title":"View your Trace","text":"<p>Login to LangSmith and view your results</p> <p></p>"},{"location":"cookbook/langsmith/#extract","title":"Extract","text":"<p>Yes, this also works for extract and stream. Here is a simple extract example to demonstrate this.</p> <pre><code>import os\n\nfrom langsmith import wrappers\nfrom config import Settings\nfrom pydantic import BaseModel\n\nfrom mirascope.openai import OpenAICallParams, OpenAIPrompt\n\nsettings = Settings()\n\nos.environ[\"LANGCHAIN_API_KEY\"] = settings.langchain_api_key\nos.environ[\"LANGCHAIN_TRACING_V2\"] = settings.langchain_tracing_v2\nos.environ[\"OPENAI_API_KEY\"] = settings.openai_api_key\n\nclass BookInfo(BaseModel):\n    \"\"\"A model for book info.\"\"\"\n\n    title: str\n    author: str\n\n\nclass BookRecommendation(OpenAIPrompt):\n    \"\"\"The Name of the Wind by Patrick Rothfuss.\"\"\"\n\n    call_params = OpenAICallParams(wrapper=wrappers.wrap_openai)\n\n\nbook_recommendation = BookRecommendation()\nbook_info = book_recommendation.extract(\n    BookInfo,\n    retries=5,\n)\nprint(book_info)\n</code></pre>"},{"location":"cookbook/langsmith/#adding-metadata-to-langsmith","title":"Adding Metadata to LangSmith","text":"<pre><code>book_recommendation = BookRecommendation()\ncompletion = book_recommendation.create(langsmith_extra={\"metadata\": {\"hello\": \"world\"}})\n</code></pre>"},{"location":"cookbook/langsmith/#langchain-specific","title":"LangChain-specific","text":"<p>There may be some integrations of LangSmith specific to LangChain. In situations like this, we recommend you use Mirascope to replace LangChain prompts. This will enable you to receive all the benefits from LangChain-specific integrations with LangSmith while also benefitting from Prompt validation and type hints.</p> <p>Check out our basic examples of using Mirascope with LangChain.</p>"},{"location":"cookbook/langsmith/#prompt-engineering-should-not-be-complex","title":"Prompt Engineering Should Not be Complex","text":"<p>By adding a few lines of code, you gain access to LangSmith\u2019s wide array of development, monitoring, and testing. While this cookbook focuses on LangSmith, one benefit of using Mirascope is that any tool that integrates with OpenAI will automatically integrate with Mirascope.</p>"},{"location":"cookbook/rag/","title":"Retrieval Augmented Generation (RAG)","text":"<p>When we need to provide additional information to the model that it hasn't yet been trained on, we can retrieve the relevant information from an external source and provide it as context to our model. For our example, we will use a dataset of\u00a0BBC news articles from 2004. We will:</p> <ul> <li>query the dataset with a topic of our choosing</li> <li>retrieve a number of relevant articles</li> <li>ask our chat model to summarize the relevant ones</li> <li>show how to query the dataset both locally and using a vector database (Pinecone)</li> <li>show how Mirascope can simplify RAG.</li> </ul> <p>Note</p> <p>The following code snippets have been moved around for the sake of clarity in the walkthrough, and may not work if you copy and paste them. For a fully functional script, take a look at the code in our repo.</p>"},{"location":"cookbook/rag/#before-we-get-started","title":"Before we get started","text":"<p>If you haven't already, it will be worth taking a look some of our relevant concept pages for a more detailed explanation of the functionality we'll be using in this walkthrough. Specifically, you'll want to know about how Pydantic allows us to integrate python functionality into our prompts.</p> <p>In addition, here are the variables we will be using in this cookbook recipe:</p> <pre><code># .env\n\nOPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\nPINECONE_API_KEY = \"YOUR_PINECONE_API_KEY\"\n</code></pre> <pre><code># rag_config.py\n\nfrom typing import Optional\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nMODEL = \"gpt-3.5-turbo\"\nEMBEDDINGS_MODEL = \"text-embedding-ada-002\"\nMAX_TOKENS = 1000\nTEXT_COLUMN = \"Text\"\nEMBEDDINGS_COLUMN = \"embeddings\"\nFILENAME = \"news_article_dataset.pkl\"\nURL = \"https://raw.githubusercontent.com/Dawit-1621/BBC-News-Classification/main/Data/BBC%20News%20Test.csv\"\nPINECONE_INDEX = \"news-articles\"\nPINECONE_NAMESPACE = \"articles\"\n\nclass Settings(BaseSettings):\n    openai_api_key: Optional[str] = None\n    pinecone_api_key: Optional[str] = None\n\n    model_config = SettingsConfigDict(env_file=\".env\")\n</code></pre>"},{"location":"cookbook/rag/#load-and-preprocess-the-data","title":"Load and preprocess the data","text":"<p>Before we load raw data into a pandas\u00a0<code>Dataframe</code>, we have to handle large texts which may exceed token limits. In our case, we are using <code>gpt-3.5-turbo</code>, which has a token limit of 4096. A crude solution is to split any article of token count greater than <code>MAX_TOKENS=1000</code> into equal chunks, with each resulting chunk consisting of fewer than <code>MAX_TOKENS</code>. This way we can fit up to 3 article snippets as well as any text in our hand-written section of the prompt. Note that we have also made sure that <code>MAX_TOKENS</code> is less than the token limit for our embedding model <code>text-embedding-ada-02</code> that has a token limit of 8191.</p> <p>The function\u00a0<code>split_text()</code>\u00a0below contains the implementation.</p> <p>For counting the number of tokens in an article, we will use\u00a0tiktoken\u00a0since we will be using OpenAI for both the chat and embedding models.\u00a0tiktoken\u00a0is a useful library provided by OpenAI for encoding strings and decoding tokens for their models.</p> <pre><code># rag_utils.py\n\nimport pandas as pd\nimport tiktoken\nfrom rag_config import MODEL, TEXT_COLUMN,\n\ndef load_data(url: str, max_tokens: int) -&gt; pd.DataFrame:\n    \"\"\"Loads data from a url after splitting larger texts into smaller chunks.\n\n    Args:\n        url: the url to load the data from.\n        max_tokens: the maximum number of tokens per chunk.\n    Returns:\n        The dataframe with the data from the url.\n    \"\"\"\n    df = pd.read_csv(url)\n    split_articles = []\n    encoder = tiktoken.encoding_for_model(MODEL)\n    for i, row in df.iterrows():\n        text = row[TEXT_COLUMN]\n        tokens = encoder.encode(text)\n        if len(tokens) &gt; max_tokens:\n            split_articles += split_text(text, tokens, max_tokens)\n            df.drop(i, inplace=True)\n\n    # Long texts which were dropped from the dataframe are now readded.\n    df = pd.concat(\n        [df, pd.DataFrame(split_articles, columns=[TEXT_COLUMN])], ignore_index=True\n    )\n\n    return df\n\n\ndef split_text(text: str, tokens: list[int], max_tokens: int) -&gt; list[str]:\n    \"\"\"Roughly splits a text into chunks according to max_tokens.\n\n    Text is split into equal word counts, with number of splits determined by how many\n    times `max_tokens` goes into the total number of tokens (including partially). Note\n    that tokens and characters do not have an exact correspondence, so in certain edge\n    cases a chunk may be slightly larger than max_tokens.\n\n    Args:\n        text: The text to split.\n        tokens: How many tokens `text` is.\n        max_tokens: The (rough) number of maximum tokens per chunk.\n    Returns:\n        A list of the split texts.\n    \"\"\"\n    words = text.split()\n    num_splits = len(tokens) // max_tokens + 1\n    split_texts = []\n    for i in range(num_splits):\n        start = i * len(words) // num_splits\n        end = (i + 1) * len(words) // num_splits\n        split_texts.append(\" \".join(words[start:end]))\n\n    return split_texts\n</code></pre> <pre><code># rag_example.py\n\ndf = load_data(url=URL, max_tokens=MAX_TOKENS)\n</code></pre> <p>For further clarity, here is an example of the output of <code>split_texts()</code>:</p> <pre><code># sample_output of split_texts()\n\ntext = \"...\"\ntokens = encoder.encode(text)\nprint(len(tokens))\n\n# Output: 3200\n\nsplit_texts = split_text(text=text, tokens=tokens, max_tokens=1000)\nprint([len(encoder.encode(split_text)) for split_text in split_texts])\n\n# Output: [800 800, 800, 800]\n# Explanation: 4 is the minimum number of times to split 3200 until each\n# piece is less than max_tokens=1000, so we get 3200/4 = 800.\n</code></pre> <p>Great! Now our <code>Dataframe</code> is loaded in with article snippets where each snippet is less than a thousand tokens.</p>"},{"location":"cookbook/rag/#embeddings","title":"Embeddings","text":"<p>To be able to take a topic of our choosing and determine each article\u2019s relevancy, we need to embed them. We define some helper functions to use OpenAI's embeddings:</p> <pre><code># rag_utils.py\n\nfrom typing import Union\n\nimport pandas as pd\nimport tiktoken\nfrom rag_config import EMBEDDINGS_COLUMN, EMBEDDINGS_MODEL, MODEL, Settings\nfrom openai import OpenAI\n\ndef embed_with_openai(text: Union[str, list[str]]) -&gt; list[list[float]]:\n    \"\"\"Embeds a string using OpenAI's embedding model.\n\n    Args:\n        text: A `str` or list of `str` to embed.\n        client: The `OpenAI` instance used for embedding.\n\n    Returns:\n        The embeddings of the text.\n    \"\"\"\n    if isinstance(text, str):\n        text = [text]\n    client = OpenAI(api_key=settings.openai_api_key)\n    embeddings_response = client.embeddings.create(model=EMBEDDINGS_MODEL, input=text)\n    return [datum.embedding for datum in embeddings_response.data]\n\n\ndef embed_df_with_openai(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Embeds a Pandas Series of texts in batches using minimal OpenAI calls.\n\n    Note that this function assumes all texts are less than 8192 tokens long.\n\n    Args:\n        texts: The texts to embed.\n        client: The `OpenAI` instance used for embedding.\n\n    Returns:\n        The dataframe with the embeddings column added.\n    \"\"\"\n    encoder = tiktoken.encoding_for_model(MODEL)\n    max_tokens = 8191\n\n    embeddings: list[list[float]] = []\n    batch: list[str] = []\n    batch_token_count = 0\n\n    # We can embed multiple texts in a single OpenAI call, so we implement a\n    # simple greedy algorithm according to ada-02's token limit of 8191.\n    for i, text in enumerate(df[TEXT_COLUMN]):\n        if batch_token_count + len(encoder.encode(text)) &gt; max_tokens:\n            embeddings += embed_with_openai(batch)\n            batch = [text]\n            batch_token_count = len(encoder.encode(text))\n        else:\n            batch.append(text)\n            batch_token_count += len(encoder.encode(text))\n\n    if batch:\n        embeddings += embed_with_openai(batch)\n\n    df[EMBEDDINGS_COLUMN] = embeddings\n    return df\n</code></pre> <p>We call these functions on our pandas <code>Dataframe</code> of article snippets, giving us the embedding of each article snippet in the new column <code>EMBEDDINGS_COLUMN=\"embeddings\"</code>.</p> <pre><code># rag_example.py\n\ndf = embed_df_with_openai(df=df)\n\n# df[TEXT_COLUMN] contains article snippets\n# df[EMBEDDINGS_COLUMN] contains embedding for each snippet\n</code></pre>"},{"location":"cookbook/rag/#retrieval-but-built-into-our-prompts","title":"Retrieval ... but built into our prompts","text":"<p>We mentioned earlier that we will show how to perform retrieval in two ways: locally and via a vector database (Pinecone). In your own projects, you may want to perform retrieval in an entirely different way.</p> <p>With any of Mirascope's prompts, we can use Python built-ins with Pydantic to implement complex, prompt-specific logic directly within the prompt itself \u2014 we can focus on prompt engineering, not the little things. This ensures that prompt-specific logic is well encapsulated, forcing a clean separation from the rest of the codebase. Furthermore, any updates to the prompt logic or template can be maintained and versioned with our CLI - check that out here.</p> <p>In this example, we are going to create two different prompt classes using <code>OpenAIPrompt</code>:</p> <ul> <li><code>LocalNewsRag</code>: this prompt class will use a local <code>pd.DataFrame</code> to find the relevant article chunks.</li> <li><code>PineconeNewsRag</code>: this prompt class will query a Pinecone vector database to find the relevant article chunks.</li> </ul> <p>The querying logic for relevant article retrieval will live within each prompt's <code>context</code> property, regardless of whether it is the local or vector database implementation. In the local iteration, we manually calculate (using the dot product) the distances between each article snippet's embedding and the embedding of our chosen topic - the articles whose embeddings are closest are then chosen. For the vector database, we make a Pinecone API call to perform the same task via their streamlined architecture.</p> <p>Note</p> <p>The querying logic for Pinecone lives within the PineconeNewsRagPrompt, but you must still do a one-time pinecone setup.</p>"},{"location":"cookbook/rag/#localnewsragprompt","title":"LocalNewsRagPrompt","text":"<pre><code># rag_prompts/local_news_rag_prompt.py\n\nimport numpy as np\nimport pandas as pd\nfrom pydantic import ConfigDict\nfrom rag_config import EMBEDDINGS_COLUMN, TEXT_COLUMN, Settings\nfrom rag_utils import embed_with_openai\n\nfrom mirascope.openai import OpenAIPrompt\n\nsettings = Settings()\n\n\nclass LocalNewsRag(OpenAIPrompt):\n    \"\"\"\n    SYSTEM:\n    You are an expert at:\n    1) determining the relevancy of articles to a topic, and\n    2) summarizing articles concisely and eloquently.\n\n    When given a topic and a list of possibly relevant texts, you format your responses\n    as a single list, where you summarize the articles relevant to the topic or explain\n    why the article is not relevant to the topic.\n\n    USER:\n    Here are {num_statements} article snippets about this topic: {topic}\n\n    {context}\n\n    Pick only the snippets which are truly relevant to the topic, and summarize them.\n    \"\"\"\n\n    num_statements: int\n    topic: str\n    df: pd.DataFrame\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def context(self) -&gt; str:\n        \"\"\"Finds most similar articles in dataframe using embeddings.\"\"\"\n\n        query_embedding = embed_with_openai(self.topic)[0]\n        self.df[\"similarities\"] = self.df[EMBEDDINGS_COLUMN].apply(\n            lambda x: np.dot(x, query_embedding)\n        )\n        most_similar = self.df.sort_values(\"similarities\", ascending=False).iloc[\n            : self.num_statements\n        ][TEXT_COLUMN]\n        statements = most_similar.to_list()\n        return \"\\n\".join(\n            [f\"{i+1}. {statement}\" for i, statement in enumerate(statements)]\n        )\n</code></pre>"},{"location":"cookbook/rag/#pineconenewsragprompt","title":"PineconeNewsRagPrompt","text":"<pre><code># rag_prompts/pinecone_news_rag_prompt.py\n\nimport pandas as pd\nfrom pinecone import Pinecone\nfrom pydantic import ConfigDict\nfrom rag_config import PINECONE_INDEX, PINECONE_NAMESPACE, TEXT_COLUMN, Settings\nfrom rag_utils import embed_with_openai\n\nfrom mirascope.openai import OpenAIPrompt\n\nsettings = Settings()\n\n\nclass PineconeNewsRag(OpenAIPrompt):\n    \"\"\"\n    SYSTEM:\n    You are an expert at:\n    1) determining the relevancy of articles to a topic, and\n    2) summarizing articles concisely and eloquently.\n\n    When given a topic and a list of possibly relevant texts, you format your responses\n    as a single list, where you summarize the articles relevant to the topic or explain\n    why the article is not relevant to the topic.\n\n    USER:\n    Here are {num_statements} article snippets about this topic: {topic}\n\n    {context}\n\n    Pick only the snippets which are truly relevant to the topic, and summarize them.\n    \"\"\"\n\n    num_statements: int\n    topic: str\n    df: pd.DataFrame\n\n    _index: Pinecone.Index\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        pc = Pinecone(api_key=settings.pinecone_api_key)\n        self._index = pc.Index(PINECONE_INDEX)\n\n    @property\n    def context(self) -&gt; str:\n        \"\"\"Finds most similar articles in pinecone using embeddings.\"\"\"\n        query_embedding = embed_with_openai(self.topic)[0]\n        query_response = self._index.query(\n            namespace=PINECONE_NAMESPACE,\n            vector=query_embedding,\n            top_k=self.num_statements,\n        )\n        indices = [int(article[\"id\"]) for article in query_response[\"matches\"]]\n        statements = self.df.iloc[indices][TEXT_COLUMN].to_list()\n        return \"\\n\".join(\n            [f\"{i+1}. {statement}\" for i, statement in enumerate(statements)]\n        )\n</code></pre>"},{"location":"cookbook/rag/#retrieval-script-for-topics","title":"Retrieval Script For Topics","text":"<p>We've built the retrieval functionality into the prompts themselves - now, we can build a script to use these prompts without having to worry about formatting the context or retrieving relevant articles:</p> <pre><code># rag_example.py\n\nimport os\nfrom argparse import ArgumentParser\n\nimport pandas as pd\nfrom rag_config import FILENAME, MAX_TOKENS, URL, Settings\nfrom local_news_rag_prompt import LocalNewsRagPrompt\nfrom pinecone_news_rag_prompt import PineconeNewsRagPrompt\nfrom setup_pinecone import setup_pinecone\nfrom rag_utils import embed_df_with_openai, load_data\n\nfrom mirascope import OpenAIChat\n\nsettings = Settings()\n\ndef main(use_pinecone=False):\n    chat = OpenAIChat(api_key=settings.openai_api_key)\n    df = load_data(url=URL, max_tokens=MAX_TOKENS)\n    df = embed_df_with_openai(df=df, client=chat.client)\n\n    # This method does nothing if a Pinecone index is already set up.\n    if use_pinecone:\n        setup_pinecone(df=df)\n\n    topics = [\n        \"soccer teams/players going through trouble\",\n        \"environmental factors affecting economy\",\n        \"celebrity or politician scandals\",\n    ]\n    for topic in topics:\n        if use_pinecone:\n            pinecone = PineconeNewsRag(num_statements=3, topic=topic, df=df)\n            print(pinecone.create())\n        else:\n            local = LocalNewsRag(num_statements=3, topic=topic, df=df)\n            print(local.create())\n        print(\"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Process some flags.\")\n    parser.add_argument(\n        \"-pc\", \"--pinecone\", action=\"store_true\", help=\"Activate Pinecone mode\"\n    )\n    args = parser.parse_args()\n    main(use_pinecone=args.pinecone)\n</code></pre>"},{"location":"cookbook/squad_extraction/","title":"SQuAD 2.0: Extracting Answers To Questions From Context Using Mirascope","text":"<p>The SQuAD 2.0 Dataset is a dataset for question-answering. Each question is either impossible to answer given the context paragraph or has answers exactly as written in the paragraph. It turns out that it's quite difficult to get an LLM to ignore it's world knowledge and output \"\" if the paragraph does not contain the answer. For this recipe, we'll be using a modified version of the dataset, restricting our questions to a single article (Geology) and removing all of the impossible questions. <p>Note</p> <p>The <code>geology-squad.json</code> modified version of the dataset is on our GitHub along with the full code for this recipe.</p> <p>The first step is loading the dataset. We've written a helper function <code>load_geology_squad</code> that will load and return a <code>list[QuestionWithContext]</code>, defined below:</p> <pre><code>from pydantic import BaseModel\n\nclass QuestionWithContext(BaseModel):\n    \"\"\"A question with it's answers and context.\"\"\"\n\n    id: str\n    question: str\n    context: str\n    answers: list[str]\n</code></pre>"},{"location":"cookbook/squad_extraction/#initial-basic-extraction","title":"Initial Basic Extraction","text":"<p>To extract an answer to a question, we can use the <code>OpenAIPrompt.extract</code> method to extract the answer. First, let's create a super basic schema and prompt for asking the question and extracting an answer:</p> <pre><code>$ mirascope init --prompts_location squad_prompts; touch prompts/question.py\n</code></pre> <pre><code># squad_prompts/question.py\nfrom mirascope.openai import OpenAICallParams, OpenAIPrompt\nfrom pydantic import BaseModel, Field\n\n\nclass ExtractedAnswer(BaseModel):\n    \"\"\"The answer to a question about a paragraph of text.\"\"\"\n\n    answer: str\n\n\nclass Question(OpenAIPrompt):\n    \"\"\"\n    Paragraph: {paragraph}\n\n    Question: {question}\n    \"\"\"\n\n    paragraph: str\n    question: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-1106\")\n</code></pre> <p>Next we'll define the schema we want to extract from the paragraph and a function to extract the schema from a given question:</p> <pre><code>import os\n\nfrom pydantic import BaseModel\n\nfrom config import Settings\nfrom prompts.question import ExtractedAnswer, QuestionPrompt\n\nsettings = Settings()\nif settings.openai_api_key:\n    os.environ[\"OPENAI_API_KEY\"] = settings.openai_api_key\n\n\ndef extract_answer(question: QuestionWithContext) -&gt; str:\n    \"\"\"Returns the extracted `str` answer to `question`.\"\"\"\n    return (\n        Question(paragraph=question.context, question=question.question)\n        .extract(\n            ExtractedAnswer,\n            retries=2,  # retry up to 2 more times on validation error\n        )\n        .answer\n    )\n</code></pre> <p>Now we just need to load the data and extract our answers:</p> <pre><code>import json\n\nfrom squad import load_geology_squad\n\nextracted_answers = {\n    question.id: extract_answer(question)\n    for question in load_geology_squad()\n}\nwith open(\"geology-squad-answers-v1.json\", \"w\") as f:\n    json.dump(extracted_answers, f)\n</code></pre> <p>Running the included modified version of the SQuAD eval script gives us the following breakdown on metrics, which we can use to see how well our system is working:</p> <pre><code>$ python eval.py geology-squad.json geology-squad-answers-v1.json\n</code></pre> <pre><code>{\n  \"exact\": 79.3103448275862,\n  \"f1\": 89.4777077966733,\n  \"total\": 116,\n}\n</code></pre> <p>Exact match is determined with a raw string match, whereas F1 score is calculated by token to account for partial matches. With our simple script, we were able to get nearly 80% exact match and an F1 score of nearly 90 on our 116 examples. Before we move on, let's include this metadata in the docstring of our prompt file and version it with the CLI:</p> <pre><code>$ mirascope add question\n</code></pre>"},{"location":"cookbook/squad_extraction/#engineering-a-better-prompt-by-analyzing-results-in-oxen","title":"Engineering A Better Prompt By Analyzing Results In Oxen","text":"<p>Now let's make the script even better. To better understand where our LLM is making mistakes, we can upload a transformed version of our data to Oxen and dig around a little.</p> <p>Note</p> <p>The <code>push_to_oxen.py</code> script shows how we upload each prompt version's answer for analysis.</p> <p></p> <p>Two things stick out immediately about the extracted answers:</p> <ol> <li>Often long or a full sentence instead of a short, concise answer.</li> <li>Not an exact match with the context paragraph.</li> </ol> <p>Let's update our schema and our prompt so that the LLM tries to extract more concise answers that better match the context paragraph:</p> <pre><code>from mirascope.openai import OpenAICallParams, OpenAIPrompt\n\nfrom pydantic import BaseModel\n\n\nclass ExtractedAnswer(BaseModel):\n    \"\"\"The answer to a question about a paragraph of text.\"\"\"\n\n    answer: str = Field(\n        ...,\n        description=(\n            \"The extracted answer to the question. This answer is as concise \"\n            \"as possible, most often just a single word. It is also an exact \"\n            \"text match with text in the provided context.\"\n        ),\n    )\n\n\nclass Question(OpenAIPrompt):\n    \"\"\"\n    SYSTEM:\n    You will be asked a question after you read a paragraph. Your task is to\n    answer the question based on the information in the paragraph. Your answer\n    should be an exact text match to text from the paragraph. Your answer should\n    also be one or two words at most is possible.\n\n    USER:\n    Paragraph: {paragraph}\n\n    USER:\n    Question: {question}\n    \"\"\"\n\n    paragraph: str\n    question: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-1106\")\n</code></pre> <pre><code>$ python extract_answers.py v2\n$ python eval.py geology-squad.json geology-squad-answers-v2.json\n</code></pre> <pre><code>{\n  \"exact\": 85.34482758620689,\n  \"f1\": 91.55579573683022,\n  \"total\": 116,\n}\n</code></pre> <pre><code>$ mirascope add question\n</code></pre> <p>So just by updating our schema and prompt we've improved the performance of the extraction script by ~6% for exact match and ~1% for F1. We can see in Oxen that there are far fewer incorrect extracted answers:</p> <p></p> <p>It's also worth noting that we now have a record of both prompt versions and their answers. Going forward, it will be easy to iterate on the prompts and more reliably track which prompts perform the best.</p>"},{"location":"cookbook/wandb_chain/","title":"Logging Prompt Chains with Weights and Biases","text":"<p>This recipe will show you:</p> <ul> <li>how to use Mirascope\u2019s Weights &amp; Biases integrated <code>WandbPrompt</code> for easy logging</li> <li>take advantage of our extraction functionality to streamline the flow of data at each step of the chain.</li> </ul> <p>For the purposes of this example, we take a silly premise where we want to:</p> <ol> <li>Find out who we\u2019re talking about from some given context</li> <li>Figure out how cool they are</li> <li>Determine based off their coolness if they should be let into the party.</li> </ol> <p>First we will log the 3-step chain using just Weight and Biases. Then we will show how we can streamline the original code using Mirascope. If you are only interested in the Mirascope section, feel free to skip to the Mirascope integration below.</p> <p>Before we get started, make sure to have API Keys for both OpenAI and Weights and Biases.</p>"},{"location":"cookbook/wandb_chain/#3-step-chain-with-only-wb","title":"3-step chain with only W&amp;B","text":"<p>This following example is adapted from the example in the Weights and Biases documentation:</p> <pre><code>import datetime\nimport openai\nimport wandb\nfrom wandb.dsk.data_types.trace_tree import Trace\n\n# W&amp;B setup\nwandb.login(key=settings.wandb_api_key)\nwandb.init(project=\"wandb_logged_chain\")\n\n# Initialize root span for the chain of prompts\nroot_span = Trace(\n  name=\"root\",\n  kind=\"chain\",\n  start_time_ms=get_time_in_ms(),\n  metadata={\"user\": \"some_user\"},\n)\n\n# Helper function to track start/end times\ndef get_time_in_ms() -&gt; int:\n    \"\"\"Returns current time in milliseconds.\"\"\"\n    return round(datetime.datetime.now().timestamp() * 1000)\n\nmodel_name = \"gpt-3.5-turbo-1106\"\nclient = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n\n\"\"\"First step of chain\"\"\"\n# OpenAI call\nperson = \"Brian\"\nwho_messages = [\n    {\"role\": \"system\", \"content\": \"Give me the name as one word.\"}\n    {\"role\": \"user\", \"content\": f\"{person} is writing some code examples. \"\n    \"Based on the sentence above, what is the name of the person?\"}\n}\nstart_time = get_time_in_ms()\nwho_completion = client.chat.completions.create(\n    model=model,\n    messages=who_messages\n)\nend_time = get_time_in_ms()\nwho = who_completion.choices[0].message\n\n# Trace setup and linking\nwho_span = Trace(\n    name=\"who_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    status_message=(None,),\n    start_time_ms=start_time,\n    end_time_ms=end_time,\n    inputs={message[\"role\"]: message[\"content\"] for message in self.messages},\n    outputs=who\n)\nroot_span.add_child(who_span)\n\n\"\"\"Second step of chain\"\"\"\n# OpenAI call\ncoolness_messages = [\n    {\"role\": \"system\", \"content\": \"You determine coolness on a scale of\"\n    \"1 to 10. If the person's name is Brian, they get an automatic 10 out\"\n    \" of 10. Otherwise, they get a random whole number between 1 and 9.\"\n    \"respond with just the number for the rating, nothing else.\"},\n    {\"role\": \"user\", \"content\": f\"How cool is {who}?\"}\n]\nstart_time = get_time_in_ms()\ncoolness_completion = client.chat.completions.create(\n    model=model,\n    messages=coolness_messages\n)\nend_time = get_time_in_ms()\ncoolness = coolness_completion.choices[0].message\n\n# Trace setup and linking\ncoolness_span = Trace(\n    name=\"coolness_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    status_message=(None,),\n    start_time_ms=start_time,\n    end_time_ms=end_time,\n    inputs={message[\"role\"]: message[\"content\"] for message in self.messages},\n    outputs=coolness\n)\nwho_span.add_child(coolness_span)\n\n\"\"\"Third step of chain\"\"\"\n# OpenAI call\ninvite_messages = [\n    {\"role\": \"system\", \"content\": \"You're a bouncer and you let people into\"\n    \"the party only if they're at least somewhat cool.\"},\n    {\"role\": \"user\", \"content\": f\"If I were to say how cool this person is\"\n    f\"out of 10, I'd say {coolness}. Should they be let into the party?\"}\n]\nstart_time = get_time_in_ms()\ninvite_completion = client.chat.completions.create(\n    model=model,\n    messages=invite_messages\n)\nend_time = get_time_in_ms()\ninvite = invite_completion.choices[0].message\n\n# Trace setup and linking\ninvite_span = Trace(\n    name=\"invite_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    status_message=(None,),\n    start_time_ms=start_time,\n    end_time_ms=end_time,\n    inputs={message[\"role\"]: message[\"content\"] for message in self.messages},\n    outputs=coolness\n)\ncoolness_span.add_child(invite_span)\n\n# Log the results\nroot_span._span.end_time_ms = get_time_in_ms()\nroot_span.add_inputs_and_outputs(\n  inputs={\n        \"query1\": who_messages[0],\n        \"query2\": coolness_messages[0],\n        \"query3\": invite_messages[0]\n    },\n    outputs={\"result\": invite}\n)\nroot_span.log(name=\"mirascope_trace\")\n</code></pre> <p>We have to manually calculate start and end times for each call to OpenAI. Furthermore, GPT responses contain lots of filler text (\u201dYour name is Brian\u201d instead of just \u201cBrian\u201d) so we have to use system messages to massage the output in a way that fits smoothly into the next prompt, and just hope for the best.  </p>"},{"location":"cookbook/wandb_chain/#integration-with-mirascope","title":"Integration with Mirascope","text":"<p>Mirascope provides a <code>WandbPrompt</code> class which can be inherited by any Mirascope <code>Prompt</code> , giving access to:</p> <ul> <li>methods which interally create and link a W&amp;B <code>Trace</code> when creating chat completions and extractions</li> <li>automatically passing in the entire <code>Prompt.call_params</code> field so you can get every detail of your api logged to W&amp;B.</li> <li>an overall cleaner interface to interact with OpenAI's GPT.</li> </ul> <p>Wait, did someone say extractions? Mirascope provides extraction functionality, which let you structure output and pass information along an LLM chain much more smoothly. Let\u2019s define the prompts as well as their extraction models in 3 files within a directory <code>/wandb_prompts</code>:</p> <pre><code># who_prompt.py\n\nfrom pydantic import BaseModel\nfrom mirascope.wandb import WandbPrompt\n\nclass Person(BaseModel):\n    \"\"\"Person model.\"\"\"\n\n    person: str\n\n\nclass Who(WandbPrompt):\n    \"\"\"Who is {person}?\"\"\"\n\n    person: str\n</code></pre> <pre><code># coolness_prompt.py\n\nfrom pydantic import BaseModel\nfrom mirascope.wandb import WandbPrompt\n\nclass CoolRating(BaseModel):\n    \"\"\"Coolness rating out of 10.\"\"\"\n\n    coolness: int\n\n\nclass Coolness(WandbPrompt):\n    \"\"\"\n    SYSTEM: You determine coolness on a scale of 1 to 10. If the person's name is Brian,\n    they get an automatic 10 out of 10, otherwise, they get a random whole number\n    between 1 and 9.\n\n    USER: How cool is {person}?\n    \"\"\"\n\n    person: str\n</code></pre> <pre><code># party_invite_prompt.py\n\nfrom mirascope.wandb import WandbPrompt\n\nclass PartyInvite(WandbPrompt):\n    \"\"\"\n    SYSTEM:\n    You're a bouncer and you decide if people are allowed into the party. You only let\n    people in if they're at least somewhat cool.\n\n    USER:\n    This person is {coolness} out of 10 cool. Should they be let into the party?\n    \"\"\"\n\n    coolness: int\n</code></pre> <p>With the prompts written out, the main script becomes much simpler. First, we still need to set up W&amp;B and initialize our root_span:</p> <pre><code># wandb_chain.py\n\nimport datetime\nimport os\nfrom typing import Optional\n\nimport wandb\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom wandb.sdk.data_types.trace_tree import Trace\nfrom wandb_prompts.coolness_prompt import Coolness, CoolRating\nfrom wandb_prompts.party_invite_prompt import PartyInvite\nfrom wandb_prompts.who_prompt import Person, Who\n\n\nclass Settings(BaseSettings):\n    \"\"\"Settings for wandb_logged_chain.\"\"\"\n\n    wandb_api_key: Optional[str] = None\n    openai_api_key: Optional[str] = None\n\n    model_config = SettingsConfigDict(env_file=\".env\")\n\n\nsettings = Settings()\nif settings.openai_api_key:\n    os.environ[\"OPENAI_API_KEY\"] = settings.openai_api_key\n\n\nif __name__ == \"__main__\":\n    wandb.login(key=settings.wandb_api_key)\n    wandb.init(project=\"wandb_logged_chain\")\n\n    root_span = Trace(\n        name=\"root\",\n        kind=\"chain\",\n        start_time_ms=round(datetime.datetime.now().timestamp() * 1000),\n        metadata={\"user\": \"mirascope_user\"},\n    )\n</code></pre> <p>Now, all we need to do is create an instance of each prompt and use the integrated functions <code>create_with_trace()</code> or <code>extract_with_trace()</code> - these methods call GPT then create a <code>Trace</code> affiliated with each completion, which can be linked to a parent span if desired through the <code>parent</code> parameter.</p> <p><pre><code># wandb_chain.py\n# continued within main:\n\n    who_prompt = Who(span_type=\"tool\", person=\"Brian\")\n    who_completion, who_span = who_prompt.extract_with_trace(\n        schema=Person, parent=root_span\n    )\n\n    coolness_prompt = Coolness(span_type=\"tool\", person=who_completion.person)\n    coolness_completion, coolness_span = coolness_prompt.extract_with_trace(\n        schema=CoolRating, parent=who_span\n    )\n\n    party_invite_prompt = PartyInvite(\n        span_type=\"llm\", coolness=coolness_completion.coolness\n    )\n    party_completion, party_span = party_invite_prompt.create_with_trace(\n        parent=coolness_span,\n    )\n\n    root_span._span.end_time_ms = party_span._span.end_time_ms\n    root_span.add_inputs_and_outputs(\n        inputs={\n            \"query1\": str(who_prompt),\n            \"query2\": str(coolness_prompt),\n            \"query3\": str(party_invite_prompt),\n        },\n        outputs={\n            \"result1\": str(who_completion),\n            \"result2\": str(coolness_completion),\n            \"result3\": str(party_completion),\n        },\n    )\n    root_span.log(name=\"mirascope_trace\")\n</code></pre> All done - you should now see the logs with a local directory, as well as have access to them online via W&amp;B.</p>"}]}