{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mirascope","text":"<p> Prompt Engineering focused on: </p> Simplicity through idiomatic syntax \u2192 Faster and more reliable releases Semi-opinionated methods \u2192 Reduced complexity that speeds up development Reliability through validation \u2192 More robust applications with fewer bugs High-quality up-to-date documentation \u2192 Trust and reliability <p> </p> <p>Documentation: https://docs.mirascope.io</p> <p>Source Code: https://github.com/Mirascope/mirascope</p> <p>Mirascope is a library purpose-built for Prompt Engineering on top of Pydantic 2.0:</p> <ul> <li>Prompts can live as self-contained classes in their own directory:</li> </ul> <pre><code># prompts/book_recommendation.py\nfrom mirascope import Prompt\n\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    I've recently read the following books: {titles_in_quotes}.\n\n    What should I read next?\n    \"\"\"\n\n    book_titles: list[str]\n\n    @property\n    def titles_in_quotes(self) -&gt; str:\n        \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n        return \", \".join([f'\"{title}\"' for title in self.book_titles])\n</code></pre> <ul> <li>Use the prompt anywhere without worrying about internals such as formatting:</li> </ul> <pre><code># script.py\nfrom prompts import BookRecommendationPrompt\n\nprompt = BookRecommendationPrompt(\n    book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n)\n\nprint(str(prompt))\n#&gt; I've recently read the following books: \"The Name of the Wind\", \"The Lord of the Rings\".\n#  What should I read next?\n\nprint(prompt.messages)\n#&gt; [('user', 'I\\'ve recently read the following books: \"The Name of the Wind\", \"The Lord of the Rings\".\\nWhat should I read next?')]\n</code></pre>"},{"location":"#why-use-mirascope","title":"Why use Mirascope?","text":"<ul> <li>Intuitive: Editor support that you expect (e.g. autocompletion, inline errors)</li> <li>Peace of Mind: Pydantic together with our Prompt CLI eliminate prompt-related bugs.</li> <li>Durable: Seamlessly customize and extend functionality. Never maintain a fork.</li> <li>Integration: Easily integrate with JSON Schema and other tools such as FastAPI</li> <li>Convenience: Tooling that is clean, elegant, and delightful that you don't need to maintain.</li> <li>Open: Dedication to building open-source tools you can use with your choice of LLM.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install Mirascope and start building with LLMs in minutes.</p> <pre><code>pip install mirascope\n</code></pre>"},{"location":"#warning-strong-opinion","title":"\ud83d\udea8 Warning: Strong Opinion \ud83d\udea8","text":"<p>Prompt Engineering is engineering. Beyond basic illustrative examples, prompting quickly becomes complex. Separating prompts from the engineering workflow will only put limitations on what you can build with LLMs. We firmly believe that prompts are far more than \"just f-strings\" and thus require developer tools that are purpose-built for building these more complex prompts as easily as possible.</p>"},{"location":"#examples","title":"Examples","text":"<p>The <code>Prompt</code> class is an extension of <code>BaseModel</code> with some additional built-in convenience tooling for writing and formatting your prompts:</p> <pre><code>from mirascope import Prompt\n\nclass GreetingsPrompt(Prompt):\n    \"\"\"\n    Hello! It's nice to meet you. My name is {name}. How are you today?\n    \"\"\"\n\n    name: str\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\n\nprint(GreetingsPrompt.template())\n#&gt; Hello! It's nice to meet you. My name is {name}. How are you today?\n\nprint(prompt)\n#&gt; Hello! It's nice to meet you. My name is William Bakst. How are you today?\n</code></pre> Example of autocomplete and inline errors: <ul> <li>Autocomplete:</li> <li>Inline Errors:</li> </ul> <p>You can access the docstring prompt template through the <code>GreetingsPrompt.template()</code> class method, which will automatically take care of removing any additional special characters such as newlines. This enables writing longer prompts that still adhere to the style of your codebase:</p> <pre><code>class GreetingsPrompt(Prompt):\n    \"\"\"\n    Salutations! It is a lovely day. Wouldn't you agree? I find that lovely days\n    such as these brighten up my mood quite a bit. I'm rambling...\n\n    My name is {name}. It's great to meet you.\n    \"\"\"\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\nprint(prompt)\n#&gt; Salutations, good being! It is a lovely day. Wouldn't you agree? I find that lovely days such as these brighten up my mood quite a bit. I'm rambling...\n#&gt; My name is William Bakst. It's great to meet you.\n</code></pre> <p>The <code>str</code> method is written such that it only formats properties that are templated. This enables writing more complex properties that rely on one or more provided properties:</p> <pre><code>class GreetingsPrompt(Prompt):\n    \"\"\"\n    Hi! My name is {formatted_name}. {name_specific_remark}\n\n    What's your name? Is your name also {name_specific_question}?\n    \"\"\"\n\n    name: str\n\n    @property\n    def formatted_name(self) -&gt; str:\n        \"\"\"Returns `name` with pizzazz.\"\"\"\n        return f\"\u2b50{self.name}\u2b50\"\n\n    @property\n    def name_specific_question(self) -&gt; str:\n        \"\"\"Returns a question based on `name`.\"\"\"\n        if self.name == self.name[::-1]:\n            return \"a palindrome\"\n        else:\n            return \"not a palindrome\"\n\n    @property\n    def name_specific_remark(self) -&gt; str:\n        \"\"\"Returns a remark based on `name`.\"\"\"\n        return f\"Can you believe my name is {self.name_specific_question}\"\n\nprompt = GreetingsPrompt(name=\"Bob\")\nprint(prompt)\n#&gt; Hi! My name is \u2b50Bob\u2b50. Can you believe my name is a palindrome?\n#&gt; What's your name? Is your name also a palindrome?\n</code></pre> <p>Notice that writing properties in this way ensures prompt-specific logic is tied directly to the prompt. It happens under the hood from the perspective of the person using <code>GreetingsPrompt</code> class. Constructing the prompt only requires <code>name</code>.</p> <p>For writing promps with multiple messages with different roles, you can use the <code>messages</code> decorator to extend the functionality of the <code>messages</code> property:</p> <pre><code>from mirascope import Prompt, messages\n\n@messages\nclass GreetingsPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You can only speak in haikus.\n\n    USER:\n    Hello! It's nice to meet you. My name is {name}. How are you today?\n    \"\"\"\n\n    name: str\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\n\nprint(GreetingsPrompt.template())\n#&gt; SYSTEM: You can only speak in haikus.\n#&gt; USER: Hello! It's nice to meet you. My name is {name}. How are you today?\n\nprint(prompt)\n#&gt; SYSTEM: You can only speak in haikus.\n#&gt; USER: Hello! It's nice to meet you. My name is William Bakst. How are you today?\n\nprint(prompt.messages)\n#&gt; [('system', 'You can only speak in haikus.'), ('user', \"Hello! It's nice to meet you. My name is William Bakst. How are you today?\")]\n</code></pre> <p>The base <code>Prompt</code> class without the decorator will still have the <code>messages</code> attribute, but it will return a single user message in the list.</p> Remember: this is python <p>There's nothing stopping you from doing things however you'd like. For example, reclaim the docstring:</p> <pre><code>from mirascope import Prompt\n\nTEMPLATE = \"\"\"\nThis is now my prompt template for {topic}\n\"\"\"\n\nclass NormalDocstringPrompt(Prompt):\n    \"\"\"This is now just a normal docstring.\"\"\"\n\n    topic: str\n\n    def template(self) -&gt; str:\n        \"\"\"Returns this prompt's template.\"\"\"\n        return TEMPLATE\n\nprompt = NormalDocstringPrompt(topic=\"prompts\")\nprint(prompt)\n#&gt; This is now my prompt template for prompt\n</code></pre> <p>Since the `Prompt`'s `str` method uses template, the above will work as expected.</p>"},{"location":"#additional-examples","title":"Additional Examples","text":"<p>If you're using our LLM convenience wrappers, you'll need to get an API key to use the model of your choice. OpenAI Account Setup will walk you through how to get your OpenAI API Key. You can follow similar steps for the provider of your choice (e.g. Anyscale or Together), or you can use a raw client for non-OpenAI models, like Mistral.</p> <p>Because the <code>Prompt</code> class is built on top of <code>BaseModel</code>, prompts easily integrate with tools like FastAPI:</p> FastAPI Example <pre><code>import os\n\nfrom fastapi import FastAPI\nfrom mirascope import OpenAIChat\n\nfrom prompts import GreetingsPrompt\n\napp = FastAPI()\n\n\n@app.post(\"/greetings\")\ndef root(prompt: GreetingsPrompt) -&gt; str:\n    \"\"\"Returns an AI generated greeting.\"\"\"\n    model = OpenAIChat(api_key=os.environ[\"OPENAI_API_KEY\"])\n    return str(model.create(prompt))\n</code></pre> <p>You can also use the <code>Prompt</code> class with whichever LLM you want to use:</p> Mistral Example <pre><code>import os\n\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\nfrom prompts import GreetingsPrompt\n\nclient = MistralClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\nmessages = [\n    ChatMessage(role=role, content=content)\n    for role, content in prompt.messages\n]\n\n# No streaming\nchat_response = client.chat(\n    model=\"mistral-tiny\",\n    messages=messages,\n)\n</code></pre> OpenAI Example <pre><code>import os\n\nfrom openai import OpenAI\n\nfrom prompts import GreetingsPrompt\n\nclient = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\nmessages = [\n    {\"role\": role, \"content\": content}\n    for role, content in prompt.messages\n]\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=messages,\n)\n</code></pre>"},{"location":"#dive-deeper","title":"Dive Deeper","text":"<ul> <li>Check out all of the possibilities of what you can do with Pydantic Prompts.</li> <li>Take a look at our Mirascope CLI for a semi-opinionated prompt mangement system.</li> <li>For convenience, we provide some wrappers around the OpenAI Python SDK for common tasks such as creation, streaming, tools, and extraction. We've found that the concepts covered in LLM Convenience Wrappers make building LLM-powered apps delightful.</li> <li>The API Reference contains full details on all classes, methods, functions, etc.</li> <li>You can take a look at code examples in the repo that demonstrate how to use the library effectively.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Mirascope welcomes contributions from the community! See the contribution guide for more information on the development workflow. For bugs and feature requests, visit our GitHub Issues and check out our templates.</p>"},{"location":"#how-to-help","title":"How To Help","text":"<p>Any and all help is greatly appreciated! Check out our page on how you can help.</p>"},{"location":"#roadmap-whats-on-our-mind","title":"Roadmap (What's on our mind)","text":"<ul> <li> Better DX for Mirascope CLI (e.g. autocomplete)</li> <li> Functions as OpenAI tools</li> <li> Better chat history</li> <li> Testing for prompts</li> <li> Logging prompts and their responses</li> <li> Evaluating prompt quality</li> <li> RAG</li> <li> Agents</li> </ul>"},{"location":"#versioning","title":"Versioning","text":"<p>Mirascope uses Semantic Versioning.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use poetry as our package and dependency manager.</p> <p>To create a virtual environment for development, run the following in your shell:</p> <pre><code>pip install poetry\npoetry shell\npoetry install --with dev\n</code></pre> <p>Simply use <code>exit</code> to deactivate the environment. The next time you call <code>poetry shell</code> the environment will already be setup and ready to go.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout dev\ngit pull upstream dev\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run mypy .\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR. Make sure to fill in a detailed title and description. Submit your PR for review.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p> <p>Furthermore, we have a full coverage requirement, so all incoming code must have 100% coverage. This policy ensures that every line of code is run in our tests. However, while achieving full coverage is essential, it is not sufficient on its own. Coverage metrics ensure code execution but do not guarantee correctness under all conditions. Make sure to stress test beyond coverage to reduce bugs.</p> <p>We use a Codecov dashboard to monitor and track our coverage.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>mypy</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>mypy</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>mypy</code> errors.</p>"},{"location":"HELP/","title":"How to help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li> <p>Follow us on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow William Bakst on Twitter/X</p> <ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with William Bakst on LinkedIn</p> <ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how Mirascope has helped you and how you are using it.</p> </li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues / Discussions.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"api/enums/","title":"enums","text":"<p>Enum Classes for mirascope.</p>"},{"location":"api/enums/#mirascope.enums.MirascopeCommand","title":"<code>MirascopeCommand</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>CLI commands to be executed.</p> Source code in <code>mirascope/enums.py</code> <pre><code>class MirascopeCommand(_Enum):\n    \"\"\"CLI commands to be executed.\"\"\"\n\n    ADD = \"add\"\n    USE = \"use\"\n    STATUS = \"status\"\n    INIT = \"init\"\n</code></pre>"},{"location":"api/prompts/","title":"prompts","text":"<p>A class for better prompting.</p>"},{"location":"api/prompts/#mirascope.prompts.Prompt","title":"<code>Prompt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for prompts.</p> <p>Example:</p> <pre><code>from mirascope import Prompt, messages\n\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    I've recently read the following books: {titles_in_quotes}.\n    What should I read next?\n    \"\"\"\n\n    book_titles: list[str]\n\n    @property\n    def titles_in_quotes(self) -&gt; str:\n        \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n        return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\nprompt = BookRecommendationPrompt(\n    book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n)\n\nprint(BookRecommendationPrompt.template())\n#&gt; SYSTEM: You are the world's greatest librarian.\n#&gt; USER: I've recently read the following books: {titles_in_quotes}. What should I\n#  read next?\n\nprint(str(prompt))\n#&gt; SYSTEM: You are the world's greatest librarian.\n#&gt; USER: I've recently read the following books: \"The Name of the Wind\", \"The Lord\n#  of the Rings\". What should I read next?\n\nprompt.messages\n#&gt; [('system', \"You are the world's greatest librarian.\"), ('user', 'I've recently\n#   read the following books: \"The Name of the Wind\", \"The Lord of the Rings\". What\n#   should I read next?')]\n</code></pre> Source code in <code>mirascope/prompts.py</code> <pre><code>class Prompt(BaseModel):\n    '''A Pydantic model for prompts.\n\n    Example:\n\n    ```python\n    from mirascope import Prompt, messages\n\n\n    @messages\n    class BookRecommendationPrompt(Prompt):\n        \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        I've recently read the following books: {titles_in_quotes}.\n        What should I read next?\n        \"\"\"\n\n        book_titles: list[str]\n\n        @property\n        def titles_in_quotes(self) -&gt; str:\n            \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n            return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\n    prompt = BookRecommendationPrompt(\n        book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n    )\n\n    print(BookRecommendationPrompt.template())\n    #&gt; SYSTEM: You are the world's greatest librarian.\n    #&gt; USER: I've recently read the following books: {titles_in_quotes}. What should I\n    #  read next?\n\n    print(str(prompt))\n    #&gt; SYSTEM: You are the world's greatest librarian.\n    #&gt; USER: I've recently read the following books: \"The Name of the Wind\", \"The Lord\n    #  of the Rings\". What should I read next?\n\n    prompt.messages\n    #&gt; [('system', \"You are the world's greatest librarian.\"), ('user', 'I\\'ve recently\n    #   read the following books: \"The Name of the Wind\", \"The Lord of the Rings\". What\n    #   should I read next?')]\n    ```\n    '''\n\n    @classmethod\n    def template(cls) -&gt; str:\n        \"\"\"Custom parsing functionality for docstring prompt.\n\n        This function is the first step in formatting the prompt template docstring.\n        For the default `Prompt`, this function dedents the docstring and replaces all\n        repeated sequences of newlines with one fewer newline character. This enables\n        writing blocks of text instead of really long single lines. To include any\n        number of newline characters, simply include one extra.\n\n        Raises:\n            ValueError: If the class docstring is empty.\n        \"\"\"\n        if cls.__doc__ is None:\n            raise ValueError(\"`Prompt` must have a prompt template docstring.\")\n\n        return re.sub(\n            \"(\\n+)\",\n            lambda x: x.group(0)[:-1] if len(x.group(0)) &gt; 1 else \" \",\n            dedent(cls.__doc__).strip(\"\\n\"),\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the docstring prompt template formatted with template variables.\"\"\"\n        template = self.template()\n        template_vars = [\n            var for _, var, _, _ in Formatter().parse(template) if var is not None\n        ]\n        return template.format(**{var: getattr(self, var) for var in template_vars})\n\n    @property\n    def messages(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Returns the docstring as a list of messages.\"\"\"\n        return [(\"user\", str(self))]\n\n    def save(self, filepath: str):\n        \"\"\"Saves the prompt to the given filepath.\"\"\"\n        with open(filepath, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, filepath: str) -&gt; Prompt:\n        \"\"\"Loads the prompt from the given filepath.\"\"\"\n        with open(filepath, \"rb\") as f:\n            return pickle.load(f)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.messages","title":"<code>messages: list[tuple[str, str]]</code>  <code>property</code>","text":"<p>Returns the docstring as a list of messages.</p>"},{"location":"api/prompts/#mirascope.prompts.Prompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the docstring prompt template formatted with template variables.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the docstring prompt template formatted with template variables.\"\"\"\n    template = self.template()\n    template_vars = [\n        var for _, var, _, _ in Formatter().parse(template) if var is not None\n    ]\n    return template.format(**{var: getattr(self, var) for var in template_vars})\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Loads the prompt from the given filepath.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str) -&gt; Prompt:\n    \"\"\"Loads the prompt from the given filepath.\"\"\"\n    with open(filepath, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.save","title":"<code>save(filepath)</code>","text":"<p>Saves the prompt to the given filepath.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def save(self, filepath: str):\n    \"\"\"Saves the prompt to the given filepath.\"\"\"\n    with open(filepath, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.template","title":"<code>template()</code>  <code>classmethod</code>","text":"<p>Custom parsing functionality for docstring prompt.</p> <p>This function is the first step in formatting the prompt template docstring. For the default <code>Prompt</code>, this function dedents the docstring and replaces all repeated sequences of newlines with one fewer newline character. This enables writing blocks of text instead of really long single lines. To include any number of newline characters, simply include one extra.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class docstring is empty.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>@classmethod\ndef template(cls) -&gt; str:\n    \"\"\"Custom parsing functionality for docstring prompt.\n\n    This function is the first step in formatting the prompt template docstring.\n    For the default `Prompt`, this function dedents the docstring and replaces all\n    repeated sequences of newlines with one fewer newline character. This enables\n    writing blocks of text instead of really long single lines. To include any\n    number of newline characters, simply include one extra.\n\n    Raises:\n        ValueError: If the class docstring is empty.\n    \"\"\"\n    if cls.__doc__ is None:\n        raise ValueError(\"`Prompt` must have a prompt template docstring.\")\n\n    return re.sub(\n        \"(\\n+)\",\n        lambda x: x.group(0)[:-1] if len(x.group(0)) &gt; 1 else \" \",\n        dedent(cls.__doc__).strip(\"\\n\"),\n    )\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.messages","title":"<code>messages(cls)</code>","text":"<p>A decorator for updating the <code>messages</code> class attribute of a <code>Prompt</code>.</p> <p>Adding this decorator to a <code>Prompt</code> updates the <code>messages</code> class attribute to parse the docstring as a list of messages. Each message is a tuple containing the role and the content. The docstring should have the following format:</p> <pre><code>&lt;role&gt;:\n&lt;content&gt;\n</code></pre> <p>For example, you might want to first include a system prompt followed by a user prompt, which you can structure as follows:</p> <pre><code>SYSTEM:\nThis would be the system message content.\n\nUSER:\nThis would be the user message content.\n</code></pre> <p>This decorator currently supports the SYSTEM, USER, and ASSISTANT roles.</p> <p>Returns:</p> Type Description <code>Type[T]</code> <p>The decorated class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the docstring is empty.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def messages(cls: Type[T]) -&gt; Type[T]:\n    \"\"\"A decorator for updating the `messages` class attribute of a `Prompt`.\n\n    Adding this decorator to a `Prompt` updates the `messages` class attribute\n    to parse the docstring as a list of messages. Each message is a tuple containing\n    the role and the content. The docstring should have the following format:\n\n        &lt;role&gt;:\n        &lt;content&gt;\n\n    For example, you might want to first include a system prompt followed by a user\n    prompt, which you can structure as follows:\n\n        SYSTEM:\n        This would be the system message content.\n\n        USER:\n        This would be the user message content.\n\n    This decorator currently supports the SYSTEM, USER, and ASSISTANT roles.\n\n    Returns:\n        The decorated class.\n\n    Raises:\n        ValueError: If the docstring is empty.\n    \"\"\"\n\n    def messages_fn(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Returns the docstring as a list of messages.\"\"\"\n        return [\n            (match.group(1).lower(), match.group(2))\n            for match in re.finditer(\n                r\"(SYSTEM|USER|ASSISTANT|TOOL): \"\n                r\"((.|\\n)+?)(?=\\n(SYSTEM|USER|ASSISTANT|TOOL):|\\Z)\",\n                str(self),\n            )\n        ]\n\n    setattr(cls, \"messages\", property(messages_fn))\n    return cls\n</code></pre>"},{"location":"api/chat/","title":"chat","text":"<p>A module for interacting with Chat APIs.</p>"},{"location":"api/chat/tools/","title":"chat.tools","text":"<p>Classes for using tools with Chat APIs.</p>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A base class for more easily using tools with the OpenAI Chat client.</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>class OpenAITool(BaseModel):\n    \"\"\"A base class for more easily using tools with the OpenAI Chat client.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def fn(self) -&gt; Optional[Callable]:\n        \"\"\"Returns the function that the tool describes.\"\"\"\n        return None\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n        Returns:\n            The constructed `ChatCompletionToolParam` schema.\n\n        Raises:\n            ValueError: if the class doesn't have\n        \"\"\"\n        model_schema = cls.model_json_schema()\n        if \"description\" not in model_schema:\n            raise ValueError(\"Tool must have a docstring description.\")\n\n        fn = {\n            \"name\": model_schema[\"title\"],\n            \"description\": model_schema[\"description\"],\n        }\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = {\n                \"type\": \"object\",\n                \"properties\": {\n                    prop: {key: value for key, value in prop_schema.items()}\n                    for prop, prop_schema in model_schema[\"properties\"].items()\n                    if prop != \"tool_call\"\n                },\n                \"required\": [\n                    prop for prop in model_schema[\"required\"] if prop != \"tool_call\"\n                ]\n                if \"required\" in model_schema\n                else [],\n                \"$defs\": {\n                    key: value\n                    for key, value in model_schema[\"$defs\"].items()\n                    if key != \"ChatCompletionMessageToolCall\" and key != \"Function\"\n                }\n                if \"$defs\" in model_schema\n                else {},\n            }\n\n        return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Args:\n            tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        try:\n            model_json = json.loads(tool_call.function.arguments)\n        except JSONDecodeError as e:\n            raise ValueError() from e\n\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool.fn","title":"<code>fn: Optional[Callable]</code>  <code>property</code>","text":"<p>Returns the function that the tool describes.</p>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The <code>ChatCompletionMessageToolCall</code> to extract the tool from.</p> required <p>Returns:</p> Type Description <code>OpenAITool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Args:\n        tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    try:\n        model_json = json.loads(tool_call.function.arguments)\n    except JSONDecodeError as e:\n        raise ValueError() from e\n\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the OpenAI Chat client.</p> <p>Returns:</p> Type Description <code>ChatCompletionToolParam</code> <p>The constructed <code>ChatCompletionToolParam</code> schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the class doesn't have</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n    Returns:\n        The constructed `ChatCompletionToolParam` schema.\n\n    Raises:\n        ValueError: if the class doesn't have\n    \"\"\"\n    model_schema = cls.model_json_schema()\n    if \"description\" not in model_schema:\n        raise ValueError(\"Tool must have a docstring description.\")\n\n    fn = {\n        \"name\": model_schema[\"title\"],\n        \"description\": model_schema[\"description\"],\n    }\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = {\n            \"type\": \"object\",\n            \"properties\": {\n                prop: {key: value for key, value in prop_schema.items()}\n                for prop, prop_schema in model_schema[\"properties\"].items()\n                if prop != \"tool_call\"\n            },\n            \"required\": [\n                prop for prop in model_schema[\"required\"] if prop != \"tool_call\"\n            ]\n            if \"required\" in model_schema\n            else [],\n            \"$defs\": {\n                key: value\n                for key, value in model_schema[\"$defs\"].items()\n                if key != \"ChatCompletionMessageToolCall\" and key != \"Function\"\n            }\n            if \"$defs\" in model_schema\n            else {},\n        }\n\n    return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n</code></pre>"},{"location":"api/chat/tools/#mirascope.chat.tools.openai_tool_fn","title":"<code>openai_tool_fn(fn)</code>","text":"<p>A decorator for adding a function to a tool class.</p> <p>Adding this decorator will add an <code>fn</code> property to the tool class that returns the function that the tool describes. This is convenient for calling the function given an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to add to the tool class.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated tool class.</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>def openai_tool_fn(fn: Callable) -&gt; Callable:\n    \"\"\"A decorator for adding a function to a tool class.\n\n    Adding this decorator will add an `fn` property to the tool class that returns the\n    function that the tool describes. This is convenient for calling the function given\n    an instance of the tool.\n\n    Args:\n        fn: The function to add to the tool class.\n\n    Returns:\n        The decorated tool class.\n    \"\"\"\n\n    def decorator(cls: Type[T]) -&gt; Type[T]:\n        \"\"\"A decorator for adding a function to a tool class.\"\"\"\n        setattr(cls, \"fn\", property(lambda self: fn))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/chat/types/","title":"chat.types","text":"<p>Classes for responses when interacting with a Chat API.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion","title":"<code>OpenAIChatCompletion</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around chat completions.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>class OpenAIChatCompletion(BaseModel):\n    \"\"\"Convenience wrapper around chat completions.\"\"\"\n\n    completion: ChatCompletion\n    tool_types: Optional[list[Type[OpenAITool]]] = None\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.completion.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.completion.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.completion.choices[0].message\n\n    @property\n    def content(self) -&gt; Optional[str]:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.completion.choices[0].message.content\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChatCompletionMessageToolCall]]:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @property\n    def tools(self) -&gt; Optional[list[OpenAITool]]:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[OpenAITool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls or len(self.tool_calls) == 0:\n            return None\n\n        tool_call = self.tool_calls[0]\n        for tool_type in self.tool_types:\n            if self.tool_calls[0].function.name == tool_type.__name__:\n                return tool_type.from_tool_call(tool_call)\n\n        return None\n\n    def __str__(self):\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.content","title":"<code>content: Optional[str]</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.tool","title":"<code>tool: Optional[OpenAITool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.tool_calls","title":"<code>tool_calls: Optional[list[ChatCompletionMessageToolCall]]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.tools","title":"<code>tools: Optional[list[OpenAITool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.__str__","title":"<code>__str__()</code>","text":"<p>Returns the contained string content for the 0th choice.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>def __str__(self):\n    \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n    return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk","title":"<code>OpenAIChatCompletionChunk</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>class OpenAIChatCompletionChunk(BaseModel):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\"\"\"\n\n    chunk: ChatCompletionChunk\n    tool_types: Optional[list[Type[OpenAITool]]] = None\n\n    @property\n    def choices(self) -&gt; list[ChunkChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChunkChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        return self.choices[0].delta\n\n    @property\n    def content(self) -&gt; Optional[str]:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return self.delta.content\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChoiceDeltaToolCall]]:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first and last `list[ChoiceDeltaToolCall]` will be None indicating start\n        and end of stream respectively. The next `list[ChoiceDeltaToolCall]` will\n        contain the name of the tool and index and subsequent\n        `list[ChoiceDeltaToolCall]`s will contain the arguments which will be strings\n        that need to be concatenated with future `list[ChoiceDeltaToolCall]`s to form a\n        complete JSON tool calls.\n        \"\"\"\n        return self.delta.tool_calls\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.choice","title":"<code>choice: ChunkChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.choices","title":"<code>choices: list[ChunkChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.content","title":"<code>content: Optional[str]</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.delta","title":"<code>delta: ChoiceDelta</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.tool_calls","title":"<code>tool_calls: Optional[list[ChoiceDeltaToolCall]]</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first and last <code>list[ChoiceDeltaToolCall]</code> will be None indicating start and end of stream respectively. The next <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the chunk content for the 0th choice.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n    return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/utils/","title":"chat.utils","text":"<p>Utility functions for mirascope chat.</p>"},{"location":"api/chat/utils/#mirascope.chat.utils.convert_base_model_to_openai_tool","title":"<code>convert_base_model_to_openai_tool(schema)</code>","text":"<p>Converts a <code>BaseModel</code> schema to an <code>OpenAITool</code> instance.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def convert_base_model_to_openai_tool(schema: Type[BaseModel]) -&gt; Type[OpenAITool]:\n    \"\"\"Converts a `BaseModel` schema to an `OpenAITool` instance.\"\"\"\n    internal_doc = (\n        f\"An `{schema.__name__}` instance with all correctly typed parameters \"\n        \"extracted from the completion. Must include required parameters and may \"\n        \"exclude optional parameters unless present in the text.\"\n    )\n    field_definitions = {\n        field_name: (field_info.annotation, field_info)\n        for field_name, field_info in schema.model_fields.items()\n    }\n    return create_model(\n        f\"{schema.__name__}Tool\",\n        __base__=OpenAITool,\n        __doc__=schema.__doc__ if schema.__doc__ else internal_doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.convert_function_to_openai_tool","title":"<code>convert_function_to_openai_tool(fn)</code>","text":"<p>Constructs and <code>OpenAITool</code> type from the given function.</p> <p>If parameters are not defined in the Args section, then the description will simply be the name of the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to convert.</p> required <p>Returns:</p> Type Description <code>Type[OpenAITool]</code> <p>The constructed <code>OpenAITool</code> type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the given function doesn't have a docstring.</p> <code>ValueError</code> <p>if the given function's parameters don't have type annotations.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but the name doesn't match the docstring's parameter name.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but doesn't have a dosctring description.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def convert_function_to_openai_tool(fn: Callable) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs and `OpenAITool` type from the given function.\n\n    If parameters are not defined in the Args section, then the description will simply\n    be the name of the parameter.\n\n    Args:\n        fn: The function to convert.\n\n    Returns:\n        The constructed `OpenAITool` type.\n\n    Raises:\n        ValueError: if the given function doesn't have a docstring.\n        ValueError: if the given function's parameters don't have type annotations.\n        ValueError: if a given function's parameter is in the docstring args section but\n            the name doesn't match the docstring's parameter name.\n        ValueError: if a given function's parameter is in the docstring args section but\n            doesn't have a dosctring description.\n    \"\"\"\n    if not fn.__doc__:\n        raise ValueError(\"Function must have a docstring.\")\n\n    docstring = parse(fn.__doc__)\n\n    doc = \"\"\n    if docstring.short_description:\n        doc = docstring.short_description\n    if docstring.long_description:\n        doc += \"\\n\\n\" + docstring.long_description\n\n    field_definitions = {}\n    hints = get_type_hints(fn)\n    for i, parameter in enumerate(signature(fn).parameters.values()):\n        if parameter.name == \"self\" or parameter.name == \"cls\":\n            continue\n        if parameter.annotation == Parameter.empty:\n            raise ValueError(\"All parameters must have a type annotation.\")\n\n        docstring_description = None\n        if i &lt; len(docstring.params):\n            docstring_param = docstring.params[i]\n            if docstring_param.arg_name != parameter.name:\n                raise ValueError(\n                    f\"Function parameter name {parameter.name} does not match docstring \"\n                    f\"parameter name {docstring_param.arg_name}. Make sure that the \"\n                    \"parameter names match exactly.\"\n                )\n            if not docstring_param.description:\n                raise ValueError(\"All parameters must have a description.\")\n            docstring_description = docstring_param.description\n\n        field_info = FieldInfo(annotation=hints[parameter.name])\n        if parameter.default != Parameter.empty:\n            field_info.default = parameter.default\n        if docstring_description:  # we check falsy here because this comes from docstr\n            field_info.description = docstring_description\n\n        param_name = parameter.name\n        if param_name.startswith(\"model_\"):  # model_ is a BaseModel reserved namespace\n            param_name = \"aliased_\" + param_name\n            field_info.alias = parameter.name\n            field_info.validation_alias = parameter.name\n            field_info.serialization_alias = parameter.name\n\n        field_definitions[param_name] = (\n            hints[parameter.name],\n            field_info,\n        )\n\n    return create_model(\n        \"\".join(word.title() for word in fn.__name__.split(\"_\")),\n        __base__=openai_tool_fn(fn)(OpenAITool),\n        __doc__=doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.convert_tools_list_to_openai_tools","title":"<code>convert_tools_list_to_openai_tools(tools)</code>","text":"<p>Converts a list of <code>Callable</code> or <code>OpenAITool</code> instances to an <code>OpenAITool</code> list.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def convert_tools_list_to_openai_tools(\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]],\n) -&gt; Optional[list[Type[OpenAITool]]]:\n    \"\"\"Converts a list of `Callable` or `OpenAITool` instances to an `OpenAITool` list.\"\"\"\n    if not tools:\n        return None\n    return [\n        tool if isclass(tool) else convert_function_to_openai_tool(tool)\n        for tool in tools\n    ]\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.get_openai_messages_from_prompt","title":"<code>get_openai_messages_from_prompt(prompt)</code>","text":"<p>Returns a list of messages parsed from the prompt.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def get_openai_messages_from_prompt(\n    prompt: Union[Prompt, str],\n) -&gt; list[ChatCompletionMessageParam]:\n    \"\"\"Returns a list of messages parsed from the prompt.\"\"\"\n    if isinstance(prompt, Prompt):\n        return [\n            cast(ChatCompletionMessageParam, {\"role\": role, \"content\": content})\n            for role, content in prompt.messages\n        ]\n    else:\n        return [cast(ChatCompletionMessageParam, {\"role\": \"user\", \"content\": prompt})]\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.patch_openai_kwargs","title":"<code>patch_openai_kwargs(kwargs, prompt, tools)</code>","text":"<p>Sets up the kwargs for an OpenAI API call.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def patch_openai_kwargs(\n    kwargs: dict[str, Any],\n    prompt: Optional[Union[Prompt, str]],\n    tools: Optional[list[Type[OpenAITool]]],\n):\n    \"\"\"Sets up the kwargs for an OpenAI API call.\"\"\"\n    if prompt is None:\n        if \"messages\" not in kwargs:\n            raise ValueError(\"Either `prompt` or `messages` must be provided.\")\n    else:\n        kwargs[\"messages\"] = get_openai_messages_from_prompt(prompt)\n\n    if tools:\n        kwargs[\"tools\"] = [tool.tool_schema() for tool in tools]\n        if \"tool_choice\" not in kwargs:\n            kwargs[\"tool_choice\"] = \"auto\"\n</code></pre>"},{"location":"api/chat/models/","title":"chat.models","text":"<p>Classes for interactings with LLMs through Chat APIs.</p>"},{"location":"api/chat/models/openai_chat/","title":"chat.models.OpenAIChat","text":"<p>A convenience wrapper for the OpenAI Chat client.</p> Source code in <code>mirascope/chat/models/openai_chat.py</code> <pre><code>class OpenAIChat:\n    \"\"\"A convenience wrapper for the OpenAI Chat client.\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"Initializes an instance of `OpenAIChat.\"\"\"\n        self.client = OpenAI(api_key=api_key, base_url=base_url, **kwargs)\n        self.model = model\n\n    def create(\n        self,\n        prompt: Optional[Union[Prompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs,\n    ) -&gt; OpenAIChatCompletion:\n        \"\"\"Makes a call to the model using `prompt`.\n\n        Args:\n            prompt: The prompt to use for the call. This can either be a `Prompt`\n                instance, a raw string, or `None`. If `prompt` is `None`, then the call\n                will attempt to use the `messages` keyword argument.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            A `OpenAIChatCompletion` instance.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n        return OpenAIChatCompletion(\n            completion=self.client.chat.completions.create(\n                model=self.model,\n                stream=False,\n                **kwargs,\n            ),\n            tool_types=openai_tools if tools else None,\n        )\n\n    def stream(\n        self,\n        prompt: Optional[Union[Prompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs,\n    ) -&gt; Generator[OpenAIChatCompletionChunk, None, None]:\n        \"\"\"Streams the response for a call to the model using `prompt`.\n\n        Args:\n            prompt: The `Prompt` to use for the call.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Yields:\n            A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n        completion_stream = self.client.chat.completions.create(\n            model=self.model,\n            stream=True,\n            **kwargs,\n        )\n\n        for chunk in completion_stream:\n            yield OpenAIChatCompletionChunk(\n                chunk=chunk,\n                tool_types=openai_tools if tools else None,\n            )\n\n    def extract(\n        self,\n        schema: Type[BaseModelT],\n        prompt: Optional[Union[Prompt, str]] = None,\n        retries: int = 0,\n        **kwargs,\n    ) -&gt; BaseModelT:\n        \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            prompt: The prompt from which the schema will be extracted.\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        tool = convert_base_model_to_openai_tool(schema)\n        completion = self.create(\n            prompt,\n            tools=[tool],\n            tool_choice={\n                \"type\": \"function\",\n                \"function\": {\"name\": tool.__name__},\n            },\n            **kwargs,\n        )\n\n        try:\n            return schema(**completion.tool.model_dump())  # type: ignore\n        except (AttributeError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: update this to include failure history once prompts can handle\n                # chat history properly.\n                return self.extract(schema, prompt, retries - 1)\n            raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/chat/models/openai_chat/#mirascope.chat.models.OpenAIChat.__init__","title":"<code>__init__(model='gpt-3.5-turbo', api_key=None, base_url=None, **kwargs)</code>","text":"<p>Initializes an instance of `OpenAIChat.</p> Source code in <code>mirascope/chat/models/openai_chat.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gpt-3.5-turbo\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes an instance of `OpenAIChat.\"\"\"\n    self.client = OpenAI(api_key=api_key, base_url=base_url, **kwargs)\n    self.model = model\n</code></pre>"},{"location":"api/chat/models/openai_chat/#mirascope.chat.models.OpenAIChat.create","title":"<code>create(prompt=None, tools=None, **kwargs)</code>","text":"<p>Makes a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[Prompt, str]]</code> <p>The prompt to use for the call. This can either be a <code>Prompt</code> instance, a raw string, or <code>None</code>. If <code>prompt</code> is <code>None</code>, then the call will attempt to use the <code>messages</code> keyword argument.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIChatCompletion</code> <p>A <code>OpenAIChatCompletion</code> instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/chat/models/openai_chat.py</code> <pre><code>def create(\n    self,\n    prompt: Optional[Union[Prompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs,\n) -&gt; OpenAIChatCompletion:\n    \"\"\"Makes a call to the model using `prompt`.\n\n    Args:\n        prompt: The prompt to use for the call. This can either be a `Prompt`\n            instance, a raw string, or `None`. If `prompt` is `None`, then the call\n            will attempt to use the `messages` keyword argument.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        A `OpenAIChatCompletion` instance.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n    return OpenAIChatCompletion(\n        completion=self.client.chat.completions.create(\n            model=self.model,\n            stream=False,\n            **kwargs,\n        ),\n        tool_types=openai_tools if tools else None,\n    )\n</code></pre>"},{"location":"api/chat/models/openai_chat/#mirascope.chat.models.OpenAIChat.extract","title":"<code>extract(schema, prompt=None, retries=0, **kwargs)</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModelT]</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>prompt</code> <code>Optional[Union[Prompt, str]]</code> <p>The prompt from which the schema will be extracted.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModelT</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/chat/models/openai_chat.py</code> <pre><code>def extract(\n    self,\n    schema: Type[BaseModelT],\n    prompt: Optional[Union[Prompt, str]] = None,\n    retries: int = 0,\n    **kwargs,\n) -&gt; BaseModelT:\n    \"\"\"Extracts the given schema from the response of a chat `create` call.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        prompt: The prompt from which the schema will be extracted.\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    tool = convert_base_model_to_openai_tool(schema)\n    completion = self.create(\n        prompt,\n        tools=[tool],\n        tool_choice={\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__},\n        },\n        **kwargs,\n    )\n\n    try:\n        return schema(**completion.tool.model_dump())  # type: ignore\n    except (AttributeError, ValidationError) as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: update this to include failure history once prompts can handle\n            # chat history properly.\n            return self.extract(schema, prompt, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/chat/models/openai_chat/#mirascope.chat.models.OpenAIChat.stream","title":"<code>stream(prompt=None, tools=None, **kwargs)</code>","text":"<p>Streams the response for a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[Prompt, str]]</code> <p>The <code>Prompt</code> to use for the call.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Yields:</p> Type Description <code>OpenAIChatCompletionChunk</code> <p>A <code>OpenAIChatCompletionChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/chat/models/openai_chat.py</code> <pre><code>def stream(\n    self,\n    prompt: Optional[Union[Prompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs,\n) -&gt; Generator[OpenAIChatCompletionChunk, None, None]:\n    \"\"\"Streams the response for a call to the model using `prompt`.\n\n    Args:\n        prompt: The `Prompt` to use for the call.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Yields:\n        A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n    completion_stream = self.client.chat.completions.create(\n        model=self.model,\n        stream=True,\n        **kwargs,\n    )\n\n    for chunk in completion_stream:\n        yield OpenAIChatCompletionChunk(\n            chunk=chunk,\n            tool_types=openai_tools if tools else None,\n        )\n</code></pre>"},{"location":"api/chat/models/openai_chat_async/","title":"chat.models.AsyncOpenAIChat","text":"<p>A convenience wrapper for the AsyncOpenAI Chat client.</p> Source code in <code>mirascope/chat/models/async_openai_chat.py</code> <pre><code>class AsyncOpenAIChat:\n    \"\"\"A convenience wrapper for the AsyncOpenAI Chat client.\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"Initializes an instance of `AsyncOpenAIChat.\"\"\"\n        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url, **kwargs)\n        self.model = model\n\n    async def create(\n        self,\n        prompt: Optional[Union[Prompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs,\n    ) -&gt; OpenAIChatCompletion:\n        \"\"\"Asynchronously makes a call to the model using `prompt`.\n\n        Args:\n            prompt: The prompt to use for the call. This can either be a `Prompt`\n                instance, a raw string, or `None`. If `prompt` is `None`, then the call\n                will attempt to use the `messages` keyword argument.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            A `OpenAIChatCompletion` instance.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n        return OpenAIChatCompletion(\n            completion=await self.client.chat.completions.create(\n                model=self.model,\n                stream=False,\n                **kwargs,\n            ),\n            tool_types=openai_tools if tools else None,\n        )\n\n    async def stream(\n        self,\n        prompt: Optional[Union[Prompt, str]] = None,\n        tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n        **kwargs,\n    ) -&gt; AsyncGenerator[OpenAIChatCompletionChunk, None]:\n        \"\"\"Asynchronously streams the response for a call to the model using `prompt`.\n\n        Args:\n            prompt: The `Prompt` to use for the call.\n            tools: A list of `OpenAITool` types or `Callable` functions that the\n                creation call can decide to use. If `tools` is provided, `tool_choice`\n                will be set to `auto` unless manually specified.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Yields:\n            A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n        Raises:\n            ValueError: if neither `prompt` nor `messages` are provided.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        openai_tools = convert_tools_list_to_openai_tools(tools)\n        patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n        completion_stream = await self.client.chat.completions.create(\n            model=self.model,\n            stream=True,\n            **kwargs,\n        )\n\n        async for chunk in completion_stream:\n            yield OpenAIChatCompletionChunk(\n                chunk=chunk, tool_types=openai_tools if tools else None\n            )\n\n    async def extract(\n        self,\n        schema: Type[BaseModelT],\n        prompt: Optional[Union[Prompt, str]] = None,\n        retries: int = 0,\n        **kwargs,\n    ) -&gt; BaseModelT:\n        \"\"\"Extracts the given schema from the response of a chat `create` call async.\n\n        Args:\n            schema: The `BaseModel` schema to extract from the completion.\n            prompt: The prompt from which the schema will be extracted.\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments to pass to the API call. You can\n                find available keyword arguments here:\n                https://platform.openai.com/docs/api-reference/chat/create\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        tool = convert_base_model_to_openai_tool(schema)\n        completion = await self.create(\n            prompt,\n            tools=[tool],\n            tool_choice={\n                \"type\": \"function\",\n                \"function\": {\"name\": tool.__name__},\n            },\n            **kwargs,\n        )\n\n        try:\n            return schema(**completion.tool.model_dump())  # type: ignore\n        except ValidationError as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: update this to include failure history once prompts can handle\n                # chat history properly.\n                return await self.extract(schema, prompt, retries - 1)\n            raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/chat/models/openai_chat_async/#mirascope.chat.models.AsyncOpenAIChat.__init__","title":"<code>__init__(model='gpt-3.5-turbo', api_key=None, base_url=None, **kwargs)</code>","text":"<p>Initializes an instance of `AsyncOpenAIChat.</p> Source code in <code>mirascope/chat/models/async_openai_chat.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gpt-3.5-turbo\",\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Initializes an instance of `AsyncOpenAIChat.\"\"\"\n    self.client = AsyncOpenAI(api_key=api_key, base_url=base_url, **kwargs)\n    self.model = model\n</code></pre>"},{"location":"api/chat/models/openai_chat_async/#mirascope.chat.models.AsyncOpenAIChat.create","title":"<code>create(prompt=None, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously makes a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[Prompt, str]]</code> <p>The prompt to use for the call. This can either be a <code>Prompt</code> instance, a raw string, or <code>None</code>. If <code>prompt</code> is <code>None</code>, then the call will attempt to use the <code>messages</code> keyword argument.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAIChatCompletion</code> <p>A <code>OpenAIChatCompletion</code> instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/chat/models/async_openai_chat.py</code> <pre><code>async def create(\n    self,\n    prompt: Optional[Union[Prompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs,\n) -&gt; OpenAIChatCompletion:\n    \"\"\"Asynchronously makes a call to the model using `prompt`.\n\n    Args:\n        prompt: The prompt to use for the call. This can either be a `Prompt`\n            instance, a raw string, or `None`. If `prompt` is `None`, then the call\n            will attempt to use the `messages` keyword argument.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        A `OpenAIChatCompletion` instance.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n    return OpenAIChatCompletion(\n        completion=await self.client.chat.completions.create(\n            model=self.model,\n            stream=False,\n            **kwargs,\n        ),\n        tool_types=openai_tools if tools else None,\n    )\n</code></pre>"},{"location":"api/chat/models/openai_chat_async/#mirascope.chat.models.AsyncOpenAIChat.extract","title":"<code>extract(schema, prompt=None, retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Extracts the given schema from the response of a chat <code>create</code> call async.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModelT]</code> <p>The <code>BaseModel</code> schema to extract from the completion.</p> required <code>prompt</code> <code>Optional[Union[Prompt, str]]</code> <p>The prompt from which the schema will be extracted.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModelT</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/chat/models/async_openai_chat.py</code> <pre><code>async def extract(\n    self,\n    schema: Type[BaseModelT],\n    prompt: Optional[Union[Prompt, str]] = None,\n    retries: int = 0,\n    **kwargs,\n) -&gt; BaseModelT:\n    \"\"\"Extracts the given schema from the response of a chat `create` call async.\n\n    Args:\n        schema: The `BaseModel` schema to extract from the completion.\n        prompt: The prompt from which the schema will be extracted.\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    tool = convert_base_model_to_openai_tool(schema)\n    completion = await self.create(\n        prompt,\n        tools=[tool],\n        tool_choice={\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__},\n        },\n        **kwargs,\n    )\n\n    try:\n        return schema(**completion.tool.model_dump())  # type: ignore\n    except ValidationError as e:\n        if retries &gt; 0:\n            logging.info(f\"Retrying due to exception: {e}\")\n            # TODO: update this to include failure history once prompts can handle\n            # chat history properly.\n            return await self.extract(schema, prompt, retries - 1)\n        raise  # re-raise if we have no retries left\n</code></pre>"},{"location":"api/chat/models/openai_chat_async/#mirascope.chat.models.AsyncOpenAIChat.stream","title":"<code>stream(prompt=None, tools=None, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously streams the response for a call to the model using <code>prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[Prompt, str]]</code> <p>The <code>Prompt</code> to use for the call.</p> <code>None</code> <code>tools</code> <code>Optional[list[Union[Callable, Type[OpenAITool]]]]</code> <p>A list of <code>OpenAITool</code> types or <code>Callable</code> functions that the creation call can decide to use. If <code>tools</code> is provided, <code>tool_choice</code> will be set to <code>auto</code> unless manually specified.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the API call. You can find available keyword arguments here: https://platform.openai.com/docs/api-reference/chat/create</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[OpenAIChatCompletionChunk, None]</code> <p>A <code>OpenAIChatCompletionChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither <code>prompt</code> nor <code>messages</code> are provided.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/chat/models/async_openai_chat.py</code> <pre><code>async def stream(\n    self,\n    prompt: Optional[Union[Prompt, str]] = None,\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]] = None,\n    **kwargs,\n) -&gt; AsyncGenerator[OpenAIChatCompletionChunk, None]:\n    \"\"\"Asynchronously streams the response for a call to the model using `prompt`.\n\n    Args:\n        prompt: The `Prompt` to use for the call.\n        tools: A list of `OpenAITool` types or `Callable` functions that the\n            creation call can decide to use. If `tools` is provided, `tool_choice`\n            will be set to `auto` unless manually specified.\n        **kwargs: Additional keyword arguments to pass to the API call. You can\n            find available keyword arguments here:\n            https://platform.openai.com/docs/api-reference/chat/create\n\n    Yields:\n        A `OpenAIChatCompletionChunk` for each chunk of the response.\n\n    Raises:\n        ValueError: if neither `prompt` nor `messages` are provided.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    openai_tools = convert_tools_list_to_openai_tools(tools)\n    patch_openai_kwargs(kwargs, prompt, openai_tools)\n\n    completion_stream = await self.client.chat.completions.create(\n        model=self.model,\n        stream=True,\n        **kwargs,\n    )\n\n    async for chunk in completion_stream:\n        yield OpenAIChatCompletionChunk(\n            chunk=chunk, tool_types=openai_tools if tools else None\n        )\n</code></pre>"},{"location":"api/chat/parsers/","title":"chat.parsers","text":"<p>A module for interacting with Tool Parsers.</p>"},{"location":"api/chat/parsers/openai_tool_stream_parser/","title":"chat.parsers.OpenAIToolStreamParser","text":"<p>             Bases: <code>BaseModel</code></p> <p>A utility class to parse <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code>.</p> Source code in <code>mirascope/chat/parsers/openai_parser.py</code> <pre><code>class OpenAIToolStreamParser(BaseModel):\n    \"\"\"A utility class to parse `OpenAIChatCompletionChunk`s into `OpenAITools`.\"\"\"\n\n    tool_calls: list[ChatCompletionMessageToolCall] = []\n    tools: list[Union[Callable, Type[OpenAITool]]] = []\n\n    def from_stream(\n        self, stream: Generator[OpenAIChatCompletionChunk, None, None]\n    ) -&gt; Generator[OpenAITool, None, None]:\n        \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools`.\"\"\"\n        current_tool_type: Optional[Type[OpenAITool]] = None\n        for chunk in stream:\n            # Chunks start and end with None so we skip\n            if not chunk.tool_calls:\n                continue\n            # We are making what we think is a reasonable assumption here that\n            # tool_calls is never longer than 1. If it is, this will be updated.\n            tool_call_chunk = chunk.tool_calls[0]\n\n            if created_new_tool_call(self.tool_calls, tool_call_chunk):\n                current_tool_type = None\n\n            tool_call = self.tool_calls[tool_call_chunk.index]\n            if tool_call_chunk.id:\n                tool_call.id = tool_call_chunk.id\n\n            if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n                tool_class = find_tool_class(\n                    self.tool_calls, tool_call_chunk, self.tools\n                )\n                if tool_class:\n                    current_tool_type = tool_class\n\n            append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n            try:\n                if current_tool_type:\n                    tool_call = self.tool_calls[tool_call_chunk.index]\n                    yield current_tool_type.from_tool_call(tool_call)\n            except ValueError:\n                continue\n</code></pre>"},{"location":"api/chat/parsers/openai_tool_stream_parser/#mirascope.chat.parsers.OpenAIToolStreamParser.from_stream","title":"<code>from_stream(stream)</code>","text":"<p>Parses a stream of <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code>.</p> Source code in <code>mirascope/chat/parsers/openai_parser.py</code> <pre><code>def from_stream(\n    self, stream: Generator[OpenAIChatCompletionChunk, None, None]\n) -&gt; Generator[OpenAITool, None, None]:\n    \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools`.\"\"\"\n    current_tool_type: Optional[Type[OpenAITool]] = None\n    for chunk in stream:\n        # Chunks start and end with None so we skip\n        if not chunk.tool_calls:\n            continue\n        # We are making what we think is a reasonable assumption here that\n        # tool_calls is never longer than 1. If it is, this will be updated.\n        tool_call_chunk = chunk.tool_calls[0]\n\n        if created_new_tool_call(self.tool_calls, tool_call_chunk):\n            current_tool_type = None\n\n        tool_call = self.tool_calls[tool_call_chunk.index]\n        if tool_call_chunk.id:\n            tool_call.id = tool_call_chunk.id\n\n        if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n            tool_class = find_tool_class(\n                self.tool_calls, tool_call_chunk, self.tools\n            )\n            if tool_class:\n                current_tool_type = tool_class\n\n        append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n        try:\n            if current_tool_type:\n                tool_call = self.tool_calls[tool_call_chunk.index]\n                yield current_tool_type.from_tool_call(tool_call)\n        except ValueError:\n            continue\n</code></pre>"},{"location":"api/chat/parsers/openai_tool_stream_parser_async/","title":"chat.parsers.AsyncOpenAIToolStreamParser","text":"<p>             Bases: <code>BaseModel</code></p> <p>A utility class to parse <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code>.</p> <p>This is an async version of <code>OpenAIToolStreamParser</code>.</p> Source code in <code>mirascope/chat/parsers/openai_parser_async.py</code> <pre><code>class AsyncOpenAIToolStreamParser(BaseModel):\n    \"\"\"A utility class to parse `OpenAIChatCompletionChunk`s into `OpenAITools`.\n\n    This is an async version of `OpenAIToolStreamParser`.\n    \"\"\"\n\n    tool_calls: list[ChatCompletionMessageToolCall] = []\n    tools: list[Union[Callable, Type[OpenAITool]]] = []\n\n    async def from_stream(\n        self, stream: AsyncGenerator[OpenAIChatCompletionChunk, None]\n    ) -&gt; AsyncGenerator[OpenAITool, None]:\n        \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools` async.\"\"\"\n        current_tool_type: Optional[Type[OpenAITool]] = None\n        async for chunk in stream:\n            # Chunks start and end with None so we skip\n            if not chunk.tool_calls:\n                continue\n            # We are making what we think is a reasonable assumption here that\n            # tool_calls is never longer than 1. If it is, this will be updated.\n            tool_call_chunk = chunk.tool_calls[0]\n\n            if created_new_tool_call(self.tool_calls, tool_call_chunk):\n                current_tool_type = None\n\n            tool_call = self.tool_calls[tool_call_chunk.index]\n            if tool_call_chunk.id:\n                tool_call.id = tool_call_chunk.id\n\n            if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n                tool_class = find_tool_class(\n                    self.tool_calls, tool_call_chunk, self.tools\n                )\n                if tool_class:\n                    current_tool_type = tool_class\n\n            append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n            try:\n                if current_tool_type:\n                    tool_call = self.tool_calls[tool_call_chunk.index]\n                    yield current_tool_type.from_tool_call(tool_call)\n            except ValueError:\n                continue\n</code></pre>"},{"location":"api/chat/parsers/openai_tool_stream_parser_async/#mirascope.chat.parsers.AsyncOpenAIToolStreamParser.from_stream","title":"<code>from_stream(stream)</code>  <code>async</code>","text":"<p>Parses a stream of <code>OpenAIChatCompletionChunk</code>s into <code>OpenAITools</code> async.</p> Source code in <code>mirascope/chat/parsers/openai_parser_async.py</code> <pre><code>async def from_stream(\n    self, stream: AsyncGenerator[OpenAIChatCompletionChunk, None]\n) -&gt; AsyncGenerator[OpenAITool, None]:\n    \"\"\"Parses a stream of `OpenAIChatCompletionChunk`s into `OpenAITools` async.\"\"\"\n    current_tool_type: Optional[Type[OpenAITool]] = None\n    async for chunk in stream:\n        # Chunks start and end with None so we skip\n        if not chunk.tool_calls:\n            continue\n        # We are making what we think is a reasonable assumption here that\n        # tool_calls is never longer than 1. If it is, this will be updated.\n        tool_call_chunk = chunk.tool_calls[0]\n\n        if created_new_tool_call(self.tool_calls, tool_call_chunk):\n            current_tool_type = None\n\n        tool_call = self.tool_calls[tool_call_chunk.index]\n        if tool_call_chunk.id:\n            tool_call.id = tool_call_chunk.id\n\n        if append_tool_call_function_name(self.tool_calls, tool_call_chunk):\n            tool_class = find_tool_class(\n                self.tool_calls, tool_call_chunk, self.tools\n            )\n            if tool_class:\n                current_tool_type = tool_class\n\n        append_tool_call_arguments(self.tool_calls, tool_call_chunk)\n\n        try:\n            if current_tool_type:\n                tool_call = self.tool_calls[tool_call_chunk.index]\n                yield current_tool_type.from_tool_call(tool_call)\n        except ValueError:\n            continue\n</code></pre>"},{"location":"api/cli/","title":"cli","text":"<p>This module contains all functionality related to the Mirascope CLI.</p>"},{"location":"api/cli/commands/","title":"cli.commands","text":"<p>The Mirascope CLI prompt management tool.</p> <p>Typical usage example:</p> <pre><code>Initialize the environment:\n    $ mirascope init mirascope\n\nCreate a prompt in the prompts directory:\n    prompts/my_prompt.py\n\nAdd the prompt to create a version:\n    $ mirascope add my_prompt\n\nIterate on the prompt in the prompts directory:\n\nCheck the status of the prompt:\n    $ mirascope status my_prompt\n\nAdd the prompt to create a new version:\n    $ mirascope add my_prompt\n\nSwitch between prompts:\n    $ mirascope use my_prompt 0001\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.add","title":"<code>add(prompt_file_name=Argument(help='Prompt file to add', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name, default=''))</code>","text":"<p>Adds the given prompt to the specified version directory.</p> <p>The contents of the prompt in the user's prompts directory are copied to the version directory with the next revision number, and the version file is updated with the new revision.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_name</code> <code>str</code> <p>The name of the prompt file to add.</p> <code>Argument(help='Prompt file to add', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name, default='')</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the specified prompts directory.</p> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Add a prompt\")\ndef add(\n    prompt_file_name: str = Argument(\n        help=\"Prompt file to add\",\n        autocompletion=_prompts_directory_files,\n        parser=_parse_prompt_file_name,\n        default=\"\",\n    ),\n):\n    \"\"\"Adds the given prompt to the specified version directory.\n\n    The contents of the prompt in the user's prompts directory are copied to the version\n    directory with the next revision number, and the version file is updated with the\n    new revision.\n\n    Args:\n        prompt_file_name: The name of the prompt file to add.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the specified prompts directory.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n\n    # Check status before continuing\n    used_prompt_path = check_status(mirascope_settings, prompt_file_name)\n    if not used_prompt_path:\n        print(\"No changes detected.\")\n        return\n    class_directory = os.path.join(version_directory_path, prompt_file_name)\n\n    # Check if prompt file exists\n    if not os.path.exists(f\"{prompt_directory_path}/{prompt_file_name}.py\"):\n        raise FileNotFoundError(\n            f\"Prompt {prompt_file_name}.py not found in {prompt_directory_path}\"\n        )\n    # Create version directory if it doesn't exist\n    if not os.path.exists(class_directory):\n        os.makedirs(class_directory)\n    version_file_path = os.path.join(class_directory, version_file_name)\n    versions = get_prompt_versions(version_file_path)\n\n    # Open user's prompt file\n    with open(\n        f\"{prompt_directory_path}/{prompt_file_name}.py\", \"r+\", encoding=\"utf-8\"\n    ) as file:\n        # Increment revision id\n        if versions.latest_revision is None:\n            # first revision\n            revision_id = \"0001\"\n        else:\n            # default branch with incrementation\n            latest_revision_id = versions.latest_revision\n            revision_id = f\"{int(latest_revision_id)+1:04}\"\n        # Create revision file\n        revision_file = os.path.join(\n            class_directory, f\"{revision_id}_{prompt_file_name}.py\"\n        )\n        with open(\n            revision_file,\n            \"w+\",\n            encoding=\"utf-8\",\n        ) as file2:\n            custom_variables = {\n                \"prev_revision_id\": versions.current_revision,\n                \"revision_id\": revision_id,\n            }\n            file2.write(\n                write_prompt_to_template(\n                    file.read(), MirascopeCommand.ADD, custom_variables\n                )\n            )\n            keys_to_update = {\n                CURRENT_REVISION_KEY: revision_id,\n                LATEST_REVISION_KEY: revision_id,\n            }\n            update_version_text_file(version_file_path, keys_to_update)\n    if revision_file:\n        if mirascope_settings.format_command:\n            format_command: list[str] = mirascope_settings.format_command.split()\n            format_command.append(revision_file)\n            subprocess.run(\n                format_command,\n                check=True,\n                capture_output=True,\n            )\n    print(\n        \"Adding \"\n        f\"{version_directory_path}/{prompt_file_name}/{revision_id}_{prompt_file_name}.py\"\n    )\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.init","title":"<code>init(mirascope_location=Option(help='Main mirascope directory', default='.mirascope'), prompts_location=Option(help='Location of prompts directory', default='prompts'))</code>","text":"<p>Initializes the mirascope project.</p> <p>Creates the project structure and files needed for mirascope to work.</p> <p>Initial project structure: <pre><code>|\n|-- mirascope.ini\n|-- .mirascope\n|   |-- prompt_template.j2\n|   |-- versions/\n|   |   |-- &lt;directory_name&gt;/\n|   |   |   |-- version.txt\n|   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n|-- prompts/\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>mirascope_location</code> <code>str</code> <p>The root mirascope directory to create.</p> <code>Option(help='Main mirascope directory', default='.mirascope')</code> <code>prompts_location</code> <code>str</code> <p>The user's prompts directory.</p> <code>Option(help='Location of prompts directory', default='prompts')</code> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Initialize mirascope project\")\ndef init(\n    mirascope_location: str = Option(\n        help=\"Main mirascope directory\", default=\".mirascope\"\n    ),\n    prompts_location: str = Option(\n        help=\"Location of prompts directory\", default=\"prompts\"\n    ),\n) -&gt; None:\n    \"\"\"Initializes the mirascope project.\n\n    Creates the project structure and files needed for mirascope to work.\n\n    Initial project structure:\n    ```\n    |\n    |-- mirascope.ini\n    |-- .mirascope\n    |   |-- prompt_template.j2\n    |   |-- versions/\n    |   |   |-- &lt;directory_name&gt;/\n    |   |   |   |-- version.txt\n    |   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n    |-- prompts/\n    ```\n\n    Args:\n        mirascope_location: The root mirascope directory to create.\n        prompts_location: The user's prompts directory.\n    \"\"\"\n    destination_dir = Path.cwd()\n    versions_directory = os.path.join(mirascope_location, \"versions\")\n    os.makedirs(versions_directory, exist_ok=True)\n    print(f\"Creating {destination_dir}/{versions_directory}\")\n    os.makedirs(prompts_location, exist_ok=True)\n    print(f\"Creating {destination_dir}/{prompts_location}\")\n    prompts_init_file: Path = Path(f\"{destination_dir}/{prompts_location}/__init__.py\")\n    if not prompts_init_file.is_file():\n        prompts_init_file.touch()\n        print(f\"Creating {prompts_init_file}\")\n    # Create the 'mirascope.ini' file in the current directory with some default values\n    ini_settings = MirascopeSettings(\n        mirascope_location=mirascope_location,\n        versions_location=\"versions\",\n        prompts_location=prompts_location,\n        version_file_name=\"version.txt\",\n    )\n\n    # Get templates from the mirascope.cli.generic package\n    generic_file_path = files(\"mirascope.cli.generic\")\n    ini_path = generic_file_path.joinpath(\"mirascope.ini.j2\")\n    with open(str(ini_path), \"r\", encoding=\"utf-8\") as file:\n        template = Template(file.read())\n        rendered_content = template.render(ini_settings.model_dump())\n        destination_file_path = destination_dir / \"mirascope.ini\"\n        with open(destination_file_path, \"w\", encoding=\"utf-8\") as destination_file:\n            destination_file.write(rendered_content)\n            print(f\"Creating {destination_file_path}\")\n\n    # Create the 'prompt_template.j2' file in the mirascope directory specified by user\n    prompt_template_path = generic_file_path.joinpath(\"prompt_template.j2\")\n    with open(str(prompt_template_path), \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n    template_path = os.path.join(mirascope_location, \"prompt_template.j2\")\n    with open(template_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n        print(f\"Creating {destination_dir}/{template_path}\")\n\n    print(\"Initialization complete.\")\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.status","title":"<code>status(prompt_file_name=Argument(help='Prompt to check status on', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name))</code>","text":"<p>Checks the status of the current prompt or prompts.</p> <p>If a prompt is specified, the status of that prompt is checked. Otherwise, the status of all promps are checked. If a prompt has changed, the path to the prompt is printed.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_name</code> <code>Optional[str]</code> <p>(Optional) The name of the prompt file to check status on.</p> <code>Argument(help='Prompt to check status on', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name)</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the specified prompts directory.</p> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Check status of prompt(s)\")\ndef status(\n    prompt_file_name: Optional[str] = Argument(\n        help=\"Prompt to check status on\",\n        autocompletion=_prompts_directory_files,\n        parser=_parse_prompt_file_name,\n    ),\n) -&gt; None:\n    \"\"\"Checks the status of the current prompt or prompts.\n\n    If a prompt is specified, the status of that prompt is checked. Otherwise, the\n    status of all promps are checked. If a prompt has changed, the path to the prompt\n    is printed.\n\n    Args:\n        prompt_file_name: (Optional) The name of the prompt file to check status on.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the specified prompts directory.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    version_directory_path = mirascope_settings.versions_location\n\n    # If a prompt is specified, check the status of that prompt\n    if prompt_file_name:\n        used_prompt_path = check_status(mirascope_settings, prompt_file_name)\n        if used_prompt_path:\n            print(f\"Prompt {used_prompt_path} has changed.\")\n        else:\n            print(\"No changes detected.\")\n    else:  # Otherwise, check the status of all prompts\n        directores_changed: list[str] = []\n        for _, directories, _ in os.walk(version_directory_path):\n            for directory in directories:\n                used_prompt_path = check_status(mirascope_settings, directory)\n                if used_prompt_path:\n                    directores_changed.append(used_prompt_path)\n        if len(directores_changed) &gt; 0:\n            print(\"The following prompts have changed:\")\n            for prompt in directores_changed:\n                print(f\"\\t{prompt}\".expandtabs(4))\n        else:\n            print(\"No changes detected.\")\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.use","title":"<code>use(prompt_file_name=Argument(help='Prompt file to use', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name), version=Argument(help='Version of prompt to use'))</code>","text":"<p>Uses the version and prompt specified by the user.</p> <p>The contents of the prompt in the versions directory are copied to the user's prompts directory, based on the version specified by the user. The version file is updated with the new revision.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_name</code> <code>str</code> <p>The name of the prompt file to use.</p> <code>Argument(help='Prompt file to use', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name)</code> <code>version</code> <code>str</code> <p>The version of the prompt file to use.</p> <code>Argument(help='Version of prompt to use')</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the versions directory.</p> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Use a prompt\")\ndef use(\n    prompt_file_name: str = Argument(\n        help=\"Prompt file to use\",\n        autocompletion=_prompts_directory_files,\n        parser=_parse_prompt_file_name,\n    ),\n    version: str = Argument(\n        help=\"Version of prompt to use\",\n    ),\n) -&gt; None:\n    \"\"\"Uses the version and prompt specified by the user.\n\n    The contents of the prompt in the versions directory are copied to the user's\n    prompts directory, based on the version specified by the user. The version file is\n    updated with the new revision.\n\n    Args:\n        prompt_file_name: The name of the prompt file to use.\n        version: The version of the prompt file to use.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the versions directory.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    used_prompt_path = check_status(mirascope_settings, prompt_file_name)\n    # Check status before continuing\n    if used_prompt_path:\n        print(\"Changes detected, please add or delete changes first.\")\n        print(f\"\\tmirascope add {prompt_file_name}\".expandtabs(4))\n        return\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n    class_directory = os.path.join(version_directory_path, prompt_file_name)\n    revision_file_path = find_prompt_path(class_directory, version)\n    version_file_path = os.path.join(class_directory, version_file_name)\n    if revision_file_path is None:\n        raise FileNotFoundError(\n            f\"Prompt version {version} not found in {class_directory}\"\n        )\n    # Open versioned prompt file\n    with open(revision_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n    # Write to user's prompt file\n    prompt_file_path = os.path.join(prompt_directory_path, f\"{prompt_file_name}.py\")\n    with open(prompt_file_path, \"w+\", encoding=\"utf-8\") as file2:\n        file2.write(write_prompt_to_template(content, MirascopeCommand.USE))\n    if prompt_file_path:\n        if mirascope_settings.format_command:\n            format_command: list[str] = mirascope_settings.format_command.split()\n            format_command.append(prompt_file_path)\n            subprocess.run(\n                format_command,\n                check=True,\n                capture_output=True,\n            )\n\n    # Update version file with new current revision\n    keys_to_update = {\n        CURRENT_REVISION_KEY: version,\n    }\n    update_version_text_file(version_file_path, keys_to_update)\n\n    print(f\"Using {revision_file_path}\")\n</code></pre>"},{"location":"api/cli/constants/","title":"cli.constants","text":"<p>Constants for Mirascope CLI.</p>"},{"location":"api/cli/generic/","title":"cli.generic","text":"<p>This package contains generic templates that are used to initialize a mirascope project.</p>"},{"location":"api/cli/schemas/","title":"cli.schemas","text":"<p>Contains the schema for files created by the mirascope cli.</p>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.MirascopeSettings","title":"<code>MirascopeSettings</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the user's mirascope settings.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class MirascopeSettings(BaseModel):\n    \"\"\"Model for the user's mirascope settings.\"\"\"\n\n    mirascope_location: str\n    versions_location: str\n    prompts_location: str\n    version_file_name: str\n    format_command: Optional[str] = None\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.VersionTextFile","title":"<code>VersionTextFile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the version text file.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class VersionTextFile(BaseModel):\n    \"\"\"Model for the version text file.\"\"\"\n\n    current_revision: Optional[str] = Field(default=None)\n    latest_revision: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/cli/utils/","title":"cli.utils","text":"<p>Utility functions for the mirascope library.</p>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer","title":"<code>PromptAnalyzer</code>","text":"<p>             Bases: <code>NodeVisitor</code></p> <p>Utility class for analyzing a Mirascope prompt file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>class PromptAnalyzer(ast.NodeVisitor):\n    \"\"\"Utility class for analyzing a Mirascope prompt file.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the PromptAnalyzer.\"\"\"\n        self.imports = []\n        self.from_imports = []\n        self.variables = {}\n        self.classes = []\n        self.decorators = []\n        self.comments = \"\"\n\n    def visit_Import(self, node):\n        \"\"\"Extracts imports from the given node.\"\"\"\n        for alias in node.names:\n            self.imports.append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        \"\"\"Extracts from imports from the given node.\"\"\"\n        for alias in node.names:\n            self.from_imports.append((node.module, alias.name))\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        \"\"\"Extracts variables from the given node.\"\"\"\n        target = node.targets[0]\n        if isinstance(target, ast.Name):\n            self.variables[target.id] = ast.unparse(node.value)\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        \"\"\"Extracts classes from the given node.\"\"\"\n        class_info = {\n            \"name\": node.name,\n            \"bases\": [ast.unparse(b) for b in node.bases],\n            \"body\": \"\",\n            \"decorators\": [ast.unparse(d) for d in node.decorator_list],\n            \"docstring\": None,\n        }\n\n        # Extract docstring if present\n        docstring = ast.get_docstring(node, False)\n        if docstring:\n            class_info[\"docstring\"] = docstring\n\n        # Handle the rest of the class body\n        body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n        class_info[\"body\"] = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n        self.classes.append(class_info)\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Extracts decorators from function definitions.\"\"\"\n        for decorator in node.decorator_list:\n            self.decorators.append(ast.unparse(decorator))\n        self.generic_visit(node)\n\n    def visit_Module(self, node):\n        \"\"\"Extracts comments from the given node.\"\"\"\n        comments = ast.get_docstring(node, False)\n        self.comments = \"\" if comments is None else comments\n        self.generic_visit(node)\n\n    def check_class_changed(self, other: \"PromptAnalyzer\") -&gt; bool:\n        \"\"\"Compares the classes of this file with the classes of another file.\"\"\"\n        self_classes = {c[\"name\"]: c for c in self.classes}\n        other_classes = {c[\"name\"]: c for c in other.classes}\n\n        all_class_names = set(self_classes.keys()) | set(other_classes.keys())\n\n        for name in all_class_names:\n            if name in self_classes and name in other_classes:\n                # Compare attributes of classes with the same name\n                class_diff = {\n                    attr: (self_classes[name][attr], other_classes[name][attr])\n                    for attr in self_classes[name]\n                    if self_classes[name][attr] != other_classes[name][attr]\n                }\n                if class_diff:\n                    return True\n            else:\n                return True\n\n        return False\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PromptAnalyzer.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the PromptAnalyzer.\"\"\"\n    self.imports = []\n    self.from_imports = []\n    self.variables = {}\n    self.classes = []\n    self.decorators = []\n    self.comments = \"\"\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.check_class_changed","title":"<code>check_class_changed(other)</code>","text":"<p>Compares the classes of this file with the classes of another file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_class_changed(self, other: \"PromptAnalyzer\") -&gt; bool:\n    \"\"\"Compares the classes of this file with the classes of another file.\"\"\"\n    self_classes = {c[\"name\"]: c for c in self.classes}\n    other_classes = {c[\"name\"]: c for c in other.classes}\n\n    all_class_names = set(self_classes.keys()) | set(other_classes.keys())\n\n    for name in all_class_names:\n        if name in self_classes and name in other_classes:\n            # Compare attributes of classes with the same name\n            class_diff = {\n                attr: (self_classes[name][attr], other_classes[name][attr])\n                for attr in self_classes[name]\n                if self_classes[name][attr] != other_classes[name][attr]\n            }\n            if class_diff:\n                return True\n        else:\n            return True\n\n    return False\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Assign","title":"<code>visit_Assign(node)</code>","text":"<p>Extracts variables from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Assign(self, node):\n    \"\"\"Extracts variables from the given node.\"\"\"\n    target = node.targets[0]\n    if isinstance(target, ast.Name):\n        self.variables[target.id] = ast.unparse(node.value)\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ClassDef","title":"<code>visit_ClassDef(node)</code>","text":"<p>Extracts classes from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ClassDef(self, node):\n    \"\"\"Extracts classes from the given node.\"\"\"\n    class_info = {\n        \"name\": node.name,\n        \"bases\": [ast.unparse(b) for b in node.bases],\n        \"body\": \"\",\n        \"decorators\": [ast.unparse(d) for d in node.decorator_list],\n        \"docstring\": None,\n    }\n\n    # Extract docstring if present\n    docstring = ast.get_docstring(node, False)\n    if docstring:\n        class_info[\"docstring\"] = docstring\n\n    # Handle the rest of the class body\n    body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n    class_info[\"body\"] = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n    self.classes.append(class_info)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_FunctionDef","title":"<code>visit_FunctionDef(node)</code>","text":"<p>Extracts decorators from function definitions.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_FunctionDef(self, node):\n    \"\"\"Extracts decorators from function definitions.\"\"\"\n    for decorator in node.decorator_list:\n        self.decorators.append(ast.unparse(decorator))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Import","title":"<code>visit_Import(node)</code>","text":"<p>Extracts imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Import(self, node):\n    \"\"\"Extracts imports from the given node.\"\"\"\n    for alias in node.names:\n        self.imports.append(alias.name)\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ImportFrom","title":"<code>visit_ImportFrom(node)</code>","text":"<p>Extracts from imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ImportFrom(self, node):\n    \"\"\"Extracts from imports from the given node.\"\"\"\n    for alias in node.names:\n        self.from_imports.append((node.module, alias.name))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Module","title":"<code>visit_Module(node)</code>","text":"<p>Extracts comments from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Module(self, node):\n    \"\"\"Extracts comments from the given node.\"\"\"\n    comments = ast.get_docstring(node, False)\n    self.comments = \"\" if comments is None else comments\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_prompt_changed","title":"<code>check_prompt_changed(file1_path, file2_path)</code>","text":"<p>Checks if the given prompts have changed.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_prompt_changed(file1_path: Optional[str], file2_path: Optional[str]) -&gt; bool:\n    \"\"\"Checks if the given prompts have changed.\"\"\"\n    if file1_path is None or file2_path is None:\n        raise FileNotFoundError(\"Prompt or version file is missing.\")\n    # Parse the first file\n    try:\n        with open(file1_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file1_path} was not found.\") from e\n    analyzer1 = PromptAnalyzer()\n    tree1 = ast.parse(content)\n    analyzer1.visit(tree1)\n\n    # Parse the second file\n    try:\n        with open(file2_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file2_path} was not found.\") from e\n    analyzer2 = PromptAnalyzer()\n    tree2 = ast.parse(content)\n    analyzer2.visit(tree2)\n    # Compare the contents of the two files\n    differences = {\n        \"comments\": analyzer1.comments != analyzer2.comments,\n        \"imports_diff\": bool(set(analyzer1.imports) ^ set(analyzer2.imports)),\n        \"from_imports_diff\": bool(\n            set(analyzer1.from_imports) ^ set(analyzer2.from_imports)\n        ),\n        \"decorators_diff\": bool(set(analyzer1.decorators) ^ set(analyzer2.decorators)),\n        \"variables_diff\": set(analyzer1.variables.keys()) - ignore_variables\n        ^ set(analyzer2.variables.keys()) - ignore_variables,\n        \"classes_diff\": analyzer1.check_class_changed(analyzer2),\n        # Add other comparisons as needed\n    }\n    return any(differences.values())\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_status","title":"<code>check_status(mirascope_settings, directory)</code>","text":"<p>Checks the status of the given directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_status(\n    mirascope_settings: MirascopeSettings, directory: str\n) -&gt; Optional[str]:\n    \"\"\"Checks the status of the given directory.\"\"\"\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n    prompt_directory = os.path.join(version_directory_path, directory)\n    used_prompt_path = f\"{prompt_directory_path}/{directory}.py\"\n\n    # Get the currently used prompt version\n    versions = get_prompt_versions(f\"{prompt_directory}/{version_file_name}\")\n    if versions is None:\n        return used_prompt_path\n    current_head = versions.current_revision\n    if current_head is None:\n        return used_prompt_path\n    current_version_prompt_path = find_prompt_path(prompt_directory, current_head)\n\n    # Check if users prompt matches the current prompt version\n    has_file_changed = check_prompt_changed(\n        current_version_prompt_path, used_prompt_path\n    )\n    if has_file_changed:\n        return used_prompt_path\n    return None\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_file_names","title":"<code>find_file_names(directory, prefix='')</code>","text":"<p>Finds all files in a directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_file_names(directory: str, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"Finds all files in a directory.\"\"\"\n    pattern = f\"[!_]{prefix}*.py\"  # ignores private files\n    return glob.glob(pattern, root_dir=directory)  # Returns all files found\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_prompt_path","title":"<code>find_prompt_path(directory, prefix)</code>","text":"<p>Finds and opens the prompt with the given directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_prompt_path(directory: Union[Path, str], prefix: str) -&gt; Optional[str]:\n    \"\"\"Finds and opens the prompt with the given directory.\"\"\"\n    pattern = os.path.join(directory, prefix + \"*.py\")\n    prompt_files = glob.glob(pattern)\n\n    if not prompt_files:\n        return None  # No files found\n\n    # Return first file found\n    return prompt_files[0]\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_prompt_versions","title":"<code>get_prompt_versions(version_file_path)</code>","text":"<p>Returns the versions of the given prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_prompt_versions(version_file_path: str) -&gt; VersionTextFile:\n    \"\"\"Returns the versions of the given prompt.\"\"\"\n    versions = VersionTextFile()\n    try:\n        with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n            file.seek(0)\n            for line in file:\n                # Check if the current line contains the key\n                if line.startswith(CURRENT_REVISION_KEY + \"=\"):\n                    versions.current_revision = line.split(\"=\")[1].strip()\n                elif line.startswith(LATEST_REVISION_KEY + \"=\"):\n                    versions.latest_revision = line.split(\"=\")[1].strip()\n            return versions\n    except FileNotFoundError:\n        return versions\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_user_mirascope_settings","title":"<code>get_user_mirascope_settings(ini_file_path='mirascope.ini')</code>","text":"<p>Returns the user's mirascope settings.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_user_mirascope_settings(\n    ini_file_path: str = \"mirascope.ini\",\n) -&gt; MirascopeSettings:\n    \"\"\"Returns the user's mirascope settings.\"\"\"\n    config = ConfigParser(allow_no_value=True)\n    try:\n        read_ok = config.read(ini_file_path)\n        if not read_ok:\n            raise FileNotFoundError(\n                \"The mirascope.ini file was not found. Please run \"\n                \"`mirascope init` to create one or run the mirascope CLI from the \"\n                \"same directory as the mirascope.ini file.\"\n            )\n        mirascope_config = config[\"mirascope\"]\n        return MirascopeSettings(**mirascope_config)\n    except KeyError as e:\n        raise KeyError(\n            \"The mirascope.ini file is missing the [mirascope] section.\"\n        ) from e\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.update_version_text_file","title":"<code>update_version_text_file(version_file, updates)</code>","text":"<p>Updates the version text file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def update_version_text_file(version_file: str, updates: dict):\n    \"\"\"Updates the version text file.\"\"\"\n    try:\n        modified_lines = []\n        edits_made = {\n            key: False for key in updates\n        }  # Track which keys already exist in the file\n        version_file_path: Path = Path(version_file)\n        if not version_file_path.is_file():\n            version_file_path.touch()\n        # Read the file and apply updates\n        with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n            for line in file:\n                # Check if the current line contains any of the keys\n                for key, value in updates.items():\n                    if line.startswith(key + \"=\"):\n                        modified_lines.append(f\"{key}={value}\\n\")\n                        edits_made[key] = True\n                        break\n                else:\n                    # No key found, so keep the line as is\n                    modified_lines.append(line)\n\n            # Add any keys that were not found at the end of the file\n            for key, value in updates.items():\n                if not edits_made[key]:\n                    modified_lines.append(f\"{key}={value}\\n\")\n\n        # Write the modified content back to the file\n        with open(version_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.writelines(modified_lines)\n    except FileNotFoundError:\n        print(f\"The file {version_file} was not found.\")\n    except IOError as e:\n        print(f\"An I/O error occurred: {e}\")\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.write_prompt_to_template","title":"<code>write_prompt_to_template(file, command, variables=None)</code>","text":"<p>Writes the given prompt to the template.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def write_prompt_to_template(\n    file: str,\n    command: Literal[MirascopeCommand.ADD, MirascopeCommand.USE],\n    variables: Optional[dict] = None,\n):\n    \"\"\"Writes the given prompt to the template.\"\"\"\n    mirascope_directory = get_user_mirascope_settings().mirascope_location\n    if variables is None:\n        variables = {}\n    template_loader = FileSystemLoader(searchpath=mirascope_directory)\n    template_env = Environment(loader=template_loader)\n    template = template_env.get_template(\"prompt_template.j2\")\n    analyzer = PromptAnalyzer()\n    tree = ast.parse(file)\n    analyzer.visit(tree)\n    if command == MirascopeCommand.ADD:\n        new_variables = variables | analyzer.variables\n    else:  # command == MirascopeCommand.USE\n        variables = dict.fromkeys(ignore_variables, None)\n        new_variables = {\n            k: analyzer.variables[k] for k in analyzer.variables if k not in variables\n        }\n\n    data = {\n        \"comments\": analyzer.comments,\n        \"variables\": new_variables,\n        \"imports\": analyzer.imports,\n        \"from_imports\": analyzer.from_imports,\n        \"classes\": analyzer.classes,\n    }\n    return template.render(**data)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/","title":"LLM Convenience Wrappers","text":"<p>Mirascope provides convenience wrappers around the OpenAI client to make writing the code even more enjoyable. We purposefully pass <code>**kwargs</code> through our calls so that you always have direct access to their arguments.</p>"},{"location":"concepts/llm_convenience_wrappers/#why-should-you-care","title":"Why should you care?","text":"<ul> <li>Easy to learn<ul> <li>There's no magic here -- it's just python</li> <li>Chaining is no different from writing basic python functions</li> </ul> </li> <li>Convenient<ul> <li>You could do it yourself -- and you still can -- but there's just something nice about calling <code>str(res)</code> to get the response content</li> <li>You only need to pass in a <code>Prompt</code> and we'll handle the rest</li> </ul> </li> </ul>"},{"location":"concepts/llm_convenience_wrappers/#openaichat","title":"OpenAIChat","text":""},{"location":"concepts/llm_convenience_wrappers/#create","title":"Create","text":"<p>You can initialize an <code>OpenAIChat</code> instance and call <code>create</code> to generate an <code>OpenAIChatCompletion</code>:</p> <pre><code>from mirascope import OpenAIChat, Prompt\n\nclass RecipePrompt(Prompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\nchat = OpenAIChat(api_key=\"YOUR_OPENAI_API_KEY\")\ncompletion = chat.create(RecipePrompt(ingredient=\"apples\"))\nprint(completion)  # prints the string content of the completion\n</code></pre> <p>You can also pass a <code>str</code> in directly as your prompt if you'd prefer to use your own prompt tooling:</p> <pre><code>from mirascope import OpenAIChat\n\nchat = OpenAIChat(api_key=\"YOUR_OPENAI_API_KEY\")\ncompletion = chat.create(\"Recommend recipes that use apples.\")\nprint(completion)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#completion","title":"Completion","text":"<p>The <code>create</code> method returns an <code>OpenAIChatCompletion</code> class instance, which is a simple wrapper around the <code>ChatCompletion</code> class in <code>openai</code>. In fact, you can access everything from the original chunk as desired. The primary purpose of the class is to provide convenience.</p> <pre><code>from mirascope.chat.types import OpenAIChatCompletion\n\ncompletion = OpenAIChatCompletion(...)\n\ncompletion.completion  # ChatCompletion(...)\nstr(completion)        # original.choices[0].delta.content\ncompletion.choices     # original.choices\ncompletion.choice      # original.choices[0]\ncompletion.message     # original.choices[0].message\ncompletion.content     # original.choices[0].message.content\ncompletion.tool_calls  # original.choices[0].message.tool_calls\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#chaining","title":"Chaining","text":"<p>Adding a chain of calls is as simple as writing a function:</p> <pre><code>from mirascope import OpenAIChat, Prompt\n\nclass ChefPrompt(Prompt):\n    \"\"\"\n    Name the best chef in the world at cooking {food_type} food\n    \"\"\"\n\n    food_type: str\n\n\nclass RecipePrompt(Prompt):\n    \"\"\"\n    Recommend a recipe that uses {ingredient} as an ingredient\n    that chef {chef} would serve in their restuarant\n    \"\"\"\n\n    ingredient: str\n    chef: str\n\n\ndef recipe_by_chef_using(ingredient: str, food_type: str) -&gt; str:\n    \"\"\"Returns a recipe using `ingredient`.\n\n    The recipe will be generated based on what dish using `ingredient`\n    the best chef in the world at cooking `food_type` might serve in\n    their restaurant\n    \"\"\"\n    chat = OpenAIChat(api_key=\"YOUR_OPENAI_API_KEY\")\n    chef_prompt = ChefPrompt(food_type=food_type)\n    chef = str(chat.create(chef_prompt))\n    recipe_prompt = RecipePrompt(ingredient=ingredient, chef=chef)\n    return str(chat.create(recipe_prompt))\n\n\nrecipe = recipe_by_chef_using(\"apples\", \"japanese\")\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#streaming","title":"Streaming","text":"<p>You can use the <code>stream</code> method to stream a response. All this is doing is setting <code>stream=True</code> and providing the <code>OpenAIChatCompletionChunk</code> convenience wrappers around the response chunks.</p> <pre><code>chat = OpenAIChat()\nstream = chat.stream(prompt)\nfor chunk in stream:\n    print(str(chunk), end=\"\")\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#openaichatcompletionchunk","title":"OpenAIChatCompletionChunk","text":"<p>The <code>stream</code> method returns an <code>OpenAIChatCompletionChunk</code> instance, which is a convenience wrapper around the <code>ChatCompletionChunk</code> class in <code>openai</code></p> <pre><code>from mirascope.chat.types import OpenAIChatCompletionChunk\n\nchunk = OpenAIChatCompletionChunk(...)\n\nchunk.chunk    # ChatCompletionChunk(...)\nstr(chunk)     # original.choices[0].delta.content\nchunk.choices  # original.choices\nchunk.choice   # original.choices[0]\nchunk.delta    # original.choices[0].delta\nchunk.content  # original.choices[0].delta.content\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#extraction","title":"Extraction","text":"<p>Often you want to extract structured information into a format like JSON. The <code>extract</code> method makes this extremely easy by extracting the information into a Pydantic <code>BaseModel</code> schema that you define:</p> <pre><code>from mirascope import OpenAIChat\nfrom pydantic import BaseModel\n\nclass BookInfo(BaseModel):\n    \"\"\"Information about a book.\"\"\"\n\n    title: str\n    author: str\n\nbook_info = chat.extract(BookInfo, \"The Name of the Wind is by Patrick Rothfuss.\")\nassert isinstance(book_info, BookInfo)\nassert book_info.model_dump() == {\n    \"title\": \"The Name of the Wind\",\n    \"author\": \"Patrick Rothfuss\",\n}\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#retries","title":"Retries","text":"<p>Often you will want to retry your query in the event of a <code>ValidationError</code> when <code>extract</code> fails to convert the model response into the Pydantic model you've provided. Set the number of retries and <code>extract</code> will automatically retry up to that many times (by default <code>retries</code> is <code>0</code>):</p> <pre><code>book_info = chat.extract(\n    BookInfo,\n    \"The Name of the Wind is by Patrick Rothfuss.\",\n    retries=5,  # this will result in 6 total creation attempts if it never succeeds\n)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#tools","title":"Tools","text":"<p>Tools are extremely useful when you want the model to intelligently choose to output the arguments to call one or more functions. With mirascope it is extremely easy to use tools. Any function properly documented with a docstring will be automatically converted into a tool. This means that you can use any such function as a tool with no additional work:</p> <pre><code>from typing import Literal\n\nfrom mirascope import OpenAIChat, Prompt\n\nclass CurrentWeatherPrompt(Prompt):\n    \"\"\"What's the weather like in Los Angeles?\"\"\"\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON string containing the location, temperature, and unit.\n    \"\"\"\n    return f\"{location} is 65 degrees {unit}.\"\n\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\")\ncompletion = chat.create(\n    CurrentWeatherPrompt(),\n    tools=[get_current_weather],  # pass in the function itself\n)\n\nfor tool in completion.tools or []:\n    print(tool)                      # this is a `GetCurrentWeather` instance\n    print(tool.fn(**tool.__dict__))  # this will call `get_current_weather`\n</code></pre> <p>This works by automatically converting the given function into an <code>OpenAITool</code> class. The <code>completion.tools</code> property then returns an actual instance of the tool.</p> <p>You can also define your own <code>OpenAITool</code> class. This is necessary when the function you want to use as a tool does not have a docstring. Additionally, the <code>OpenAITool</code> class makes it easy to further update the descriptions, which is useful when you want to further engineer your prompt:</p> <pre><code>from typing import Literal\n\nfrom mirascope import OpenAIChat, OpenAITool, Prompt, openai_tool_fn\nfrom pydantic import Field\n\nclass CurrentWeatherPrompt(Prompt):\n    \"\"\"What's the weather like in Los Angeles?\"\"\"\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f\"{location} is 65 degrees {unit}.\"\n\n@openai_tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\")\ncompletion = chat.create(\n    CurrentWeatherPrompt(),\n    tools=[GetCurrentWeather],  # pass in the tool class\n)\n\nfor tool in completion.tools or []:\n    print(tool)                      # this is a `GetCurrentWeather` instance\n    print(tool.fn(**tool.__dict__))  # this will call `get_current_weather`\n</code></pre> <p>Notice that using the <code>openai_tool_fn</code> decorator will attach the function defined by the tool to the tool for easier calling of the function. This happens automatically when using the function directly.</p> <p>However, attaching the function is not necessary. In fact, often there are times where the intention of using a tool is not to call a function but to extract information from text. In these cases there is no need to attach the function at all. Simply define the <code>OpenAITool</code> class without the attached function and access the extracted information through the arguments of the <code>completion.tools</code> instances:</p> <pre><code>from typing import Literal\n\nfrom mirascope import OpenAIChat, OpenAITool, Prompt\nfrom pydantic import Field\n\nclass CurrentWeatherPrompt(Prompt):\n    \"\"\"What's the weather like in Los Angeles?\"\"\"\n\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\")\ncompletion = chat.create(\n    CurrentWeatherPrompt(),\n    tools=[GetCurrentWeather],  # pass in the tool class\n)\n\nfor tool in completion.tools or []:\n    print(tool)                      # this is a `GetCurrentWeather` instance\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#streaming-tools","title":"Streaming Tools","text":"<p>We also support streaming of tools using our <code>OpenAIToolStreamParser</code> class. Simply replace the <code>chat.create</code> with <code>chat.stream</code> and call <code>from_stream</code>, like so:</p> <pre><code>from typing import Callable, Literal, Type, Union\n\nfrom mirascope import OpenAIChat, OpenAITool, OpenAIToolStreamParser, Prompt\n\n# Using same OpenAITool and Prompt defined in Tool example\n\ntools: list[Union[Callable, Type[OpenAITool]]] = [GetCurrentWeather]\ncompletion = chat.stream(\n    CurrentWeatherPrompt(),\n    tools=tools,  # pass in the tool class\n)\n\nparser = OpenAIToolStreamParser(tools=tools) # pass in the same tool class\nfor tool in parser.from_stream(completion):\n    print(tool)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#async","title":"Async","text":"<p>All of the examples above also work with <code>async</code> by updating the import from <code>OpenAIChat</code> to <code>AsyncOpenAIChat</code>.</p>"},{"location":"concepts/llm_convenience_wrappers/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to the Mirascope models. Here is a list in no order of things we are thinking about adding next: </p> <ul> <li>data extraction streaming - extract information from text into a Partial Pydantic model</li> <li>additional models - support models beyond OpenAI, particularly OSS models</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"concepts/mirascope_cli/","title":"Mirascope CLI","text":"<p>One of the main frustrations of dealing with prompts is keeping track of all the various versions. Taking inspiration from alembic and git, the mirascope cli provides a couple of key pieces of functionality to make managing prompts easier.</p>"},{"location":"concepts/mirascope_cli/#the-prompt-management-environment","title":"The prompt management environment","text":"<p>The first step to using the Mirascope CLI is to use the <code>init</code> command in your project's root directory.</p> <pre><code>mirascope init\n</code></pre> <p>This will create the directories and files to help manage prompts. Here is a sample structure created by the <code>init</code> function: <pre><code>|\n|-- mirascope.ini\n|-- mirascope\n|   |-- prompt_template.j2\n|   |-- versions/\n|   |   |-- &lt;directory_name&gt;/\n|   |   |   |-- version.txt\n|   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n|-- prompts/\n</code></pre></p> <p>Here is a rundown of each directory and file:</p> <ul> <li><code>mirascope.ini</code> - The INI file that can be customized for your project</li> <li><code>mirascope</code> - The default name of the directory that is home to the prompt management environment</li> <li><code>prompt_template.j2</code> - The Jinja2 template file that is used to generate prompt versions</li> <li><code>versions</code> - The directory that holds the various prompt versions</li> <li><code>versions/&lt;directory_name</code> - The sub-directory that is created for each prompt file in the <code>prompts</code> directory</li> <li><code>version.txt</code> - A file system method of keeping track of current and latest revisions. Coming soon is revision tracking using a database instead</li> <li><code>&lt;revision_id&gt;_&lt;directory_name&gt;.py</code> - A prompt version that is created by the <code>mirascope add</code> command, more on this later.</li> <li><code>prompts</code> - The user's prompt directory that stores all prompt files</li> </ul> <p>The directory names can be changed anytime by modifying the <code>mirascope.ini</code> file or when running the <code>init</code> command.</p> <pre><code>mirascope init --mirascope_location my_mirascope --prompts_location my_prompts\n</code></pre>"},{"location":"concepts/mirascope_cli/#saving-your-first-prompt","title":"Saving your first prompt","text":"<p>After creating the prompt management directory, you are now ready to build and iterate on some prompts. Begin by adding a Mirascope Prompt to the prompts directory.</p> <pre><code># prompts/my_prompt.py\nfrom mirascope import Prompt\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n</code></pre> <p>Once you are happy with the first iteration of this prompt, you can run:</p> <pre><code>mirascope add my_prompt\n</code></pre> <p>This will commit <code>my_prompt.py</code> to your <code>versions/</code> directory, creating a <code>my_prompt</code> sub-directory and a <code>0001_my_prompt.py</code>.</p> <p>Here is what <code>0001_my_prompt.py</code> will look like:</p> <pre><code># versions/my_prompt/0001_my_prompt.py\nfrom mirascope import Prompt\n\nprev_revision_id = \"None\"\nrevision_id = \"0001\"\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n</code></pre> <p>The prompt inside the versions directory is almost identical to the prompt inside the prompts directory with a few differences.</p> <p>The variables <code>prev_revision_id</code> and <code>revision_id</code> will be used for features coming soon, so stay tuned for updates.</p>"},{"location":"concepts/mirascope_cli/#iterating-on-the-prompt","title":"Iterating on the prompt","text":"<p>Now that this version of <code>my_prompt</code> has been saved, you are now free to modify the original <code>my_prompt.py</code> and iterate. Maybe, you want to add a system message to obtain advice from a professional.</p> <p>Here is what the next iteration of <code>my_prompt.py</code> will look like:</p> <pre><code># prompts/my_prompt.py\nfrom mirascope.prompts import Prompt, messages\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are an expert in your field giving advice\n\n    USER:\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n</code></pre> <p>Before adding the next revision of <code>my_prompt</code>, you may want to check the status of your prompt.</p> <pre><code># You can specify a specific prompt\nmirascope status my_prompt\n\n# or, you can check the status of all prompts\nmirascope status\n</code></pre> <p>Note that status will be checked before the <code>add</code> or <code>use</code> command is run. Now we can run the same <code>add</code> command in the previous section to commit another version <code>0002_my_prompt.py</code></p>"},{"location":"concepts/mirascope_cli/#switching-between-versions","title":"Switching between versions","text":"<p>Often times when prompt engineering, you will want to try out different models with different prompts to obtain the best results.</p> <p>You can use the <code>use</code> command to quickly switch between the prompts:</p> <pre><code>mirascope use my_prompt 0001\n</code></pre> <p>Here you specify which prompt and also which version you want to use. This will update your <code>prompts/my_prompt.py</code> with the contents of <code>versions/0001_my_prompt.py</code> (minus the variables used internally).</p> <p>This will let you quickly swap prompts with no code change, the exception being when prompts have different properties.</p>"},{"location":"concepts/mirascope_cli/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to the Mirascope CLI. Here is a list in no order of things we are thinking about adding next: </p> <ul> <li>prompt comparison - A way to compare two different versions with a golden test</li> <li>remove - Remove a prompt</li> <li>history - View the revision history of a version</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"concepts/pydantic_prompts/","title":"Pydantic Prompts","text":"<p>The <code>Prompt</code> class is the core of Mirascope, which extends Pydantic's <code>BaseModel</code>. The class leverages the power of python to make writing more complex prompts as easy and readable as possible. The docstring is automatically formatted as a prompt so that you can write prompts in the style of your codebase.</p>"},{"location":"concepts/pydantic_prompts/#why-should-you-care","title":"Why should you care?","text":"<ul> <li>You get all of the benefits of using Pydantic:<ul> <li>type hints, json schema, customization, ecosystem, production-grade</li> </ul> </li> <li>Speeds up development<ul> <li>Fewer bugs through validation</li> <li>Auto-complete, editor (and linter) support for errors</li> </ul> </li> <li>Easy to learn         - You only need to learn Pydantic</li> <li>Standardization and compatibility<ul> <li>Integrations with other libraries that use JSON Schema such as OpenAPI and FastAPI means writing less code.</li> </ul> </li> <li>Customization<ul> <li>Everything is Pydantic or basic python, so changing anything is as easy as overriding what you want to change</li> </ul> </li> <li>All of the above helps lead to production ready code</li> </ul>"},{"location":"concepts/pydantic_prompts/#the-prompt-class","title":"The <code>Prompt</code> Class","text":"<p>The docstring of the class acts as the prompt's template, and the attributes act as the template variables:</p> <pre><code>from mirascope import Prompt\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nstr(prompt)\n</code></pre> <pre><code>Can you recommend some books on coding?\n</code></pre> <p>The <code>__str__</code> method, which formats all of the template variables, relies on the <code>template</code> function, which provides built-in string formatting so that you can write prettier docstrings. This means that longer prompts will still look well-formatted in your code base:</p> <pre><code>class LongerPrompt(Prompt):\n    \"\"\"\n    Longer prompts can be edited in a more organized format that looks\n    better in your code base. Any unwanted characters such as newlines\n    or tabs that are purely for text alignment and structure will be \n    automatically removed.\n\n    For newlines, just add one extra (e.g. 2 newlines -&gt; 1 newline here)\n\n        - The same goes for things you want indented\n    \"\"\"\n</code></pre> <p>Note</p> <p>If you want custom docstring formatting or none at all, simply override the <code>template</code> method.</p>"},{"location":"concepts/pydantic_prompts/#editor-support","title":"Editor Support","text":"<ul> <li>Inline Errors </li> <li>Autocomplete </li> </ul>"},{"location":"concepts/pydantic_prompts/#template-variables","title":"Template Variables","text":"<p>When you call <code>str(prompt)</code> the template will be formatted using the properties of the class that match the template variables. This means that you can define more complex properties through code. This is particularly useful when you want to inject template variables with custom formatting or template variables that depend on multiple attributes. </p> <pre><code>from mirascope import Prompt\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on the following topic and genre pairs?\n\n        {topics_x_genres}\n    \"\"\"\n\n    topics: list[str]\n    genres: list[str]\n\n    @property\n    def topics_x_genres(self) -&gt; str:\n        \"\"\"Returns `topics` as a comma separated list.\"\"\"\n        return \"\\n\\t\".expandtabs(4).join(\n            [\n                f\"Topic: {topic}, Genre: {genre}\"\n                for topic in self.topics\n                for genre in self.genres\n            ]\n        )\n\nprompt = BookRecommendationPrompt(\n    topics=[\"coding\", \"music\"], genres=[\"fiction\", \"fantasy\"]\n)\nstr(prompt)\n</code></pre> <pre><code>Can you recommend some books on the following topic and genre pairs?\n    Topic: coding, Genre: fiction\n    Topic: coding, Genre: fantasy\n    Topic: music, Genre: fiction\n    Topic: music, Genre fantasy\n</code></pre>"},{"location":"concepts/pydantic_prompts/#messages","title":"Messages","text":"<p>By default, the <code>Prompt</code> class treats the prompt template as a single user message. If you want to specify a list of messages instead, we provide a decorator to make this easy:</p> <p>Note</p> <p><code>@messages</code> decorator adds <code>messages</code> property to the class</p> <pre><code>from mirascope import messages, Prompt\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages)\n</code></pre> <pre><code>[(\"system\", \"You are the world's greatest librarian\"), (\"user\", \"Can you recommend some books on coding?\")]\n</code></pre>"},{"location":"concepts/pydantic_prompts/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to Mirascope prompts. Here is a list in no order of things we are thinking about adding next: </p> <ul> <li>more complex prompts - handle more complex prompts for things like history</li> <li>testing for prompts - test the quality of your prompt input/output</li> <li>prompt response tracking - track the input/output when using a prompt</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"cookbook/basic_examples/","title":"Basic Examples","text":"<p>Note</p> <p>Take a look at the code in our repo.</p> <p>Full walkthrough with explanations coming soon...</p>"},{"location":"cookbook/rag/","title":"Retrieval Augmented Generation (RAG)","text":"<p>When we need to provide additional information to the model that it hasn't yet been trained on, we can retrieve the relevant information from an external source and provide it as context to our model. For our example, we will use a dataset of\u00a0BBC news articles from 2004. We will:</p> <ul> <li>query the dataset with a topic of our choosing</li> <li>retrieve a number of relevant articles</li> <li>ask our chat model to summarize the relevant ones</li> <li>show how to query the dataset both locally and using a vector database (Pinecone)</li> <li>show how Mirascope can simplify RAG.</li> </ul> <p>Note</p> <p>The following code snippets have been moved around for the sake of clarity in the walkthrough, and may not work if you copy and paste them. For a fully functional script, take a look at the code in our repo.</p>"},{"location":"cookbook/rag/#before-we-get-started","title":"Before we get started","text":"<p>If you haven't already, it will be worth taking a look some of our relevant concept pages for a more detailed explanation of the functionality we'll be using in this walkthrough.</p> <ul> <li>Pydantic Prompts\u00a0(template variables, messages)</li> <li>LLM Convenience Wrappers</li> </ul> <p>In addition, here are the variables we will be using in this cookbook recipe:</p> <pre><code># .env\n\nOPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\nPINECONE_API_KEY = \"YOUR_PINECONE_API_KEY\"\n</code></pre> <pre><code># config.py\n\nfrom typing import Optional\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nMODEL = \"gpt-3.5-turbo\"\nEMBEDDINGS_MODEL = \"text-embedding-ada-002\"\nMAX_TOKENS = 1000\nTEXT_COLUMN = \"Text\"\nEMBEDDINGS_COLUMN = \"embeddings\"\nFILENAME = \"news_article_dataset.pkl\"\nURL = \"https://raw.githubusercontent.com/Dawit-1621/BBC-News-Classification/main/Data/BBC%20News%20Test.csv\"\nPINECONE_INDEX = \"news-articles\"\nPINECONE_NAMESPACE = \"articles\"\n\nclass Settings(BaseSettings):\n    openai_api_key: Optional[str] = None\n    pinecone_api_key: Optional[str] = None\n\n    model_config = SettingsConfigDict(env_file=\".env\")\n</code></pre>"},{"location":"cookbook/rag/#load-and-preprocess-the-data","title":"Load and preprocess the data","text":"<p>Before we load raw data into a pandas\u00a0<code>Dataframe</code>, we have to handle large texts which may exceed token limits. In our case, we are using <code>gpt-3.5-turbo</code>, which has a token limit of 4096. A crude solution is to split any article of token count greater than <code>MAX_TOKENS=1000</code> into equal chunks, with each resulting chunk consisting of fewer than <code>MAX_TOKENS</code>. This way we can fit up to 3 article snippets as well as any text in our hand-written section of the prompt. Note that we have also made sure that <code>MAX_TOKENS</code> is less than the token limit for our embedding model <code>text-embedding-ada-02</code> that has a otken limit of 8191.</p> <p>The function\u00a0<code>split_text()</code>\u00a0below contains the implementation.</p> <p>For counting the number of tokens in an article, we will use\u00a0tiktoken\u00a0since we will be using OpenAI for both the chat and embedding models.\u00a0tiktoken\u00a0is a useful library provided by OpenAI for encoding strings and decoding tokens for their models.</p> <pre><code># utils.py\n\nimport pandas as pd\nimport tiktoken\nfrom config import MODEL, TEXT_COLUMN,\n\ndef load_data(url: str, max_tokens: int) -&gt; pd.DataFrame:\n    \"\"\"Loads data from a url after splitting larger texts into smaller chunks.\n\n    Args:\n        url: the url to load the data from.\n        max_tokens: the maximum number of tokens per chunk.\n    Returns:\n        The dataframe with the data from the url.\n    \"\"\"\n    df = pd.read_csv(url)\n    split_articles = []\n    encoder = tiktoken.encoding_for_model(MODEL)\n    for i, row in df.iterrows():\n        text = row[TEXT_COLUMN]\n        tokens = encoder.encode(text)\n        if len(tokens) &gt; max_tokens:\n            split_articles += split_text(text, tokens, max_tokens)\n            df.drop(i, inplace=True)\n\n    # Long texts which were dropped from the dataframe are now readded.\n    df = pd.concat(\n        [df, pd.DataFrame(split_articles, columns=[TEXT_COLUMN])], ignore_index=True\n    )\n\n    return df\n\n\ndef split_text(text: str, tokens: list[int], max_tokens: int) -&gt; list[str]:\n    \"\"\"Roughly splits a text into chunks according to max_tokens.\n\n    Text is split into equal word counts, with number of splits determined by how many\n    times `max_tokens` goes into the total number of tokens (including partially). Note\n    that tokens and characters do not have an exact correspondence, so in certain edge\n    cases a chunk may be slightly larger than max_tokens.\n\n    Args:\n        text: The text to split.\n        tokens: How many tokens `text` is.\n        max_tokens: The (rough) number of maximum tokens per chunk.\n    Returns:\n        A list of the split texts.\n    \"\"\"\n    words = text.split()\n    num_splits = len(tokens) // max_tokens + 1\n    split_texts = []\n    for i in range(num_splits):\n        start = i * len(words) // num_splits\n        end = (i + 1) * len(words) // num_splits\n        split_texts.append(\" \".join(words[start:end]))\n\n    return split_texts\n</code></pre> <pre><code># rag_example.py\n\ndf = load_data(url=URL, max_tokens=MAX_TOKENS)\n</code></pre> <p>For further clarity, here is an example of the output of <code>split_texts()</code>:</p> <pre><code># sample_output of split_texts()\n\ntext = \"...\"\ntokens = encoder.encode(text)\nprint(len(tokens))\n\n# Output: 3200\n\nsplit_texts = split_text(text=text, tokens=tokens, max_tokens=1000)\nprint([len(encoder.encode(split_text)) for split_text in split_texts])\n\n# Output: [800 800, 800, 800]\n# Explanation: 4 is the minimum number of times to split 3200 until each\n# piece is less than max_tokens=1000, so we get 3200/4 = 800.\n</code></pre> <p>Great! Now our <code>Dataframe</code> is loaded in with article snippets where each snippet is less than a thousand tokens.</p>"},{"location":"cookbook/rag/#embeddings","title":"Embeddings","text":"<p>To be able to take a topic of our choosing and determine each article\u2019s relevancy, we need to embed them. We define some helper functions to use OpenAI's embeddings:</p> <pre><code># utils.py\n\nfrom typing import Union\n\nimport pandas as pd\nimport tiktoken\nfrom config import EMBEDDINGS_COLUMN, EMBEDDINGS_MODEL, MODEL\nfrom openai import OpenAI\n\ndef embed_with_openai(text: Union[str, list[str]], client: OpenAI) -&gt; list[list[float]]:\n    \"\"\"Embeds a string using OpenAI's embedding model.\n\n    Args:\n        text: A `str` or list of `str` to embed.\n        client: The `OpenAI` instance used for embedding.\n\n    Returns:\n        The embeddings of the text.\n    \"\"\"\n    if isinstance(text, str):\n        text = [text]\n    embeddings_response = client.embeddings.create(model=EMBEDDINGS_MODEL, input=text)\n    return [datum.embedding for datum in embeddings_response.data]\n\n\ndef embed_df_with_openai(\n    df: pd.DataFrame,\n    client: OpenAI,\n) -&gt; pd.DataFrame:\n    \"\"\"Embeds a Pandas Series of texts in batches using minimal OpenAI calls.\n\n    Note that this function assumes all texts are less than 8192 tokens long.\n\n    Args:\n        texts: The texts to embed.\n        client: The `OpenAI` instance used for embedding.\n\n    Returns:\n        The dataframe with the embeddings column added.\n    \"\"\"\n    encoder = tiktoken.encoding_for_model(MODEL)\n    max_tokens = 8191\n\n    embeddings: list[list[float]] = []\n    batch: list[str] = []\n    batch_token_count = 0\n\n    # We can embed multiple texts in a single OpenAI call, so we implement a\n    # simple greedy algorithm according to ada-02's token limit of 8191.\n    for i, text in enumerate(df[TEXT_COLUMN]):\n        if batch_token_count + len(encoder.encode(text)) &gt; max_tokens:\n            embeddings += embed_with_openai(batch, client)\n            batch = [text]\n            batch_token_count = len(encoder.encode(text))\n        else:\n            batch.append(text)\n            batch_token_count += len(encoder.encode(text))\n\n    if batch:\n        embeddings += embed_with_openai(batch, client)\n\n    df[EMBEDDINGS_COLUMN] = embeddings\n    return df\n</code></pre> <p>We call these functions on our pandas <code>Dataframe</code> of article snippets, giving us the embedding of each article snippet in the new column <code>EMBEDDINGS_COLUMN=\"embeddings\"</code>.</p> <pre><code># rag_example.py\n\nchat = OpenAIChat(api_key=os.getenv(\"OPENAI_API_KEY\"))\ndf = embed_df_with_openai(df=df, chat=chat.client)\n\n# df[TEXT_COLUMN] contains article snippets\n# df[EMBEDDINGS_COLUMN] contains embedding for each snippet\n</code></pre>"},{"location":"cookbook/rag/#retrieval-but-built-into-our-prompts","title":"Retrieval ... but built into our prompts","text":"<p>We mentioned earlier that we will show how to perform retrieval in two ways: locally and via a vector database (Pinecone). In your own projects, you may want to perform retrieval in an entirely different way.</p> <p>With Mirascope <code>Prompt</code>, we can use Python built-ins with Pydantic to implement complex, prompt-specific logic directly within the prompt itself \u2014 we can focus on prompt engineering, not the little things. This ensures that prompt-specific logic is well encapsulated, forcing a clean separation from the rest of the codebase. Furthermore, any updates to the prompt logic or template can be maintained and versioned with our CLI - check that out here.</p> <p>In this example, we are going to create two different prompts:</p> <ul> <li><code>LocalNewsRagPrompt</code>: this prompt will use a local <code>pd.DataFrame</code> to find the relevant article chunks.</li> <li><code>PineconeNewsRagPrompt</code>: this prompt will query a Pinecone vector database to find the relevant article chunks.</li> </ul> <p>The querying logic for relevant article retrieval will live within each prompt's <code>context</code> property, regardless of whether it is the local or vector database implementation. In the local iteration, we manually calculate (using the dot product) the distances between each article snippet's embedding and the embedding of our chosen topic - the articles whose embeddings are closest are then chosen. For the vector database, we make a Pinecone API call to perform the same task via their streamlined architecture.</p> <p>Note</p> <p>The querying logic for Pinecone lives within the PineconeNewsRagPrompt, but you must still do a one-time pinecone setup.</p>"},{"location":"cookbook/rag/#localnewsragprompt","title":"LocalNewsRagPrompt","text":"<pre><code># rag_prompts/local_news_rag_prompt.py\n\nimport numpy as np\nimport pandas as pd\nfrom config import EMBEDDINGS_COLUMN, TEXT_COLUMN, Settings\nfrom openai import OpenAI\nfrom pydantic import ConfigDict\nfrom utils import embed_with_openai\n\nfrom mirascope import Prompt, messages\n\nsettings = Settings()\n\n\n@messages\nclass LocalNewsRagPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are an expert at:\n    1) determining the relevancy of articles to a topic, and\n    2) summarizing articles concisely and eloquently.\n\n    When given a topic and a list of possibly relevant texts, you format your responses\n    as a single list, where you summarize the articles relevant to the topic or explain\n    why the article is not relevant to the topic.\n\n    USER:\n    Here are {num_statements} article snippets about this topic: {topic}\n\n    {context}\n\n    Pick only the snippets which are truly relevant to the topic, and summarize them.\n    \"\"\"\n\n    num_statements: int\n    topic: str\n    df: pd.DataFrame\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def context(self) -&gt; str:\n        \"\"\"Finds most similar articles in dataframe using embeddings.\"\"\"\n\n        query_embedding = embed_with_openai(\n            self.topic, OpenAI(api_key=settings.openai_api_key)\n        )[0]\n        self.df[\"similarities\"] = self.df[EMBEDDINGS_COLUMN].apply(\n            lambda x: np.dot(x, query_embedding)\n        )\n        most_similar = self.df.sort_values(\"similarities\", ascending=False).iloc[\n            : self.num_statements\n        ][TEXT_COLUMN]\n        statements = most_similar.to_list()\n        return \"\\n\".join(\n            [f\"{i+1}. {statement}\" for i, statement in enumerate(statements)]\n        )\n</code></pre>"},{"location":"cookbook/rag/#pineconenewsragprompt","title":"PineconeNewsRagPrompt","text":"<pre><code># rag_prompts/pinecone_news_rag_prompt.py\n\nimport pandas as pd\nfrom config import PINECONE_INDEX, PINECONE_NAMESPACE, TEXT_COLUMN, Settings\nfrom openai import OpenAI\nfrom pinecone import Pinecone\nfrom pydantic import ConfigDict\nfrom utils import embed_with_openai\n\nfrom mirascope import Prompt, messages\n\nsettings = Settings()\n\n\n@messages\nclass PineconeNewsRagPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are an expert at:\n    1) determining the relevancy of articles to a topic, and\n    2) summarizing articles concisely and eloquently.\n\n    When given a topic and a list of possibly relevant texts, you format your responses\n    as a single list, where you summarize the articles relevant to the topic or explain\n    why the article is not relevant to the topic.\n\n    USER:\n    Here are {num_statements} article snippets about this topic: {topic}\n\n    {context}\n\n    Pick only the snippets which are truly relevant to the topic, and summarize them.\n    \"\"\"\n\n    num_statements: int\n    topic: str\n    df: pd.DataFrame\n\n    _index: Pinecone.Index\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **data):\n        super().__init__(**data)\n        pc = Pinecone(api_key=settings.pinecone_api_key)\n        self._index = pc.Index(PINECONE_INDEX)\n\n    @property\n    def context(self) -&gt; str:\n        \"\"\"Finds most similar articles in pinecone using embeddings.\"\"\"\n        query_embedding = embed_with_openai(\n            self.topic, OpenAI(api_key=settings.openai_api_key)\n        )[0]\n        query_response = self._index.query(\n            namespace=PINECONE_NAMESPACE,\n            vector=query_embedding,\n            top_k=self.num_statements,\n        )\n        indices = [int(article[\"id\"]) for article in query_response[\"matches\"]]\n        statements = self.df.iloc[indices][TEXT_COLUMN].to_list()\n        return \"\\n\".join(\n            [f\"{i+1}. {statement}\" for i, statement in enumerate(statements)]\n        )\n</code></pre>"},{"location":"cookbook/rag/#retrieval-script-for-topics","title":"Retrieval Script For Topics","text":"<p>We've built the retrieval functionality into the prompts themselves - now, we can build a script to use these prompts without having to worry about formatting the context or retrieving relevant articles:</p> <pre><code># rag_example.py\n\nimport os\nfrom argparse import ArgumentParser\n\nimport pandas as pd\nfrom config import FILENAME, MAX_TOKENS, URL, Settings\nfrom local_news_rag_prompt import LocalNewsRagPrompt\nfrom pinecone_news_rag_prompt import PineconeNewsRagPrompt\nfrom setup_pinecone import setup_pinecone\nfrom utils import embed_df_with_openai, load_data\n\nfrom mirascope import OpenAIChat\n\nsettings = Settings()\n\ndef main(use_pinecone=False):\n    chat = OpenAIChat(api_key=settings.openai_api_key)\n    df = load_data(url=URL, max_tokens=MAX_TOKENS)\n    df = embed_df_with_openai(df=df, client=chat.client)\n\n    # This method does nothing if a Pinecone index is already set up.\n    if use_pinecone:\n        setup_pinecone(df=df)\n\n    topics = [\n        \"soccer teams/players going through trouble\",\n        \"environmental factors affecting economy\",\n        \"celebrity or politician scandals\",\n    ]\n    for topic in topics:\n        if use_pinecone:\n            print(\n                chat.create(PineconeNewsRagPrompt(num_statements=3, topic=topic, df=df))\n            )\n        else:\n            print(chat.create(LocalNewsRagPrompt(num_statements=3, topic=topic, df=df)))\n        print(\"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Process some flags.\")\n    parser.add_argument(\n        \"-pc\", \"--pinecone\", action=\"store_true\", help=\"Activate Pinecone mode\"\n    )\n    args = parser.parse_args()\n    main(use_pinecone=args.pinecone)\n</code></pre>"},{"location":"cookbook/squad_extraction/","title":"SQuAD 2.0: Extracting Answers To Questions From Context Using Mirascope","text":"<p>The SQuAD 2.0 Dataset is a dataset for question-answering. Each question is either impossible to answer given the context paragraph or has answers exactly as written in the paragraph. It turns out that it's quite difficult to get an LLM to ignore it's world knowledge and output \"\" if the paragraph does not contain the answer. For this recipe, we'll be using a modified version of the dataset, restricting our questions to a single article (Geology) and removing all of the impossible questions. <p>Note</p> <p>The <code>geology-squad.json</code> modified version of the dataset is on our GitHub along with the full code for this recipe.</p> <p>The first step is loading the dataset. We've written a helper function <code>load_geology_squad</code> that will load and return a <code>list[QuestionWithContext]</code>, defined below:</p> <pre><code>from pydantic import BaseModel\n\nclass QuestionWithContext(BaseModel):\n    \"\"\"A question with it's answers and context.\"\"\"\n\n    id: str\n    question: str\n    context: str\n    answers: list[str]\n</code></pre>"},{"location":"cookbook/squad_extraction/#initial-basic-extraction","title":"Initial Basic Extraction","text":"<p>To extract an answer to a question, we can use the <code>OpenAIChat.extract</code> method to extract the answer. First, let's create a super basic schema and prompt for asking the question and extracting an answer:</p> <pre><code>$ mirascope init --prompts_location squad_prompts; touch prompts/question.py\n</code></pre> <pre><code># squad_prompts/question.py\nfrom mirascope import Prompt\nfrom pydantic import BaseModel, Field\n\n\nclass ExtractedAnswer(BaseModel):\n    \"\"\"The answer to a question about a paragraph of text.\"\"\"\n\n    answer: str\n\n\nclass QuestionPrompt(Prompt):\n    \"\"\"\n    Paragraph: {paragraph}\n\n    Question: {question}\n    \"\"\"\n\n    paragraph: str\n    question: str\n</code></pre> <p>Next we'll define the schema we want to extract from the paragraph and a function to extract the schema from a given question:</p> <pre><code>from pydantic import BaseModel\n\nfrom config import Settings\nfrom prompts.question import ExtractedAnswer, QuestionPrompt\n\nsettings = Settings()\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\", api_key=settings.openai_api_key)\n\n\ndef extract_answer(question: QuestionWithContext) -&gt; str:\n    \"\"\"Returns the extracted `str` answer to `question`.\"\"\"\n    return chat.extract(\n        ExtractedAnswer,\n        QuestionPrompt(paragraph=question.context, question=question.question),\n        retries=2,  # retry up to 2 more times on validation error\n    ).answer\n</code></pre> <p>Now we just need to load the data and extract our answers:</p> <pre><code>import json\n\nfrom squad import load_geology_squad\n\nextracted_answers = {\n    question.id: extract_answer(question)\n    for question in load_geology_squad()\n}\nwith open(\"geology-squad-answers-v1.json\", \"w\") as f:\n    json.dump(extracted_answers, f)\n</code></pre> <p>Running the included modified version of the SQuAD eval script gives us the following breakdown on metrics, which we can use to see how well our system is working:</p> <pre><code>$ python eval.py geology-squad.json geology-squad-answers-v1.json\n</code></pre> <pre><code>{\n  \"exact\": 79.3103448275862,\n  \"f1\": 89.4777077966733,\n  \"total\": 116,\n}\n</code></pre> <p>Exact match is determined with a raw string match, whereas F1 score is calculated by token to account for partial matches. With our simple script, we were able to get nearly 80% exact match and an F1 score of nearly 90 on our 116 examples. Before we move on, let's include this metadata in the docstring of our prompt file and version it with the CLI:</p> <pre><code>$ mirascope add question\n</code></pre>"},{"location":"cookbook/squad_extraction/#engineering-a-better-prompt-by-analyzing-results-in-oxen","title":"Engineering A Better Prompt By Analyzing Results In Oxen","text":"<p>Now let's make the script even better. To better understand where our LLM is making mistakes, we can upload a transformed version of our data to Oxen and dig around a little.</p> <p>Note</p> <p>The <code>push_to_oxen.py</code> script shows how we upload each prompt version's answer for analysis.</p> <p></p> <p>Two things stick out immediately about the extracted answers:</p> <ol> <li>Often long or a full sentence instead of a short, concise answer.</li> <li>Not an exact match with the context paragraph.</li> </ol> <p>Let's update our schema and our prompt so that the LLM tries to extract more concise answers that better match the context paragraph:</p> <pre><code>from mirascope import Prompt, messages\n\nfrom pydantic import BaseModel\n\nclass ExtractedAnswer(BaseModel):\n    \"\"\"The answer to a question about a paragraph of text.\"\"\"\n\n    answer: str = Field(\n        ...,\n        description=(\n            \"The extracted answer to the question. This answer is as concise \"\n            \"as possible, most often just a single word. It is also an exact \"\n            \"text match with text in the provided context.\"\n        ),\n    )\n\n\n@messages\nclass QuestionPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You will be asked a question after you read a paragraph. Your task is to\n    answer the question based on the information in the paragraph. Your answer\n    should be an exact text match to text from the paragraph. Your answer should\n    also be one or two words at most is possible.\n\n    USER:\n    Paragraph: {paragraph}\n\n    USER:\n    Question: {question}\n    \"\"\"\n\n    paragraph: str\n    question: str\n</code></pre> <pre><code>$ python extract_answers.py v2\n$ python eval.py geology-squad.json geology-squad-answers-v2.json\n</code></pre> <pre><code>{\n  \"exact\": 85.34482758620689,\n  \"f1\": 91.55579573683022,\n  \"total\": 116,\n}\n</code></pre> <pre><code>$ mirascope add question\n</code></pre> <p>So just by updating our schema and prompt we've improved the performance of the extraction script by ~6% for exact match and ~1% for F1. We can see in Oxen that there are far fewer incorrect extracted answers:</p> <p></p> <p>It's also worth noting that we now have a record of both prompt versions and their answers. Going forward, it will be easy to iterate on the prompts and more reliably track which prompts perform the best.</p>"}]}