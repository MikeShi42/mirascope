{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mirascope","text":"<p> Prompt Engineering focused on: </p> Simplicity through idiomatic syntax \u2192 Faster and more reliable release Semi-opinionated methods \u2192 Reduced complexity that speeds up development Reliability through validation \u2192 More robust applications with fewer bugs High-quality up-to-date documentation \u2192 Trust and reliability <p> </p> <p>Documentation: https://docs.mirascope.io</p> <p>Source Code: https://github.com/Mirascope/mirascope</p> <p>Mirascope is a library purpose-built for Prompt Engineering on top of Pydantic 2.0:</p> <ul> <li>Prompts can live as self-contained classes in their own directory:</li> </ul> <pre><code># prompts/book_recommendation.py\nfrom mirascope import Prompt\n\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    I've recently read the following books: {titles_in_quotes}.\n\n    What should I read next?\n    \"\"\"\n\n    book_titles: list[str]\n\n    @property\n    def titles_in_quotes(self) -&gt; str:\n        \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n        return \", \".join([f'\"{title}\"' for title in self.book_titles])\n</code></pre> <ul> <li>Use the prompt anywhere without worrying about internals such as formatting:</li> </ul> <pre><code># script.py\nfrom prompts import BookRecommendationPrompt\n\nprompt = BookRecommendationPrompt(\n    book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n)\n\nprint(str(prompt))\n#&gt; I've recently read the following books: \"The Name of the Wind\", \"The Lord of the Rings\".\n#  What should I read next?\n\nprint(prompt.messages)\n#&gt; [('user', 'I\\'ve recently read the following books: \"The Name of the Wind\", \"The Lord of the Rings\".\\nWhat should I read next?')]\n</code></pre>"},{"location":"#why-use-mirascope","title":"Why use Mirascope?","text":"<ul> <li>Intuitive: Editor support that you expect (e.g. autocompletion, inline errors)</li> <li>Peace of Mind: Pydantic together with our Prompt CLI eliminate prompt-related bugs.</li> <li>Durable: Seamlessly customize and extend functionality. Never maintain a fork.</li> <li>Integration: Easily integrate with JSON Schema and other tools such as FastAPI</li> <li>Convenience: Tooling that is clean, elegant, and delightful that you don't need to maintain.</li> <li>Open: Dedication to building open-source tools you can use with your choice of LLM.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<p>Pydantic is the only strict requirement, which will be included automatically during installation.</p> <p>The Prompt CLI and LLM Convenience Wrappers have additional requirements, which you can opt-in to include if you're using those features.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install Mirascope and start building with LLMs in minutes.</p> <pre><code>$ pip install mirascope\n</code></pre> <p>This will install the <code>mirascope</code> package along with <code>pydantic</code>.</p> <p>To include extra dependencies, run:</p> <pre><code>$ pip install mirascope[cli]     #  Prompt CLI\n$ pip install mirascope[openai]  #  LLM Convenience Wrappers\n$ pip install mirascope[all]     #  All Extras\n</code></pre> For those using zsh, you'll need to escape brackets: <pre><code>$ pip install mirascope\\[all\\]\n</code></pre>"},{"location":"#warning-strong-opinion","title":"\ud83d\udea8 Warning: Strong Opinion \ud83d\udea8","text":"<p>Prompt Engineering is engineering. Beyond basic illustrative examples, prompting quickly becomes complex. Separating prompts from the engineering workflow will only put limitations on what you can build with LLMs. We firmly believe that prompts are far more than \"just f-strings\" and thus require developer tools that are purpose-built for building these more complex prompts as easily as possible.</p>"},{"location":"#examples","title":"Examples","text":"<p>The <code>Prompt</code> class is an extension of <code>BaseModel</code> with some additional built-in convenience tooling for writing and formatting your prompts:</p> <pre><code>from mirascope import Prompt\n\nclass GreetingsPrompt(Prompt):\n    \"\"\"\n    Hello! It's nice to meet you. My name is {name}. How are you today?\n    \"\"\"\n\n    name: str\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\n\nprint(GreetingsPrompt.template())\n#&gt; Hello! It's nice to meet you. My name is {name}. How are you today?\n\nprint(prompt)\n#&gt; Hello! It's nice to meet you. My name is William Bakst. How are you today?\n</code></pre> Example of autocomplete and inline errors: <ul> <li>Autocomplete:</li> <li>Inline Errors:</li> </ul> <p>You can access the docstring prompt template through the <code>GreetingsPrompt.template()</code> class method, which will automatically take care of removing any additional special characters such as newlines. This enables writing longer prompts that still adhere to the style of your codebase:</p> <pre><code>class GreetingsPrompt(Prompt):\n    \"\"\"\n    Salutations! It is a lovely day. Wouldn't you agree? I find that lovely days\n    such as these brighten up my mood quite a bit. I'm rambling...\n\n    My name is {name}. It's great to meet you.\n    \"\"\"\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\nprint(prompt)\n#&gt; Salutations, good being! It is a lovely day. Wouldn't you agree? I find that lovely days such as these brighten up my mood quite a bit. I'm rambling...\n#&gt; My name is William Bakst. It's great to meet you.\n</code></pre> <p>The <code>str</code> method is written such that it only formats properties that are templated. This enables writing more complex properties that rely on one or more provided properties:</p> <pre><code>class GreetingsPrompt(Prompt):\n    \"\"\"\n    Hi! My name is {formatted_name}. {name_specific_remark}\n\n    What's your name? Is your name also {name_specific_question}?\n    \"\"\"\n\n    name: str\n\n    @property\n    def formatted_name(self) -&gt; str:\n        \"\"\"Returns `name` with pizzazz.\"\"\"\n        return f\"\u2b50{self.name}\u2b50\"\n\n    @property\n    def name_specific_question(self) -&gt; str:\n        \"\"\"Returns a question based on `name`.\"\"\"\n        if self.name == self.name[::-1]:\n            return \"a palindrome\"\n        else:\n            return \"not a palindrome\"\n\n    @property\n    def name_specific_remark(self) -&gt; str:\n        \"\"\"Returns a remark based on `name`.\"\"\"\n        return f\"Can you believe my name is {self.name_specific_question}\"\n\nprompt = GreetingsPrompt(name=\"Bob\")\nprint(prompt)\n#&gt; Hi! My name is \u2b50Bob\u2b50. Can you believe my name is a palindrome?\n#&gt; What's your name? Is your name also a palindrome?\n</code></pre> <p>Notice that writing properties in this way ensures prompt-specific logic is tied directly to the prompt. It happens under the hood from the perspective of the person using <code>GreetingsPrompt</code> class. Constructing the prompt only requires <code>name</code>.</p> <p>For writing promps with multiple messages with different roles, you can use the <code>messages</code> decorator to extend the functionality of the <code>messages</code> property:</p> <pre><code>from mirascope import Prompt, messages\n\n@messages\nclass GreetingsPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You can only speak in haikus.\n\n    USER:\n    Hello! It's nice to meet you. My name is {name}. How are you today?\n    \"\"\"\n\n    name: str\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\n\nprint(GreetingsPrompt.template())\n#&gt; SYSTEM: You can only speak in haikus.\n#&gt; USER: Hello! It's nice to meet you. My name is {name}. How are you today?\n\nprint(prompt)\n#&gt; SYSTEM: You can only speak in haikus.\n#&gt; USER: Hello! It's nice to meet you. My name is William Bakst. How are you today?\n\nprint(prompt.messages)\n#&gt; [('system', 'You can only speak in haikus.'), ('user', \"Hello! It's nice to meet you. My name is William Bakst. How are you today?\")]\n</code></pre> <p>The base <code>Prompt</code> class without the decorator will still have the <code>messages</code> attribute, but it will return a single user message in the list.</p> Remember: this is python <p>There's nothing stopping you from doing things however you'd like. For example, reclaim the docstring:</p> <pre><code>from mirascope import Prompt\n\nTEMPLATE = \"\"\"\nThis is now my prompt template for {topic}\n\"\"\"\n\nclass NormalDocstringPrompt(Prompt):\n    \"\"\"This is now just a normal docstring.\"\"\"\n\n    topic: str\n\n    def template(self) -&gt; str:\n        \"\"\"Returns this prompt's template.\"\"\"\n        return TEMPLATE\n\nprompt = NormalDocstringPrompt(topic=\"prompts\")\nprint(prompt)\n#&gt; This is now my prompt template for prompt\n</code></pre> <p>Since the `Prompt`'s `str` method uses template, the above will work as expected.</p> <p>Because the <code>Prompt</code> class is built on top of <code>BaseModel</code>, prompts easily integrate with tools like FastAPI:</p> FastAPI Example <pre><code>from fastapi import FastAPI\nfrom mirascope import OpenAIChat\n\nfrom prompts import GreetingsPrompt\n\napp = FastAPI()\n\n\n@app.post(\"/greetings\")\ndef root(prompt: GreetingsPrompt) -&gt; str:\n    \"\"\"Returns an AI generated greeting.\"\"\"\n    model = OpenAIChat(api_key=os.environ[\"OPENAI_API_KEY\"])\n    return str(model.create(prompt))\n</code></pre> <p>You can also use the <code>Prompt</code> class with whichever LLM you want to use:</p> Mistral Example <pre><code>from mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\nfrom prompts import GreetingsPrompt\n\nclient = MistralClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\nmessages = [\n    ChatMessage(role=role, content=content)\n    for role, content in prompt.messages\n]\n\n# No streaming\nchat_response = client.chat(\n    model=\"mistral-tiny\",\n    messages=messages,\n)\n</code></pre> OpenAI Example <pre><code>from openai import OpenAI\n\nfrom prompts import GreetingsPrompt\n\nclient = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n\nprompt = GreetingsPrompt(name=\"William Bakst\")\nmessages = [\n    {\"role\": role, \"content\": content}\n    for role, content in prompt.messages\n]\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=messages,\n)\n</code></pre>"},{"location":"#dive-deeper","title":"Dive Deeper","text":"<ul> <li>Check out all of the possibilities of what you can do with Pydantic Prompts.</li> <li>Take a look at our Mirascope CLI for a semi-opinionated prompt mangement system.</li> <li>For convenience, we provide some wrappers around the OpenAI Python SDK for common tasks such as creation, streaming, tools, and extraction. We've found that the concepts covered in LLM Convenience Wrappers make building LLM-powered apps delightful.</li> <li>The API Reference contains full details on all classes, methods, functions, etc.</li> <li>You can take a look at code examples in the repo that demonstrate how to use the library effectively.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Mirascope welcomes contributions from the community! See the contribution guide for more information on the development workflow. For bugs and feature requests, visit our GitHub Issues and check out our templates.</p>"},{"location":"#how-to-help","title":"How To Help","text":"<p>Any and all help is greatly appreciated! Check out our page on how you can help.</p>"},{"location":"#roadmap-whats-on-our-mind","title":"Roadmap (What's on our mind)","text":"<ul> <li> Better DX for Mirascope CLI (e.g. autocomplete)</li> <li> Functions as OpenAI tools</li> <li> Better chat history</li> <li> Testing for prompts</li> <li> Logging prompts and their responses</li> <li> Evaluating prompt quality</li> <li> RAG</li> <li> Agents</li> </ul>"},{"location":"#versioning","title":"Versioning","text":"<p>Mirascope uses Semantic Versioning.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use poetry as our package and dependency manager.</p> <p>To create a virtual environment for development, run the following in your shell:</p> <pre><code>pip install poetry\npoetry shell\npoetry install --with dev\n</code></pre> <p>Simply use <code>exit</code> to deactivate the environment. The next time you call <code>poetry shell</code> the environment will already be setup and ready to go.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout main\ngit pull upstream main\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run mypy .\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR. Make sure to fill in a detailed title and description. Submit your PR for review.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>mypy</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>mypy</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>mypy</code> errors.</p>"},{"location":"HELP/","title":"How to help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li> <p>Follow us on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow William Bakst on Twitter/X</p> <ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with William Bakst on LinkedIn</p> <ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how Mirascope has helped you and how you are using it.</p> </li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues / Discussions.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"api/enums/","title":"enums","text":"<p>Enum Classes for mirascope.</p>"},{"location":"api/enums/#mirascope.enums.MirascopeCommand","title":"<code>MirascopeCommand</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>CLI commands to be executed.</p> Source code in <code>mirascope/enums.py</code> <pre><code>class MirascopeCommand(_Enum):\n    \"\"\"CLI commands to be executed.\"\"\"\n\n    ADD = \"add\"\n    USE = \"use\"\n    STATUS = \"status\"\n    INIT = \"init\"\n</code></pre>"},{"location":"api/prompts/","title":"prompts","text":"<p>A class for better prompting.</p>"},{"location":"api/prompts/#mirascope.prompts.Prompt","title":"<code>Prompt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A Pydantic model for prompts.</p> <p>Example:</p> <pre><code>from mirascope import Prompt, messages\n\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    I've recently read the following books: {titles_in_quotes}.\n    What should I read next?\n    \"\"\"\n\n    book_titles: list[str]\n\n    @property\n    def titles_in_quotes(self) -&gt; str:\n        \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n        return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\nprompt = BookRecommendationPrompt(\n    book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n)\n\nprint(BookRecommendationPrompt.template())\n#&gt; SYSTEM: You are the world's greatest librarian.\n#&gt; USER: I've recently read the following books: {titles_in_quotes}. What should I\n#  read next?\n\nprint(str(prompt))\n#&gt; SYSTEM: You are the world's greatest librarian.\n#&gt; USER: I've recently read the following books: \"The Name of the Wind\", \"The Lord\n#  of the Rings\". What should I read next?\n\nprompt.messages\n#&gt; [('system', \"You are the world's greatest librarian.\"), ('user', 'I've recently\n#   read the following books: \"The Name of the Wind\", \"The Lord of the Rings\". What\n#   should I read next?')]\n</code></pre> Source code in <code>mirascope/prompts.py</code> <pre><code>class Prompt(BaseModel):\n    '''A Pydantic model for prompts.\n\n    Example:\n\n    ```python\n    from mirascope import Prompt, messages\n\n\n    @messages\n    class BookRecommendationPrompt(Prompt):\n        \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        I've recently read the following books: {titles_in_quotes}.\n        What should I read next?\n        \"\"\"\n\n        book_titles: list[str]\n\n        @property\n        def titles_in_quotes(self) -&gt; str:\n            \"\"\"Returns a comma separated list of book titles each in quotes.\"\"\"\n            return \", \".join([f'\"{title}\"' for title in self.book_titles])\n\n\n    prompt = BookRecommendationPrompt(\n        book_titles=[\"The Name of the Wind\", \"The Lord of the Rings\"]\n    )\n\n    print(BookRecommendationPrompt.template())\n    #&gt; SYSTEM: You are the world's greatest librarian.\n    #&gt; USER: I've recently read the following books: {titles_in_quotes}. What should I\n    #  read next?\n\n    print(str(prompt))\n    #&gt; SYSTEM: You are the world's greatest librarian.\n    #&gt; USER: I've recently read the following books: \"The Name of the Wind\", \"The Lord\n    #  of the Rings\". What should I read next?\n\n    prompt.messages\n    #&gt; [('system', \"You are the world's greatest librarian.\"), ('user', 'I\\'ve recently\n    #   read the following books: \"The Name of the Wind\", \"The Lord of the Rings\". What\n    #   should I read next?')]\n    ```\n    '''\n\n    @classmethod\n    def template(cls) -&gt; str:\n        \"\"\"Custom parsing functionality for docstring prompt.\n\n        This function is the first step in formatting the prompt template docstring.\n        For the default `Prompt`, this function dedents the docstring and replaces all\n        repeated sequences of newlines with one fewer newline character. This enables\n        writing blocks of text instead of really long single lines. To include any\n        number of newline characters, simply include one extra.\n\n        Raises:\n            ValueError: If the class docstring is empty.\n        \"\"\"\n        if cls.__doc__ is None:\n            raise ValueError(\"`Prompt` must have a prompt template docstring.\")\n\n        return re.sub(\n            \"(\\n+)\",\n            lambda x: x.group(0)[:-1] if len(x.group(0)) &gt; 1 else \" \",\n            dedent(cls.__doc__).strip(\"\\n\"),\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the docstring prompt template formatted with template variables.\"\"\"\n        template = self.template()\n        template_vars = [\n            var for _, var, _, _ in Formatter().parse(template) if var is not None\n        ]\n        return template.format(**{var: getattr(self, var) for var in template_vars})\n\n    @property\n    def messages(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Returns the docstring as a list of messages.\"\"\"\n        return [(\"user\", str(self))]\n\n    def save(self, filepath: str):\n        \"\"\"Saves the prompt to the given filepath.\"\"\"\n        with open(filepath, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, filepath: str) -&gt; Prompt:\n        \"\"\"Loads the prompt from the given filepath.\"\"\"\n        with open(filepath, \"rb\") as f:\n            return pickle.load(f)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.messages","title":"<code>messages: list[tuple[str, str]]</code>  <code>property</code>","text":"<p>Returns the docstring as a list of messages.</p>"},{"location":"api/prompts/#mirascope.prompts.Prompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the docstring prompt template formatted with template variables.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the docstring prompt template formatted with template variables.\"\"\"\n    template = self.template()\n    template_vars = [\n        var for _, var, _, _ in Formatter().parse(template) if var is not None\n    ]\n    return template.format(**{var: getattr(self, var) for var in template_vars})\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Loads the prompt from the given filepath.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str) -&gt; Prompt:\n    \"\"\"Loads the prompt from the given filepath.\"\"\"\n    with open(filepath, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.save","title":"<code>save(filepath)</code>","text":"<p>Saves the prompt to the given filepath.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def save(self, filepath: str):\n    \"\"\"Saves the prompt to the given filepath.\"\"\"\n    with open(filepath, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.Prompt.template","title":"<code>template()</code>  <code>classmethod</code>","text":"<p>Custom parsing functionality for docstring prompt.</p> <p>This function is the first step in formatting the prompt template docstring. For the default <code>Prompt</code>, this function dedents the docstring and replaces all repeated sequences of newlines with one fewer newline character. This enables writing blocks of text instead of really long single lines. To include any number of newline characters, simply include one extra.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class docstring is empty.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>@classmethod\ndef template(cls) -&gt; str:\n    \"\"\"Custom parsing functionality for docstring prompt.\n\n    This function is the first step in formatting the prompt template docstring.\n    For the default `Prompt`, this function dedents the docstring and replaces all\n    repeated sequences of newlines with one fewer newline character. This enables\n    writing blocks of text instead of really long single lines. To include any\n    number of newline characters, simply include one extra.\n\n    Raises:\n        ValueError: If the class docstring is empty.\n    \"\"\"\n    if cls.__doc__ is None:\n        raise ValueError(\"`Prompt` must have a prompt template docstring.\")\n\n    return re.sub(\n        \"(\\n+)\",\n        lambda x: x.group(0)[:-1] if len(x.group(0)) &gt; 1 else \" \",\n        dedent(cls.__doc__).strip(\"\\n\"),\n    )\n</code></pre>"},{"location":"api/prompts/#mirascope.prompts.messages","title":"<code>messages(cls)</code>","text":"<p>A decorator for updating the <code>messages</code> class attribute of a <code>Prompt</code>.</p> <p>Adding this decorator to a <code>Prompt</code> updates the <code>messages</code> class attribute to parse the docstring as a list of messages. Each message is a tuple containing the role and the content. The docstring should have the following format:</p> <pre><code>&lt;role&gt;:\n&lt;content&gt;\n</code></pre> <p>For example, you might want to first include a system prompt followed by a user prompt, which you can structure as follows:</p> <pre><code>SYSTEM:\nThis would be the system message content.\n\nUSER:\nThis would be the user message content.\n</code></pre> <p>This decorator currently supports the SYSTEM, USER, and ASSISTANT roles.</p> <p>Returns:</p> Type Description <code>Type[T]</code> <p>The decorated class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the docstring is empty.</p> Source code in <code>mirascope/prompts.py</code> <pre><code>def messages(cls: Type[T]) -&gt; Type[T]:\n    \"\"\"A decorator for updating the `messages` class attribute of a `Prompt`.\n\n    Adding this decorator to a `Prompt` updates the `messages` class attribute\n    to parse the docstring as a list of messages. Each message is a tuple containing\n    the role and the content. The docstring should have the following format:\n\n        &lt;role&gt;:\n        &lt;content&gt;\n\n    For example, you might want to first include a system prompt followed by a user\n    prompt, which you can structure as follows:\n\n        SYSTEM:\n        This would be the system message content.\n\n        USER:\n        This would be the user message content.\n\n    This decorator currently supports the SYSTEM, USER, and ASSISTANT roles.\n\n    Returns:\n        The decorated class.\n\n    Raises:\n        ValueError: If the docstring is empty.\n    \"\"\"\n\n    def messages_fn(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Returns the docstring as a list of messages.\"\"\"\n        if self.__doc__ is None:\n            raise ValueError(\"`Prompt` must have a prompt template docstring.\")\n\n        return [\n            (match.group(1).lower(), match.group(2))\n            for match in re.finditer(\n                r\"(SYSTEM|USER|ASSISTANT|TOOL): \"\n                r\"((.|\\n)+?)(?=\\n(SYSTEM|USER|ASSISTANT|TOOL):|\\Z)\",\n                str(self),\n            )\n        ]\n\n    setattr(cls, \"messages\", property(messages_fn))\n    return cls\n</code></pre>"},{"location":"api/chat/","title":"chat","text":"<p>A module for interacting with Chat APIs.</p>"},{"location":"api/chat/models/","title":"chat.models","text":"<p>Classes for interactings with LLMs through Chat APIs.</p>"},{"location":"api/chat/tools/","title":"chat.tools","text":"<p>Classes for using tools with Chat APIs.</p>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A base class for more easily using tools with the OpenAI Chat client.</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>class OpenAITool(BaseModel):\n    \"\"\"A base class for more easily using tools with the OpenAI Chat client.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def fn(self) -&gt; Optional[Callable]:\n        \"\"\"Returns the function that the tool describes.\"\"\"\n        return None\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n        Returns:\n            The constructed `ChatCompletionToolParam` schema.\n\n        Raises:\n            ValueError: if the class doesn't have\n        \"\"\"\n        model_schema = cls.model_json_schema()\n        if \"description\" not in model_schema:\n            raise ValueError(\"Tool must have a docstring description.\")\n\n        fn = {\n            \"name\": model_schema[\"title\"],\n            \"description\": model_schema[\"description\"],\n        }\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = {\n                \"type\": \"object\",\n                \"properties\": {\n                    prop: {\n                        key: value\n                        for key, value in prop_schema.items()\n                        if key != \"default\" and key != \"title\"\n                    }\n                    for prop, prop_schema in model_schema[\"properties\"].items()\n                },\n                \"required\": model_schema[\"required\"]\n                if \"required\" in model_schema\n                else [],\n            }\n\n        return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Args:\n            tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        try:\n            model_json = json.loads(tool_call.function.arguments)\n        except JSONDecodeError as e:\n            raise ValueError() from e\n\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool.fn","title":"<code>fn: Optional[Callable]</code>  <code>property</code>","text":"<p>Returns the function that the tool describes.</p>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The <code>ChatCompletionMessageToolCall</code> to extract the tool from.</p> required <p>Returns:</p> Type Description <code>OpenAITool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Args:\n        tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    try:\n        model_json = json.loads(tool_call.function.arguments)\n    except JSONDecodeError as e:\n        raise ValueError() from e\n\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/chat/tools/#mirascope.chat.tools.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the OpenAI Chat client.</p> <p>Returns:</p> Type Description <code>ChatCompletionToolParam</code> <p>The constructed <code>ChatCompletionToolParam</code> schema.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the class doesn't have</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n    Returns:\n        The constructed `ChatCompletionToolParam` schema.\n\n    Raises:\n        ValueError: if the class doesn't have\n    \"\"\"\n    model_schema = cls.model_json_schema()\n    if \"description\" not in model_schema:\n        raise ValueError(\"Tool must have a docstring description.\")\n\n    fn = {\n        \"name\": model_schema[\"title\"],\n        \"description\": model_schema[\"description\"],\n    }\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = {\n            \"type\": \"object\",\n            \"properties\": {\n                prop: {\n                    key: value\n                    for key, value in prop_schema.items()\n                    if key != \"default\" and key != \"title\"\n                }\n                for prop, prop_schema in model_schema[\"properties\"].items()\n            },\n            \"required\": model_schema[\"required\"]\n            if \"required\" in model_schema\n            else [],\n        }\n\n    return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n</code></pre>"},{"location":"api/chat/tools/#mirascope.chat.tools.openai_tool_fn","title":"<code>openai_tool_fn(fn)</code>","text":"<p>A decorator for adding a function to a tool class.</p> <p>Adding this decorator will add an <code>fn</code> property to the tool class that returns the function that the tool describes. This is convenient for calling the function given an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to add to the tool class.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated tool class.</p> Source code in <code>mirascope/chat/tools.py</code> <pre><code>def openai_tool_fn(fn: Callable) -&gt; Callable:\n    \"\"\"A decorator for adding a function to a tool class.\n\n    Adding this decorator will add an `fn` property to the tool class that returns the\n    function that the tool describes. This is convenient for calling the function given\n    an instance of the tool.\n\n    Args:\n        fn: The function to add to the tool class.\n\n    Returns:\n        The decorated tool class.\n    \"\"\"\n\n    def decorator(cls: Type[T]) -&gt; Type[T]:\n        \"\"\"A decorator for adding a function to a tool class.\"\"\"\n        setattr(cls, \"fn\", property(lambda self: fn))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/chat/types/","title":"chat.types","text":"<p>Classes for responses when interacting with a Chat API.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion","title":"<code>OpenAIChatCompletion</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around chat completions.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>class OpenAIChatCompletion(BaseModel):\n    \"\"\"Convenience wrapper around chat completions.\"\"\"\n\n    completion: ChatCompletion\n    tool_types: Optional[list[Type[OpenAITool]]] = None\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.completion.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.completion.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.completion.choices[0].message\n\n    @property\n    def content(self) -&gt; Optional[str]:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.completion.choices[0].message.content\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChatCompletionMessageToolCall]]:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @property\n    def tools(self) -&gt; Optional[list[OpenAITool]]:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[OpenAITool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls or len(self.tool_calls) == 0:\n            return None\n\n        tool_call = self.tool_calls[0]\n        for tool_type in self.tool_types:\n            if self.tool_calls[0].function.name == tool_type.__name__:\n                return tool_type.from_tool_call(tool_call)\n\n        return None\n\n    def __str__(self):\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.content","title":"<code>content: Optional[str]</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.tool","title":"<code>tool: Optional[OpenAITool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.tool_calls","title":"<code>tool_calls: Optional[list[ChatCompletionMessageToolCall]]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.tools","title":"<code>tools: Optional[list[OpenAITool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletion.__str__","title":"<code>__str__()</code>","text":"<p>Returns the contained string content for the 0th choice.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>def __str__(self):\n    \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n    return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk","title":"<code>OpenAIChatCompletionChunk</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>class OpenAIChatCompletionChunk(BaseModel):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\"\"\"\n\n    chunk: ChatCompletionChunk\n    tool_types: Optional[list[Type[OpenAITool]]] = None\n\n    @property\n    def choices(self) -&gt; list[ChunkChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChunkChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        return self.choices[0].delta\n\n    @property\n    def content(self) -&gt; Optional[str]:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return self.delta.content\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChoiceDeltaToolCall]]:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first and last `list[ChoiceDeltaToolCall]` will be None indicating start\n        and end of stream respectively. The next `list[ChoiceDeltaToolCall]` will\n        contain the name of the tool and index and subsequent\n        `list[ChoiceDeltaToolCall]`s will contain the arguments which will be strings\n        that need to be concatenated with future `list[ChoiceDeltaToolCall]`s to form a\n        complete JSON tool calls.\n        \"\"\"\n        return self.delta.tool_calls\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.choice","title":"<code>choice: ChunkChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.choices","title":"<code>choices: list[ChunkChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.content","title":"<code>content: Optional[str]</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.delta","title":"<code>delta: ChoiceDelta</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.tool_calls","title":"<code>tool_calls: Optional[list[ChoiceDeltaToolCall]]</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first and last <code>list[ChoiceDeltaToolCall]</code> will be None indicating start and end of stream respectively. The next <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls.</p>"},{"location":"api/chat/types/#mirascope.chat.types.OpenAIChatCompletionChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the chunk content for the 0th choice.</p> Source code in <code>mirascope/chat/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n    return self.content if self.content is not None else \"\"\n</code></pre>"},{"location":"api/chat/utils/","title":"chat.utils","text":"<p>Utility functions for mirascope chat.</p>"},{"location":"api/chat/utils/#mirascope.chat.utils.convert_base_model_to_openai_tool","title":"<code>convert_base_model_to_openai_tool(schema)</code>","text":"<p>Converts a <code>BaseModel</code> schema to an <code>OpenAITool</code> instance.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def convert_base_model_to_openai_tool(schema: Type[BaseModel]) -&gt; Type[OpenAITool]:\n    \"\"\"Converts a `BaseModel` schema to an `OpenAITool` instance.\"\"\"\n    internal_doc = (\n        f\"An `{schema.__name__}` instance with all correctly typed parameters \"\n        \"extracted from the completion. Must include required parameters and may \"\n        \"exclude optional parameters unless present in the text.\"\n    )\n    field_definitions = {\n        field_name: (field_info.annotation, field_info)\n        for field_name, field_info in schema.model_fields.items()\n    }\n    return create_model(\n        f\"{schema.__name__}Tool\",\n        __base__=OpenAITool,\n        __doc__=schema.__doc__ if schema.__doc__ else internal_doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.convert_function_to_openai_tool","title":"<code>convert_function_to_openai_tool(fn)</code>","text":"<p>Constructs and <code>OpenAITool</code> type from the given function.</p> <p>If parameters are not defined in the Args section, then the description will simply be the name of the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to convert.</p> required <p>Returns:</p> Type Description <code>Type[OpenAITool]</code> <p>The constructed <code>OpenAITool</code> type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the given function doesn't have a docstring.</p> <code>ValueError</code> <p>if the given function's parameters don't have type annotations.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but the name doesn't match the docstring's parameter name.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but doesn't have a dosctring description.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def convert_function_to_openai_tool(fn: Callable) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs and `OpenAITool` type from the given function.\n\n    If parameters are not defined in the Args section, then the description will simply\n    be the name of the parameter.\n\n    Args:\n        fn: The function to convert.\n\n    Returns:\n        The constructed `OpenAITool` type.\n\n    Raises:\n        ValueError: if the given function doesn't have a docstring.\n        ValueError: if the given function's parameters don't have type annotations.\n        ValueError: if a given function's parameter is in the docstring args section but\n            the name doesn't match the docstring's parameter name.\n        ValueError: if a given function's parameter is in the docstring args section but\n            doesn't have a dosctring description.\n    \"\"\"\n    if not fn.__doc__:\n        raise ValueError(\"Function must have a docstring.\")\n\n    docstring = parse(fn.__doc__)\n\n    doc = \"\"\n    if docstring.short_description:\n        doc = docstring.short_description\n    if docstring.long_description:\n        doc += \"\\n\\n\" + docstring.long_description\n\n    field_definitions = {}\n    hints = get_type_hints(fn)\n    for i, parameter in enumerate(signature(fn).parameters.values()):\n        if parameter.name == \"self\" or parameter.name == \"cls\":\n            continue\n        if parameter.annotation == Parameter.empty:\n            raise ValueError(\"All parameters must have a type annotation.\")\n\n        docstring_description = None\n        if i &lt; len(docstring.params):\n            docstring_param = docstring.params[i]\n            if docstring_param.arg_name != parameter.name:\n                raise ValueError(\n                    f\"Function parameter name {parameter.name} does not match docstring \"\n                    f\"parameter name {docstring_param.arg_name}. Make sure that the \"\n                    \"parameter names match exactly.\"\n                )\n            if not docstring_param.description:\n                raise ValueError(\"All parameters must have a description.\")\n            docstring_description = docstring_param.description\n\n        field_info = FieldInfo(annotation=hints[parameter.name])\n        if parameter.default != Parameter.empty:\n            field_info.default = parameter.default\n        if docstring_description:  # we check falsy here because this comes from docstr\n            field_info.description = docstring_description\n\n        param_name = parameter.name\n        if param_name.startswith(\"model_\"):  # model_ is a BaseModel reserved namespace\n            param_name = \"aliased_\" + param_name\n            field_info.alias = parameter.name\n            field_info.validation_alias = parameter.name\n            field_info.serialization_alias = parameter.name\n\n        field_definitions[param_name] = (\n            hints[parameter.name],\n            field_info,\n        )\n\n    return create_model(\n        \"\".join(word.title() for word in fn.__name__.split(\"_\")),\n        __base__=openai_tool_fn(fn)(OpenAITool),\n        __doc__=doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.convert_tools_list_to_openai_tools","title":"<code>convert_tools_list_to_openai_tools(tools)</code>","text":"<p>Converts a list of <code>Callable</code> or <code>OpenAITool</code> instances to an <code>OpenAITool</code> list.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def convert_tools_list_to_openai_tools(\n    tools: Optional[list[Union[Callable, Type[OpenAITool]]]],\n) -&gt; Optional[list[Type[OpenAITool]]]:\n    \"\"\"Converts a list of `Callable` or `OpenAITool` instances to an `OpenAITool` list.\"\"\"\n    if not tools:\n        return None\n    return [\n        tool if isclass(tool) else convert_function_to_openai_tool(tool)\n        for tool in tools\n    ]\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.get_openai_messages_from_prompt","title":"<code>get_openai_messages_from_prompt(prompt)</code>","text":"<p>Returns a list of messages parsed from the prompt.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def get_openai_messages_from_prompt(\n    prompt: Union[Prompt, str],\n) -&gt; list[ChatCompletionMessageParam]:\n    \"\"\"Returns a list of messages parsed from the prompt.\"\"\"\n    if isinstance(prompt, Prompt):\n        return [\n            cast(ChatCompletionMessageParam, {\"role\": role, \"content\": content})\n            for role, content in prompt.messages\n        ]\n    else:\n        return [cast(ChatCompletionMessageParam, {\"role\": \"user\", \"content\": prompt})]\n</code></pre>"},{"location":"api/chat/utils/#mirascope.chat.utils.patch_openai_kwargs","title":"<code>patch_openai_kwargs(kwargs, prompt, tools)</code>","text":"<p>Sets up the kwargs for an OpenAI API call.</p> Source code in <code>mirascope/chat/utils.py</code> <pre><code>def patch_openai_kwargs(\n    kwargs: dict[str, Any],\n    prompt: Optional[Union[Prompt, str]],\n    tools: Optional[list[Type[OpenAITool]]],\n):\n    \"\"\"Sets up the kwargs for an OpenAI API call.\"\"\"\n    if prompt is None:\n        if \"messages\" not in kwargs:\n            raise ValueError(\"Either `prompt` or `messages` must be provided.\")\n    else:\n        kwargs[\"messages\"] = get_openai_messages_from_prompt(prompt)\n\n    if tools:\n        kwargs[\"tools\"] = [tool.tool_schema() for tool in tools]\n        if \"tool_choice\" not in kwargs:\n            kwargs[\"tool_choice\"] = \"auto\"\n</code></pre>"},{"location":"api/cli/","title":"cli","text":"<p>This module contains all functionality related to the Mirascope CLI.</p>"},{"location":"api/cli/commands/","title":"cli.commands","text":"<p>The Mirascope CLI prompt management tool.</p> <p>Typical usage example:</p> <pre><code>Initialize the environment:\n    $ mirascope init mirascope\n\nCreate a prompt in the prompts directory:\n    prompts/my_prompt.py\n\nAdd the prompt to create a version:\n    $ mirascope add my_prompt\n\nIterate on the prompt in the prompts directory:\n\nCheck the status of the prompt:\n    $ mirascope status my_prompt\n\nAdd the prompt to create a new version:\n    $ mirascope add my_prompt\n\nSwitch between prompts:\n    $ mirascope use my_prompt 0001\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.add","title":"<code>add(prompt_file_name=Argument(help='Prompt file to add', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name, default=''))</code>","text":"<p>Adds the given prompt to the specified version directory.</p> <p>The contents of the prompt in the user's prompts directory are copied to the version directory with the next revision number, and the version file is updated with the new revision.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_name</code> <code>str</code> <p>The name of the prompt file to add.</p> <code>Argument(help='Prompt file to add', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name, default='')</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the specified prompts directory.</p> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Add a prompt\")\ndef add(\n    prompt_file_name: str = Argument(\n        help=\"Prompt file to add\",\n        autocompletion=_prompts_directory_files,\n        parser=_parse_prompt_file_name,\n        default=\"\",\n    ),\n):\n    \"\"\"Adds the given prompt to the specified version directory.\n\n    The contents of the prompt in the user's prompts directory are copied to the version\n    directory with the next revision number, and the version file is updated with the\n    new revision.\n\n    Args:\n        prompt_file_name: The name of the prompt file to add.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the specified prompts directory.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n\n    # Check status before continuing\n    used_prompt_path = check_status(mirascope_settings, prompt_file_name)\n    if not used_prompt_path:\n        print(\"No changes detected.\")\n        return\n    class_directory = os.path.join(version_directory_path, prompt_file_name)\n\n    # Check if prompt file exists\n    if not os.path.exists(f\"{prompt_directory_path}/{prompt_file_name}.py\"):\n        raise FileNotFoundError(\n            f\"Prompt {prompt_file_name}.py not found in {prompt_directory_path}\"\n        )\n    # Create version directory if it doesn't exist\n    if not os.path.exists(class_directory):\n        os.makedirs(class_directory)\n    version_file_path = os.path.join(class_directory, version_file_name)\n    versions = get_prompt_versions(version_file_path)\n\n    # Open user's prompt file\n    with open(\n        f\"{prompt_directory_path}/{prompt_file_name}.py\", \"r+\", encoding=\"utf-8\"\n    ) as file:\n        # Increment revision id\n        if versions.latest_revision is None:\n            # first revision\n            revision_id = \"0001\"\n        else:\n            # default branch with incrementation\n            latest_revision_id = versions.latest_revision\n            revision_id = f\"{int(latest_revision_id)+1:04}\"\n        # Create revision file\n        revision_file = os.path.join(\n            class_directory, f\"{revision_id}_{prompt_file_name}.py\"\n        )\n        with open(\n            revision_file,\n            \"w+\",\n            encoding=\"utf-8\",\n        ) as file2:\n            custom_variables = {\n                \"prev_revision_id\": versions.current_revision,\n                \"revision_id\": revision_id,\n            }\n            file2.write(\n                write_prompt_to_template(\n                    file.read(), MirascopeCommand.ADD, custom_variables\n                )\n            )\n            keys_to_update = {\n                CURRENT_REVISION_KEY: revision_id,\n                LATEST_REVISION_KEY: revision_id,\n            }\n            update_version_text_file(version_file_path, keys_to_update)\n    if revision_file:\n        if mirascope_settings.format_command:\n            format_command: list[str] = mirascope_settings.format_command.split()\n            format_command.append(revision_file)\n            subprocess.run(\n                format_command,\n                check=True,\n                capture_output=True,\n            )\n    print(\n        \"Adding \"\n        f\"{version_directory_path}/{prompt_file_name}/{revision_id}_{prompt_file_name}.py\"\n    )\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.init","title":"<code>init(mirascope_location=Option(help='Main mirascope directory', default='mirascope'), prompts_location=Option(help='Location of prompts directory', default='prompts'))</code>","text":"<p>Initializes the mirascope project.</p> <p>Creates the project structure and files needed for mirascope to work.</p> <p>Initial project structure: <pre><code>|\n|-- mirascope.ini\n|-- mirascope\n|   |-- prompt_template.j2\n|   |-- versions/\n|   |   |-- &lt;directory_name&gt;/\n|   |   |   |-- version.txt\n|   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n|-- prompts/\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>mirascope_location</code> <code>str</code> <p>The root mirascope directory to create.</p> <code>Option(help='Main mirascope directory', default='mirascope')</code> <code>prompts_location</code> <code>str</code> <p>The user's prompts directory.</p> <code>Option(help='Location of prompts directory', default='prompts')</code> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Initialize mirascope project\")\ndef init(\n    mirascope_location: str = Option(\n        help=\"Main mirascope directory\", default=\"mirascope\"\n    ),\n    prompts_location: str = Option(\n        help=\"Location of prompts directory\", default=\"prompts\"\n    ),\n) -&gt; None:\n    \"\"\"Initializes the mirascope project.\n\n    Creates the project structure and files needed for mirascope to work.\n\n    Initial project structure:\n    ```\n    |\n    |-- mirascope.ini\n    |-- mirascope\n    |   |-- prompt_template.j2\n    |   |-- versions/\n    |   |   |-- &lt;directory_name&gt;/\n    |   |   |   |-- version.txt\n    |   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n    |-- prompts/\n    ```\n\n    Args:\n        mirascope_location: The root mirascope directory to create.\n        prompts_location: The user's prompts directory.\n    \"\"\"\n    destination_dir = Path.cwd()\n    versions_directory = os.path.join(mirascope_location, \"versions\")\n    os.makedirs(versions_directory, exist_ok=True)\n    print(f\"Creating {destination_dir}/{versions_directory}\")\n    os.makedirs(prompts_location, exist_ok=True)\n    print(f\"Creating {destination_dir}/{prompts_location}\")\n    prompts_init_file: Path = Path(f\"{destination_dir}/{prompts_location}/__init__.py\")\n    if not prompts_init_file.is_file():\n        prompts_init_file.touch()\n        print(f\"Creating {prompts_init_file}\")\n    # Create the 'mirascope.ini' file in the current directory with some default values\n    ini_settings = MirascopeSettings(\n        mirascope_location=mirascope_location,\n        versions_location=\"versions\",\n        prompts_location=prompts_location,\n        version_file_name=\"version.txt\",\n    )\n\n    # Get templates from the mirascope.cli.generic package\n    generic_file_path = files(\"mirascope.cli.generic\")\n    ini_path = generic_file_path.joinpath(\"mirascope.ini.j2\")\n    with open(str(ini_path), \"r\", encoding=\"utf-8\") as file:\n        template = Template(file.read())\n        rendered_content = template.render(ini_settings.model_dump())\n        destination_file_path = destination_dir / \"mirascope.ini\"\n        with open(destination_file_path, \"w\", encoding=\"utf-8\") as destination_file:\n            destination_file.write(rendered_content)\n            print(f\"Creating {destination_file_path}\")\n\n    # Create the 'prompt_template.j2' file in the mirascope directory specified by user\n    prompt_template_path = generic_file_path.joinpath(\"prompt_template.j2\")\n    with open(str(prompt_template_path), \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n    template_path = os.path.join(mirascope_location, \"prompt_template.j2\")\n    with open(template_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n        print(f\"Creating {destination_dir}/{template_path}\")\n\n    print(\"Initialization complete.\")\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.status","title":"<code>status(prompt_file_name=Argument(help='Prompt to check status on', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name))</code>","text":"<p>Checks the status of the current prompt or prompts.</p> <p>If a prompt is specified, the status of that prompt is checked. Otherwise, the status of all promps are checked. If a prompt has changed, the path to the prompt is printed.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_name</code> <code>Optional[str]</code> <p>(Optional) The name of the prompt file to check status on.</p> <code>Argument(help='Prompt to check status on', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name)</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the specified prompts directory.</p> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Check status of prompt(s)\")\ndef status(\n    prompt_file_name: Optional[str] = Argument(\n        help=\"Prompt to check status on\",\n        autocompletion=_prompts_directory_files,\n        parser=_parse_prompt_file_name,\n    ),\n) -&gt; None:\n    \"\"\"Checks the status of the current prompt or prompts.\n\n    If a prompt is specified, the status of that prompt is checked. Otherwise, the\n    status of all promps are checked. If a prompt has changed, the path to the prompt\n    is printed.\n\n    Args:\n        prompt_file_name: (Optional) The name of the prompt file to check status on.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the specified prompts directory.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    version_directory_path = mirascope_settings.versions_location\n\n    # If a prompt is specified, check the status of that prompt\n    if prompt_file_name:\n        used_prompt_path = check_status(mirascope_settings, prompt_file_name)\n        if used_prompt_path:\n            print(f\"Prompt {used_prompt_path} has changed.\")\n        else:\n            print(\"No changes detected.\")\n    else:  # Otherwise, check the status of all prompts\n        directores_changed: list[str] = []\n        for _, directories, _ in os.walk(version_directory_path):\n            for directory in directories:\n                used_prompt_path = check_status(mirascope_settings, directory)\n                if used_prompt_path:\n                    directores_changed.append(used_prompt_path)\n        if len(directores_changed) &gt; 0:\n            print(\"The following prompts have changed:\")\n            for prompt in directores_changed:\n                print(f\"\\t{prompt}\".expandtabs(4))\n        else:\n            print(\"No changes detected.\")\n</code></pre>"},{"location":"api/cli/commands/#mirascope.cli.commands.use","title":"<code>use(prompt_file_name=Argument(help='Prompt file to use', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name), version=Argument(help='Version of prompt to use'))</code>","text":"<p>Uses the version and prompt specified by the user.</p> <p>The contents of the prompt in the versions directory are copied to the user's prompts directory, based on the version specified by the user. The version file is updated with the new revision.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file_name</code> <code>str</code> <p>The name of the prompt file to use.</p> <code>Argument(help='Prompt file to use', autocompletion=_prompts_directory_files, parser=_parse_prompt_file_name)</code> <code>version</code> <code>str</code> <p>The version of the prompt file to use.</p> <code>Argument(help='Version of prompt to use')</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found in the versions directory.</p> Source code in <code>mirascope/cli/commands.py</code> <pre><code>@app.command(help=\"Use a prompt\")\ndef use(\n    prompt_file_name: str = Argument(\n        help=\"Prompt file to use\",\n        autocompletion=_prompts_directory_files,\n        parser=_parse_prompt_file_name,\n    ),\n    version: str = Argument(\n        help=\"Version of prompt to use\",\n    ),\n) -&gt; None:\n    \"\"\"Uses the version and prompt specified by the user.\n\n    The contents of the prompt in the versions directory are copied to the user's\n    prompts directory, based on the version specified by the user. The version file is\n    updated with the new revision.\n\n    Args:\n        prompt_file_name: The name of the prompt file to use.\n        version: The version of the prompt file to use.\n\n    Raises:\n        FileNotFoundError: If the file is not found in the versions directory.\n    \"\"\"\n    mirascope_settings = get_user_mirascope_settings()\n    used_prompt_path = check_status(mirascope_settings, prompt_file_name)\n    # Check status before continuing\n    if used_prompt_path:\n        print(\"Changes detected, please add or delete changes first.\")\n        print(f\"\\tmirascope add {prompt_file_name}\".expandtabs(4))\n        return\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n    class_directory = os.path.join(version_directory_path, prompt_file_name)\n    revision_file_path = find_prompt_path(class_directory, version)\n    version_file_path = os.path.join(class_directory, version_file_name)\n    if revision_file_path is None:\n        raise FileNotFoundError(\n            f\"Prompt version {version} not found in {class_directory}\"\n        )\n    # Open versioned prompt file\n    with open(revision_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n    # Write to user's prompt file\n    prompt_file_path = os.path.join(prompt_directory_path, f\"{prompt_file_name}.py\")\n    with open(prompt_file_path, \"w+\", encoding=\"utf-8\") as file2:\n        file2.write(write_prompt_to_template(content, MirascopeCommand.USE))\n    if prompt_file_path:\n        if mirascope_settings.format_command:\n            format_command: list[str] = mirascope_settings.format_command.split()\n            format_command.append(prompt_file_path)\n            subprocess.run(\n                format_command,\n                check=True,\n                capture_output=True,\n            )\n\n    # Update version file with new current revision\n    keys_to_update = {\n        CURRENT_REVISION_KEY: version,\n    }\n    update_version_text_file(version_file_path, keys_to_update)\n\n    print(f\"Using {revision_file_path}\")\n</code></pre>"},{"location":"api/cli/constants/","title":"cli.constants","text":"<p>Constants for Mirascope CLI.</p>"},{"location":"api/cli/generic/","title":"cli.generic","text":"<p>This package contains generic templates that are used to initialize a mirascope project.</p>"},{"location":"api/cli/schemas/","title":"cli.schemas","text":"<p>Contains the schema for files created by the mirascope cli.</p>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.MirascopeSettings","title":"<code>MirascopeSettings</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the user's mirascope settings.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class MirascopeSettings(BaseModel):\n    \"\"\"Model for the user's mirascope settings.\"\"\"\n\n    mirascope_location: str\n    versions_location: str\n    prompts_location: str\n    version_file_name: str\n    format_command: Optional[str] = None\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"api/cli/schemas/#mirascope.cli.schemas.VersionTextFile","title":"<code>VersionTextFile</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Model for the version text file.</p> Source code in <code>mirascope/cli/schemas.py</code> <pre><code>class VersionTextFile(BaseModel):\n    \"\"\"Model for the version text file.\"\"\"\n\n    current_revision: Optional[str] = Field(default=None)\n    latest_revision: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/cli/utils/","title":"cli.utils","text":"<p>Utility functions for the mirascope library.</p>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer","title":"<code>PromptAnalyzer</code>","text":"<p>             Bases: <code>NodeVisitor</code></p> <p>Utility class for analyzing a Mirascope prompt file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>class PromptAnalyzer(ast.NodeVisitor):\n    \"\"\"Utility class for analyzing a Mirascope prompt file.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the PromptAnalyzer.\"\"\"\n        self.imports = []\n        self.from_imports = []\n        self.variables = {}\n        self.classes = []\n        self.decorators = []\n        self.comments = \"\"\n\n    def visit_Import(self, node):\n        \"\"\"Extracts imports from the given node.\"\"\"\n        for alias in node.names:\n            self.imports.append(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        \"\"\"Extracts from imports from the given node.\"\"\"\n        for alias in node.names:\n            self.from_imports.append((node.module, alias.name))\n        self.generic_visit(node)\n\n    def visit_Assign(self, node):\n        \"\"\"Extracts variables from the given node.\"\"\"\n        target = node.targets[0]\n        if isinstance(target, ast.Name):\n            self.variables[target.id] = ast.unparse(node.value)\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        \"\"\"Extracts classes from the given node.\"\"\"\n        class_info = {\n            \"name\": node.name,\n            \"bases\": [ast.unparse(b) for b in node.bases],\n            \"body\": \"\",\n            \"decorators\": [ast.unparse(d) for d in node.decorator_list],\n            \"docstring\": None,\n        }\n\n        # Extract docstring if present\n        docstring = ast.get_docstring(node, False)\n        if docstring:\n            class_info[\"docstring\"] = docstring\n\n        # Handle the rest of the class body\n        body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n        class_info[\"body\"] = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n        self.classes.append(class_info)\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Extracts decorators from function definitions.\"\"\"\n        for decorator in node.decorator_list:\n            self.decorators.append(ast.unparse(decorator))\n        self.generic_visit(node)\n\n    def visit_Module(self, node):\n        \"\"\"Extracts comments from the given node.\"\"\"\n        comments = ast.get_docstring(node, False)\n        self.comments = \"\" if comments is None else comments\n        self.generic_visit(node)\n\n    def check_class_changed(self, other: \"PromptAnalyzer\") -&gt; bool:\n        \"\"\"Compares the classes of this file with the classes of another file.\"\"\"\n        self_classes = {c[\"name\"]: c for c in self.classes}\n        other_classes = {c[\"name\"]: c for c in other.classes}\n\n        all_class_names = set(self_classes.keys()) | set(other_classes.keys())\n\n        for name in all_class_names:\n            if name in self_classes and name in other_classes:\n                # Compare attributes of classes with the same name\n                class_diff = {\n                    attr: (self_classes[name][attr], other_classes[name][attr])\n                    for attr in self_classes[name]\n                    if self_classes[name][attr] != other_classes[name][attr]\n                }\n                if class_diff:\n                    return True\n            else:\n                return True\n\n        return False\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PromptAnalyzer.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the PromptAnalyzer.\"\"\"\n    self.imports = []\n    self.from_imports = []\n    self.variables = {}\n    self.classes = []\n    self.decorators = []\n    self.comments = \"\"\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.check_class_changed","title":"<code>check_class_changed(other)</code>","text":"<p>Compares the classes of this file with the classes of another file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_class_changed(self, other: \"PromptAnalyzer\") -&gt; bool:\n    \"\"\"Compares the classes of this file with the classes of another file.\"\"\"\n    self_classes = {c[\"name\"]: c for c in self.classes}\n    other_classes = {c[\"name\"]: c for c in other.classes}\n\n    all_class_names = set(self_classes.keys()) | set(other_classes.keys())\n\n    for name in all_class_names:\n        if name in self_classes and name in other_classes:\n            # Compare attributes of classes with the same name\n            class_diff = {\n                attr: (self_classes[name][attr], other_classes[name][attr])\n                for attr in self_classes[name]\n                if self_classes[name][attr] != other_classes[name][attr]\n            }\n            if class_diff:\n                return True\n        else:\n            return True\n\n    return False\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Assign","title":"<code>visit_Assign(node)</code>","text":"<p>Extracts variables from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Assign(self, node):\n    \"\"\"Extracts variables from the given node.\"\"\"\n    target = node.targets[0]\n    if isinstance(target, ast.Name):\n        self.variables[target.id] = ast.unparse(node.value)\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ClassDef","title":"<code>visit_ClassDef(node)</code>","text":"<p>Extracts classes from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ClassDef(self, node):\n    \"\"\"Extracts classes from the given node.\"\"\"\n    class_info = {\n        \"name\": node.name,\n        \"bases\": [ast.unparse(b) for b in node.bases],\n        \"body\": \"\",\n        \"decorators\": [ast.unparse(d) for d in node.decorator_list],\n        \"docstring\": None,\n    }\n\n    # Extract docstring if present\n    docstring = ast.get_docstring(node, False)\n    if docstring:\n        class_info[\"docstring\"] = docstring\n\n    # Handle the rest of the class body\n    body_nodes = [n for n in node.body if not isinstance(n, ast.Expr)]\n    class_info[\"body\"] = \"\\n\".join(ast.unparse(n) for n in body_nodes)\n\n    self.classes.append(class_info)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_FunctionDef","title":"<code>visit_FunctionDef(node)</code>","text":"<p>Extracts decorators from function definitions.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_FunctionDef(self, node):\n    \"\"\"Extracts decorators from function definitions.\"\"\"\n    for decorator in node.decorator_list:\n        self.decorators.append(ast.unparse(decorator))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Import","title":"<code>visit_Import(node)</code>","text":"<p>Extracts imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Import(self, node):\n    \"\"\"Extracts imports from the given node.\"\"\"\n    for alias in node.names:\n        self.imports.append(alias.name)\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_ImportFrom","title":"<code>visit_ImportFrom(node)</code>","text":"<p>Extracts from imports from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_ImportFrom(self, node):\n    \"\"\"Extracts from imports from the given node.\"\"\"\n    for alias in node.names:\n        self.from_imports.append((node.module, alias.name))\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.PromptAnalyzer.visit_Module","title":"<code>visit_Module(node)</code>","text":"<p>Extracts comments from the given node.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def visit_Module(self, node):\n    \"\"\"Extracts comments from the given node.\"\"\"\n    comments = ast.get_docstring(node, False)\n    self.comments = \"\" if comments is None else comments\n    self.generic_visit(node)\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_prompt_changed","title":"<code>check_prompt_changed(file1_path, file2_path)</code>","text":"<p>Checks if the given prompts have changed.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_prompt_changed(file1_path: Optional[str], file2_path: Optional[str]) -&gt; bool:\n    \"\"\"Checks if the given prompts have changed.\"\"\"\n    if file1_path is None or file2_path is None:\n        raise FileNotFoundError(\"Prompt or version file is missing.\")\n    # Parse the first file\n    try:\n        with open(file1_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file1_path} was not found.\") from e\n    analyzer1 = PromptAnalyzer()\n    tree1 = ast.parse(content)\n    analyzer1.visit(tree1)\n\n    # Parse the second file\n    try:\n        with open(file2_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"The file {file2_path} was not found.\") from e\n    analyzer2 = PromptAnalyzer()\n    tree2 = ast.parse(content)\n    analyzer2.visit(tree2)\n    # Compare the contents of the two files\n    differences = {\n        \"comments\": analyzer1.comments != analyzer2.comments,\n        \"imports_diff\": bool(set(analyzer1.imports) ^ set(analyzer2.imports)),\n        \"from_imports_diff\": bool(\n            set(analyzer1.from_imports) ^ set(analyzer2.from_imports)\n        ),\n        \"decorators_diff\": bool(set(analyzer1.decorators) ^ set(analyzer2.decorators)),\n        \"variables_diff\": set(analyzer1.variables.keys()) - ignore_variables\n        ^ set(analyzer2.variables.keys()) - ignore_variables,\n        \"classes_diff\": analyzer1.check_class_changed(analyzer2),\n        # Add other comparisons as needed\n    }\n    return any(differences.values())\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.check_status","title":"<code>check_status(mirascope_settings, directory)</code>","text":"<p>Checks the status of the given directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def check_status(\n    mirascope_settings: MirascopeSettings, directory: str\n) -&gt; Optional[str]:\n    \"\"\"Checks the status of the given directory.\"\"\"\n    version_directory_path = mirascope_settings.versions_location\n    prompt_directory_path = mirascope_settings.prompts_location\n    version_file_name = mirascope_settings.version_file_name\n    prompt_directory = os.path.join(version_directory_path, directory)\n    used_prompt_path = f\"{prompt_directory_path}/{directory}.py\"\n\n    # Get the currently used prompt version\n    versions = get_prompt_versions(f\"{prompt_directory}/{version_file_name}\")\n    if versions is None:\n        return used_prompt_path\n    current_head = versions.current_revision\n    if current_head is None:\n        return used_prompt_path\n    current_version_prompt_path = find_prompt_path(prompt_directory, current_head)\n\n    # Check if users prompt matches the current prompt version\n    has_file_changed = check_prompt_changed(\n        current_version_prompt_path, used_prompt_path\n    )\n    if has_file_changed:\n        return used_prompt_path\n    return None\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_file_names","title":"<code>find_file_names(directory, prefix='')</code>","text":"<p>Finds all files in a directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_file_names(directory: str, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"Finds all files in a directory.\"\"\"\n    pattern = f\"[!_]{prefix}*.py\"  # ignores private files\n    return glob.glob(pattern, root_dir=directory)  # Returns all files found\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.find_prompt_path","title":"<code>find_prompt_path(directory, prefix)</code>","text":"<p>Finds and opens the prompt with the given directory.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def find_prompt_path(directory: Union[Path, str], prefix: str) -&gt; Optional[str]:\n    \"\"\"Finds and opens the prompt with the given directory.\"\"\"\n    pattern = os.path.join(directory, prefix + \"*.py\")\n    prompt_files = glob.glob(pattern)\n\n    if not prompt_files:\n        return None  # No files found\n\n    # Return first file found\n    return prompt_files[0]\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_prompt_versions","title":"<code>get_prompt_versions(version_file_path)</code>","text":"<p>Returns the versions of the given prompt.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_prompt_versions(version_file_path: str) -&gt; VersionTextFile:\n    \"\"\"Returns the versions of the given prompt.\"\"\"\n    versions = VersionTextFile()\n    try:\n        with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n            file.seek(0)\n            for line in file:\n                # Check if the current line contains the key\n                if line.startswith(CURRENT_REVISION_KEY + \"=\"):\n                    versions.current_revision = line.split(\"=\")[1].strip()\n                elif line.startswith(LATEST_REVISION_KEY + \"=\"):\n                    versions.latest_revision = line.split(\"=\")[1].strip()\n            return versions\n    except FileNotFoundError:\n        return versions\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.get_user_mirascope_settings","title":"<code>get_user_mirascope_settings(ini_file_path='mirascope.ini')</code>","text":"<p>Returns the user's mirascope settings.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def get_user_mirascope_settings(\n    ini_file_path: str = \"mirascope.ini\",\n) -&gt; MirascopeSettings:\n    \"\"\"Returns the user's mirascope settings.\"\"\"\n    config = ConfigParser(allow_no_value=True)\n    try:\n        read_ok = config.read(ini_file_path)\n        if not read_ok:\n            raise FileNotFoundError(\n                \"The mirascope.ini file was not found. Please run \"\n                \"`mirascope init` to create one or run the mirascope CLI from the \"\n                \"same directory as the mirascope.ini file.\"\n            )\n        mirascope_config = config[\"mirascope\"]\n        return MirascopeSettings(**mirascope_config)\n    except KeyError as e:\n        raise KeyError(\n            \"The mirascope.ini file is missing the [mirascope] section.\"\n        ) from e\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.update_version_text_file","title":"<code>update_version_text_file(version_file, updates)</code>","text":"<p>Updates the version text file.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def update_version_text_file(version_file: str, updates: dict):\n    \"\"\"Updates the version text file.\"\"\"\n    try:\n        modified_lines = []\n        edits_made = {\n            key: False for key in updates\n        }  # Track which keys already exist in the file\n        version_file_path: Path = Path(version_file)\n        if not version_file_path.is_file():\n            version_file_path.touch()\n        # Read the file and apply updates\n        with open(version_file_path, \"r\", encoding=\"utf-8\") as file:\n            for line in file:\n                # Check if the current line contains any of the keys\n                for key, value in updates.items():\n                    if line.startswith(key + \"=\"):\n                        modified_lines.append(f\"{key}={value}\\n\")\n                        edits_made[key] = True\n                        break\n                else:\n                    # No key found, so keep the line as is\n                    modified_lines.append(line)\n\n            # Add any keys that were not found at the end of the file\n            for key, value in updates.items():\n                if not edits_made[key]:\n                    modified_lines.append(f\"{key}={value}\\n\")\n\n        # Write the modified content back to the file\n        with open(version_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.writelines(modified_lines)\n    except FileNotFoundError:\n        print(f\"The file {version_file} was not found.\")\n    except IOError as e:\n        print(f\"An I/O error occurred: {e}\")\n</code></pre>"},{"location":"api/cli/utils/#mirascope.cli.utils.write_prompt_to_template","title":"<code>write_prompt_to_template(file, command, variables=None)</code>","text":"<p>Writes the given prompt to the template.</p> Source code in <code>mirascope/cli/utils.py</code> <pre><code>def write_prompt_to_template(\n    file: str,\n    command: Literal[MirascopeCommand.ADD, MirascopeCommand.USE],\n    variables: Optional[dict] = None,\n):\n    \"\"\"Writes the given prompt to the template.\"\"\"\n    mirascope_directory = get_user_mirascope_settings().mirascope_location\n    if variables is None:\n        variables = {}\n    template_loader = FileSystemLoader(searchpath=mirascope_directory)\n    template_env = Environment(loader=template_loader)\n    template = template_env.get_template(\"prompt_template.j2\")\n    analyzer = PromptAnalyzer()\n    tree = ast.parse(file)\n    analyzer.visit(tree)\n    if command == MirascopeCommand.ADD:\n        new_variables = variables | analyzer.variables\n    else:  # command == MirascopeCommand.USE\n        variables = dict.fromkeys(ignore_variables, None)\n        new_variables = {\n            k: analyzer.variables[k] for k in analyzer.variables if k not in variables\n        }\n\n    data = {\n        \"comments\": analyzer.comments,\n        \"variables\": new_variables,\n        \"imports\": analyzer.imports,\n        \"from_imports\": analyzer.from_imports,\n        \"classes\": analyzer.classes,\n    }\n    return template.render(**data)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/","title":"LLM Convenience Wrappers","text":"<p>Mirascope provides convenience wrappers around the OpenAI client to make writing the code even more enjoyable. We purposefully pass <code>**kwargs</code> through our calls so that you always have direct access to their arguments.</p>"},{"location":"concepts/llm_convenience_wrappers/#why-should-you-care","title":"Why should you care?","text":"<ul> <li>Easy to learn<ul> <li>There's no magic here -- it's just python</li> <li>Chaining is no different from writing basic python functions</li> </ul> </li> <li>Convenient<ul> <li>You could do it yourself -- and you still can -- but there's just something nice about calling <code>str(res)</code> to get the response content</li> <li>You only need to pass in a <code>Prompt</code> and we'll handle the rest</li> </ul> </li> </ul>"},{"location":"concepts/llm_convenience_wrappers/#openaichat","title":"OpenAIChat","text":""},{"location":"concepts/llm_convenience_wrappers/#create","title":"Create","text":"<p>You can initialize an <code>OpenAIChat</code> instance and call <code>create</code> to generate an <code>OpenAIChatCompletion</code>:</p> <pre><code>from mirascope import OpenAIChat, Prompt\n\nclass RecipePrompt(Prompt):\n    \"\"\"\n    Recommend recipes that use {ingredient} as an ingredient\n    \"\"\"\n\n    ingredient: str\n\nchat = OpenAIChat(api_key=\"YOUR_OPENAI_API_KEY\")\ncompletion = chat.create(RecipePrompt(ingredient=\"apples\"))\nprint(completion)  # prints the string content of the completion\n</code></pre> <p>You can also pass a <code>str</code> in directly as your prompt if you'd prefer to use your own prompt tooling:</p> <pre><code>from mirascope import OpenAIChat\n\nchat = OpenAIChat(api_key=\"YOUR_OPENAI_API_KEY\")\ncompletion = chat.create(\"Recommend recipes that use apples.\")\nprint(completion)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#completion","title":"Completion","text":"<p>The <code>create</code> method returns an <code>OpenAIChatCompletion</code> class instance, which is a simple wrapper around the <code>ChatCompletion</code> class in <code>openai</code>. In fact, you can access everything from the original chunk as desired. The primary purpose of the class is to provide convenience.</p> <pre><code>from mirascope.chat.types import OpenAIChatCompletion\n\ncompletion = OpenAIChatCompletion(...)\n\ncompletion.completion  # ChatCompletion(...)\nstr(completion)        # original.choices[0].delta.content\ncompletion.choices     # original.choices\ncompletion.choice      # original.choices[0]\ncompletion.message     # original.choices[0].message\ncompletion.content     # original.choices[0].message.content\ncompletion.tool_calls  # original.choices[0].message.tool_calls\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#chaining","title":"Chaining","text":"<p>Adding a chain of calls is as simple as writing a function:</p> <pre><code>from mirascope import OpenAIChat, Prompt\n\nclass ChefPrompt(Prompt):\n    \"\"\"\n    Name the best chef in the world at cooking {food_type} food\n    \"\"\"\n\n    food_type: str\n\n\nclass RecipePrompt(Prompt):\n    \"\"\"\n    Recommend a recipe that uses {ingredient} as an ingredient\n    that chef {chef} would serve in their restuarant\n    \"\"\"\n\n    ingredient: str\n    chef: str\n\n\ndef recipe_by_chef_using(ingredient: str, food_type: str) -&gt; str:\n    \"\"\"Returns a recipe using `ingredient`.\n\n    The recipe will be generated based on what dish using `ingredient`\n    the best chef in the world at cooking `food_type` might serve in\n    their restaurant\n    \"\"\"\n    chat = OpenAIChat(api_key=\"YOUR_OPENAI_API_KEY\")\n    chef_prompt = ChefPrompt(food_type=food_type)\n    chef = str(chat.create(chef_prompt))\n    recipe_prompt = RecipePrompt(ingredient=ingredient, chef=chef)\n    return str(chat.create(recipe_prompt))\n\n\nrecipe = recipe_by_chef_using(\"apples\", \"japanese\")\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#streaming","title":"Streaming","text":"<p>You can use the <code>stream</code> method to stream a response. All this is doing is setting <code>stream=True</code> and providing the <code>OpenAIChatCompletionChunk</code> convenience wrappers around the response chunks.</p> <pre><code>chat = OpenAIChat()\nstream = chat.stream(prompt)\nfor chunk in stream:\n    print(str(chunk), end=\"\")\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#openaichatcompletionchunk","title":"OpenAIChatCompletionChunk","text":"<p>The <code>stream</code> method returns an <code>OpenAIChatCompletionChunk</code> instance, which is a convenience wrapper around the <code>ChatCompletionChunk</code> class in <code>openai</code></p> <pre><code>from mirascope.chat.types import OpenAIChatCompletionChunk\n\nchunk = OpenAIChatCompletionChunk(...)\n\nchunk.chunk    # ChatCompletionChunk(...)\nstr(chunk)     # original.choices[0].delta.content\nchunk.choices  # original.choices\nchunk.choice   # original.choices[0]\nchunk.delta    # original.choices[0].delta\nchunk.content  # original.choices[0].delta.content\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#extraction","title":"Extraction","text":"<p>Often you want to extract structured information into a format like JSON. The <code>extract</code> method makes this extremely easy by extracting the information into a Pydantic <code>BaseModel</code> schema that you define:</p> <pre><code>from mirascope import OpenAIChat\nfrom pydantic import BaseModel\n\nclass BookInfo(BaseModel):\n    \"\"\"Information about a book.\"\"\"\n\n    title: str\n    author: str\n\nbook_info = chat.extract(BookInfo, \"The Name of the Wind is by Patrick Rothfuss.\")\nassert isinstance(book_info, BookInfo)\nassert book_info.model_dump() == {\n    \"title\": \"The Name of the Wind\",\n    \"author\": \"Patrick Rothfuss\",\n}\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#retries","title":"Retries","text":"<p>Often you will want to retry your query in the event of a <code>ValidationError</code> when <code>extract</code> fails to convert the model response into the Pydantic model you've provided. Set the number of retries and <code>extract</code> will automatically retry up to that many times (by default <code>retries</code> is <code>0</code>):</p> <pre><code>book_info = chat.extract(\n    BookInfo,\n    \"The Name of the Wind is by Patrick Rothfuss.\",\n    retries=5,  # this will result in 6 total creation attempts if it never succeeds\n)\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#tools","title":"Tools","text":"<p>Tools are extremely useful when you want the model to intelligently choose to output the arguments to call one or more functions. With mirascope it is extremely easy to use tools. Any function properly documented with a docstring will be automatically converted into a tool. This means that you can use any such function as a tool with no additional work:</p> <pre><code>from typing import Literal\n\nfrom mirascope import OpenAIChat, Prompt\n\nclass CurrentWeatherPrompt(Prompt):\n    \"\"\"What's the weather like in Los Angeles?\"\"\"\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON string containing the location, temperature, and unit.\n    \"\"\"\n    return f\"{location} is 65 degrees {unit}.\"\n\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\")\ncompletion = chat.create(\n    CurrentWeatherPrompt(),\n    tools=[get_current_weather],  # pass in the function itself\n)\n\nfor tool in completion.tools or []:\n    print(tool)                      # this is a `GetCurrentWeather` instance\n    print(tool.fn(**tool.__dict__))  # this will call `get_current_weather`\n</code></pre> <p>This works by automatically converting the given function into an <code>OpenAITool</code> class. The <code>completion.tools</code> property then returns an actual instance of the tool.</p> <p>You can also define your own <code>OpenAITool</code> class. This is necessary when the function you want to use as a tool does not have a docstring. Additionally, the <code>OpenAITool</code> class makes it easy to further update the descriptions, which is useful when you want to further engineer your prompt:</p> <pre><code>from typing import Literal\n\nfrom mirascope import OpenAIChat, OpenAITool, Prompt, openai_tool_fn\nfrom pydantic import Field\n\nclass CurrentWeatherPrompt(Prompt):\n    \"\"\"What's the weather like in Los Angeles?\"\"\"\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f\"{location} is 65 degrees {unit}.\"\n\n@openai_tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\")\ncompletion = chat.create(\n    CurrentWeatherPrompt(),\n    tools=[GetCurrentWeather],  # pass in the tool class\n)\n\nfor tool in completion.tools or []:\n    print(tool)                      # this is a `GetCurrentWeather` instance\n    print(tool.fn(**tool.__dict__))  # this will call `get_current_weather`\n</code></pre> <p>Notice that using the <code>openai_tool_fn</code> decorator will attach the function defined by the tool to the tool for easier calling of the function. This happens automatically when using the function directly.</p> <p>However, attaching the function is not necessary. In fact, often there are times where the intention of using a tool is not to call a function but to extract information from text. In these cases there is no need to attach the function at all. Simply define the <code>OpenAITool</code> class without the attached function and access the extracted information through the arguments of the <code>completion.tools</code> instances:</p> <pre><code>from typing import Literal\n\nfrom mirascope import OpenAIChat, OpenAITool, Prompt, openai_tool_fn\nfrom pydantic import Field\n\nclass CurrentWeatherPrompt(Prompt):\n    \"\"\"What's the weather like in Los Angeles?\"\"\"\n\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nchat = OpenAIChat(model=\"gpt-3.5-turbo-1106\")\ncompletion = chat.create(\n    CurrentWeatherPrompt(),\n    tools=[GetCurrentWeather],  # pass in the tool class\n)\n\nfor tool in completion.tools or []:\n    print(tool)                      # this is a `GetCurrentWeather` instance\n</code></pre>"},{"location":"concepts/llm_convenience_wrappers/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to the Mirascope models. Here is a list in no order of things we are thinking about adding next: </p> <ul> <li>data extraction - extract information from text into a Pydantic model</li> <li>streaming tools - enable streaming when using tools</li> <li>additional models - support models beyond OpenAI, particularly OSS models</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"concepts/mirascope_cli/","title":"Mirascope CLI","text":"<p>One of the main frustrations of dealing with prompts is keeping track of all the various versions. Taking inspiration from alembic and git, the mirascope cli provides a couple of key pieces of functionality to make managing prompts easier.</p>"},{"location":"concepts/mirascope_cli/#the-prompt-management-environment","title":"The prompt management environment","text":"<p>The first step to using the Mirascope CLI is to use the <code>init</code> command in your project's root directory.</p> <pre><code>mirascope init\n</code></pre> <p>This will create the directories and files to help manage prompts. Here is a sample structure created by the <code>init</code> function: <pre><code>|\n|-- mirascope.ini\n|-- mirascope\n|   |-- prompt_template.j2\n|   |-- versions/\n|   |   |-- &lt;directory_name&gt;/\n|   |   |   |-- version.txt\n|   |   |   |-- &lt;revision_id&gt;_&lt;directory_name&gt;.py\n|-- prompts/\n</code></pre></p> <p>Here is a rundown of each directory and file:</p> <ul> <li><code>mirascope.ini</code> - The INI file that can be customized for your project</li> <li><code>mirascope</code> - The default name of the directory that is home to the prompt management environment</li> <li><code>prompt_template.j2</code> - The Jinja2 template file that is used to generate prompt versions</li> <li><code>versions</code> - The directory that holds the various prompt versions</li> <li><code>versions/&lt;directory_name</code> - The sub-directory that is created for each prompt file in the <code>prompts</code> directory</li> <li><code>version.txt</code> - A file system method of keeping track of current and latest revisions. Coming soon is revision tracking using a database instead</li> <li><code>&lt;revision_id&gt;_&lt;directory_name&gt;.py</code> - A prompt version that is created by the <code>mirascope add</code> command, more on this later.</li> <li><code>prompts</code> - The user's prompt directory that stores all prompt files</li> </ul> <p>The directory names can be changed anytime by modifying the <code>mirascope.ini</code> file or when running the <code>init</code> command.</p> <pre><code>mirascope init --mirascope_location my_mirascope --prompts_location my_prompts\n</code></pre>"},{"location":"concepts/mirascope_cli/#saving-your-first-prompt","title":"Saving your first prompt","text":"<p>After creating the prompt management directory, you are now ready to build and iterate on some prompts. Begin by adding a Mirascope Prompt to the prompts directory.</p> <pre><code># prompts/my_prompt.py\nfrom mirascope import Prompt\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n</code></pre> <p>Once you are happy with the first iteration of this prompt, you can run:</p> <pre><code>mirascope add my_prompt\n</code></pre> <p>This will commit <code>my_prompt.py</code> to your <code>versions/</code> directory, creating a <code>my_prompt</code> sub-directory and a <code>0001_my_prompt.py</code>.</p> <p>Here is what <code>0001_my_prompt.py</code> will look like:</p> <pre><code># versions/my_prompt/0001_my_prompt.py\nfrom mirascope import Prompt\n\nprev_revision_id = \"None\"\nrevision_id = \"0001\"\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n</code></pre> <p>The prompt inside the versions directory is almost identical to the prompt inside the prompts directory with a few differences.</p> <p>The variables <code>prev_revision_id</code> and <code>revision_id</code> will be used for features coming soon, so stay tuned for updates.</p>"},{"location":"concepts/mirascope_cli/#iterating-on-the-prompt","title":"Iterating on the prompt","text":"<p>Now that this version of <code>my_prompt</code> has been saved, you are now free to modify the original <code>my_prompt.py</code> and iterate. Maybe, you want to add a system message to obtain advice from a professional.</p> <p>Here is what the next iteration of <code>my_prompt.py</code> will look like:</p> <pre><code># prompts/my_prompt.py\nfrom mirascope.prompts import Prompt, messages\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are an expert in your field giving advice\n\n    USER:\n    Can you recommend some books on {topic} in a list format?\n    \"\"\"\n\n    topic: str\n</code></pre> <p>Before adding the next revision of <code>my_prompt</code>, you may want to check the status of your prompt.</p> <pre><code># You can specify a specific prompt\nmirascope status my_prompt\n\n# or, you can check the status of all prompts\nmirascope status\n</code></pre> <p>Note that status will be checked before the <code>add</code> or <code>use</code> command is run. Now we can run the same <code>add</code> command in the previous section to commit another version <code>0002_my_prompt.py</code></p>"},{"location":"concepts/mirascope_cli/#switching-between-versions","title":"Switching between versions","text":"<p>Often times when prompt engineering, you will want to try out different models with different prompts to obtain the best results.</p> <p>You can use the <code>use</code> command to quickly switch between the prompts:</p> <pre><code>mirascope use my_prompt 0001\n</code></pre> <p>Here you specify which prompt and also which version you want to use. This will update your <code>prompts/my_prompt.py</code> with the contents of <code>versions/0001_my_prompt.py</code> (minus the variables used internally).</p> <p>This will let you quickly swap prompts with no code change, the exception being when prompts have different properties.</p>"},{"location":"concepts/mirascope_cli/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to the Mirascope CLI. Here is a list in no order of things we are thinking about adding next: </p> <ul> <li>prompt comparison - A way to compare two different versions with a golden test</li> <li>remove - Remove a prompt</li> <li>history - View the revision history of a version</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"concepts/pydantic_prompts/","title":"Pydantic Prompts","text":"<p>The <code>Prompt</code> class is the core of Mirascope, which extends Pydantic's <code>BaseModel</code>. The class leverages the power of python to make writing more complex prompts as easy and readable as possible. The docstring is automatically formatted as a prompt so that you can write prompts in the style of your codebase.</p>"},{"location":"concepts/pydantic_prompts/#why-should-you-care","title":"Why should you care?","text":"<ul> <li>You get all of the benefits of using Pydantic:<ul> <li>type hints, json schema, customization, ecosystem, production-grade</li> </ul> </li> <li>Speeds up development<ul> <li>Fewer bugs through validation</li> <li>Auto-complete, editor (and linter) support for errors</li> </ul> </li> <li>Easy to learn         - You only need to learn Pydantic</li> <li>Standardization and compatibility<ul> <li>Integrations with other libraries that use JSON Schema such as OpenAPI and FastAPI means writing less code.</li> </ul> </li> <li>Customization<ul> <li>Everything is Pydantic or basic python, so changing anything is as easy as overriding what you want to change</li> </ul> </li> <li>All of the above helps lead to production ready code</li> </ul>"},{"location":"concepts/pydantic_prompts/#the-prompt-class","title":"The <code>Prompt</code> Class","text":"<p>The docstring of the class acts as the prompt's template, and the attributes act as the template variables:</p> <pre><code>from mirascope import Prompt\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nstr(prompt)\n</code></pre> <pre><code>Can you recommend some books on coding?\n</code></pre> <p>The <code>__str__</code> method, which formats all of the template variables, relies on the <code>template</code> function, which provides built-in string formatting so that you can write prettier docstrings. This means that longer prompts will still look well-formatted in your code base:</p> <pre><code>class LongerPrompt(Prompt):\n    \"\"\"\n    Longer prompts can be edited in a more organized format that looks\n    better in your code base. Any unwanted characters such as newlines\n    or tabs that are purely for text alignment and structure will be \n    automatically removed.\n\n    For newlines, just add one extra (e.g. 2 newlines -&gt; 1 newline here)\n\n        - The same goes for things you want indented\n    \"\"\"\n</code></pre> <p>Note</p> <p>If you want custom docstring formatting or none at all, simply override the <code>template</code> method.</p>"},{"location":"concepts/pydantic_prompts/#editor-support","title":"Editor Support","text":"<ul> <li>Inline Errors </li> <li>Autocomplete </li> </ul>"},{"location":"concepts/pydantic_prompts/#template-variables","title":"Template Variables","text":"<p>When you call <code>str(prompt)</code> the template will be formatted using the properties of the class that match the template variables. This means that you can define more complex properties through code. This is particularly useful when you want to inject template variables with custom formatting or template variables that depend on multiple attributes. </p> <pre><code>from mirascope import Prompt\n\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    Can you recommend some books on the following topic and genre pairs?\n\n        {topics_x_genres}\n    \"\"\"\n\n    topics: list[str]\n    genres: list[str]\n\n    @property\n    def topics_x_genres(self) -&gt; str:\n        \"\"\"Returns `topics` as a comma separated list.\"\"\"\n        return \"\\n\\t\".expandtabs(4).join(\n            [\n                f\"Topic: {topic}, Genre: {genre}\"\n                for topic in self.topics\n                for genre in self.genres\n            ]\n        )\n\nprompt = BookRecommendationPrompt(\n    topics=[\"coding\", \"music\"], genres=[\"fiction\", \"fantasy\"]\n)\nstr(prompt)\n</code></pre> <pre><code>Can you recommend some books on the following topic and genre pairs?\n    Topic: coding, Genre: fiction\n    Topic: coding, Genre: fantasy\n    Topic: music, Genre: fiction\n    Topic: music, Genre fantasy\n</code></pre>"},{"location":"concepts/pydantic_prompts/#messages","title":"Messages","text":"<p>By default, the <code>Prompt</code> class treats the prompt template as a single user message. If you want to specify a list of messages instead, we provide a decorator to make this easy:</p> <p>Note</p> <p><code>@messages</code> decorator adds <code>messages</code> property to the class</p> <pre><code>from mirascope import messages, Prompt\n\n@messages\nclass BookRecommendationPrompt(Prompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages)\n</code></pre> <pre><code>[(\"system\", \"You are the world's greatest librarian\"), (\"user\", \"Can you recommend some books on coding?\")]\n</code></pre>"},{"location":"concepts/pydantic_prompts/#future-updates","title":"Future updates","text":"<p>There is a lot more to be added to Mirascope prompts. Here is a list in no order of things we are thinking about adding next: </p> <ul> <li>more complex prompts - handle more complex prompts for things like history</li> <li>testing for prompts - test the quality of your prompt input/output</li> <li>prompt response tracking - track the input/output when using a prompt</li> </ul> <p>If you want some of these features implemented or if you think something is useful but not on this list, let us know!</p>"},{"location":"cookbook/basic_examples/","title":"Basic Examples","text":"<p>Note</p> <p>Take a look at the code in our repo.</p> <p>Full walkthrough with explanations coming soon...</p>"},{"location":"cookbook/rag/","title":"RAG (Retrieval-Augmented Generation)","text":"<p>Note</p> <p>Take a look at the code in our repo.</p> <p>Full walkthrough with explanations coming soon...</p>"}]}