{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mirascope","text":"<p>Mirascope is an LLM toolkit for lightning-fast, high-quality development. Building with Mirascope feels like writing the Python code you\u2019re already used to writing.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mirascope\n</code></pre> <p>You can also install additional optional dependencies if you\u2019re using those features:</p> <pre><code>pip install mirascope[anthropic]  # AnthropicCall, ...\npip install mirascope[gemini]     # GeminiCall, ...\npip install mirascope[wandb]      # WandbOpenAICall, ...\npip install mirascope[all]        # all optional dependencies\n</code></pre>"},{"location":"#examples","title":"Examples","text":""},{"location":"#colocation","title":"Colocation","text":"<p>Colocation is one of the core tenets of our philosophy. Everything that can impact the quality of a call to an LLM \u2014 from the prompt to the model to the temperature \u2014 must live together so that we can properly version and test the quality of our calls over time. This is useful since we have all of the information including metadata that we could want for analysis, which is particularly important during rapid development.</p> <pre><code>import os\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\n@tags([\"version:0003\"])\nclass Editor(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are a top class manga editor.\n\n    USER:\n    I'm working on a new storyline. What do you think?\n    {storyline}\n    \"\"\"\n\n    storyline: str\n\n    call_params = OpenAICallParams(model=\"gpt-4\", temperature=0.4)\n\n\nstoryline = \"...\"\neditor = Editor(storyline=storyline)\n\nprint(editor.messages())\n# &gt; [{'role': 'system', 'content': 'You are a top class manga editor.'}, {'role': 'user', 'content': \"I'm working on a new storyline. What do you think?\\n...\"}]\n\ncritique = editor.call()\nprint(critique.content)\n# &gt; I think the beginning starts off great, but...\n\nprint(editor.dump() | critique.dump())\n# {\n#     \"tags\": [\"version:0003\"],\n#     \"template\": \"SYSTEM:\\nYou are a top class manga editor.\\n\\nUSER:\\nI'm working on a new storyline. What do you think?\\n{storyline}\",\n#     \"inputs\": {\"storyline\": \"...\"},\n#     \"start_time\": 1710452778501.079,\n#     \"end_time\": 1710452779736.8418,\n#     \"output\": {\n#         \"id\": \"chatcmpl-92nBykcXyTpxwAbTEM5BOKp99fVmv\",\n#         \"choices\": [\n#             {\n#                 \"finish_reason\": \"stop\",\n#                 \"index\": 0,\n#                 \"logprobs\": None,\n#                 \"message\": {\n#                     \"content\": \"I think the beginning starts off great, but...\",\n#                     \"role\": \"assistant\",\n#                     \"function_call\": None,\n#                     \"tool_calls\": None,\n#                 },\n#             }\n#         ],\n#         \"created\": 1710452778,\n#         \"model\": \"gpt-4-0613\",\n#         \"object\": \"chat.completion\",\n#         \"system_fingerprint\": None,\n#         \"usage\": {\"completion_tokens\": 25, \"prompt_tokens\": 33, \"total_tokens\": 58},\n#     },\n# }\n</code></pre>"},{"location":"#chat-history","title":"Chat History","text":"<p>Our template parser makes inserting chat history beyond easy:</p> <pre><code>from openai.types.chat import ChatCompletionMessageParam\n\nfrom mirascope.openai import OpenAICall\n\n\nclass Librarian(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    MESSAGES: {history}\n    USER: {question}\n    \"\"\"\n\n    question: str\n    history: list[ChatCompletionMessageParam] = []\n\n\nlibrarian = Librarian(question=\"\", history=[])\nwhile True:\n    librarian.question = input(\"(User): \")\n    response = librarian.call()\n    librarian.history += [\n        {\"role\": \"user\", \"content\": librarian.question},\n        {\"role\": \"assistant\", \"content\": response.content},\n    ]\n    print(f\"(Assistant): {response.content}\")\n\n#&gt; (User): What fantasy book should I read?\n#&gt; (Assistant): Have you read the Name of the Wind?\n#&gt; (User): I have! What do you like about it?\n#&gt; (Assistant): I love the intricate world-building...\n</code></pre>"},{"location":"#tools-function-calling","title":"Tools (Function Calling)","text":"<p>We\u2019ve made implementing and using tools (function calling) intuitive:</p> <pre><code>from typing import Literal\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n):\n    \"\"\"Get the current weather in a given location.\"\"\"\n    if \"tokyo\" in location.lower():\n        print(f\"It is 10 degrees {unit} in Tokyo, Japan\")\n    elif \"san francisco\" in location.lower():\n        print(f\"It is 72 degrees {unit} in San Francisco, CA\")\n    elif \"paris\" in location.lower():\n        print(f\"It is 22 degress {unit} in Paris, France\")\n    else:\n        print(\"I'm not sure what the weather is like in {location}\")\n\n\nclass Forecast(OpenAICall):\n    prompt_template = \"What's the weather in Tokyo?\"\n\n    call_params = OpenAICallParams(model=\"gpt-4\", tools=[get_current_weather])\n\ntool = Forecast().call().tool\nif tool:\n    tool.fn(**tool.args)\n      #&gt; It is 10 degrees fahrenheit in Tokyo, Japan\n</code></pre>"},{"location":"#chain-of-thought-cot","title":"Chain of Thought (CoT)","text":"<p>Chaining multiple calls together for Chain of Thought (CoT) is as simple as writing a function:</p> <pre><code>import os\nfrom functools import cached_property\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass ChefSelector(OpenAICall):\n    prompt_template = \"Name a chef who is really good at cooking {food_type} food\"\n\n    food_type: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nclass RecipeRecommender(ChefSelector):\n    prompt_template = \"\"\"\n    SYSTEM:\n    Imagine that you are chef {chef}.\n    Your task is to recommend recipes that you, {chef}, would be excited to serve.\n\n    USER:\n    Recommend a {food_type} recipe using {ingredient}.\n    \"\"\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-4\")\n\n    @cached_property  # !!! so multiple access doesn't make multiple calls\n    def chef(self) -&gt; str:\n        \"\"\"Uses `ChefSelector` to select the chef based on the food type.\"\"\"\n        return ChefSelector(food_type=self.food_type).call().content\n\nresponse = RecipeRecommender(food_type=\"japanese\", ingredient=\"apples\").call()\nprint(response.content)\n# &gt; Certainly! Here's a recipe for a delicious and refreshing Japanese Apple Salad: ...\n</code></pre>"},{"location":"#extracting-structured-information","title":"Extracting Structured Information","text":"<p>Convenience built on top of tools that makes extracting structured information reliable:</p> <pre><code>from typing import Literal, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(TaskDetails)\n#&gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre>"},{"location":"#fastapi-integration","title":"FastAPI Integration","text":"<p>Since we\u2019ve built our <code>BasePrompt</code> on top of Pydantic, we integrate with tools like FastAPI out-of-the-box:</p> <pre><code>import os\nfrom typing import Type\n\nfrom fastapi import FastAPI\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\napp = FastAPI()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n\n@app.post(\"/\")\ndef root(book_recommender: BookRecommender) -&gt; Book:\n    \"\"\"Generates a book based on provided `genre`.\"\"\"\n    return book_recommender.extract()\n</code></pre>"},{"location":"#supported-providers-and-integrations","title":"Supported Providers and Integrations","text":"<p>You can find a list of supported providers with examples of how to use them with Mirascope.</p> <p>We are constantly working to further integrate Mirascope as seamlessly as possible with as many tools as possible. You can find the integrations that we currently support in our docs. If there are any integrations that you want, let us know!</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Agents<ul> <li> Easy tool calling and execution</li> <li> More convenience around TOOL messages</li> <li> Base classes for ReAct agents</li> <li> Base classes for Query Planning agents</li> <li> Tons of examples...</li> </ul> </li> <li> RAG<ul> <li> ChromaDB</li> <li> Pinecone</li> <li> OpenAI Embeddings</li> <li> Cohere Embeddings</li> <li> Hugging Face</li> <li> Tons of examples...</li> </ul> </li> <li> Mirascope CLI<ul> <li> Versioning prompts / calls / extractors</li> <li> RAG CLI (e.g. versioning stores, one-off vector store interactions)</li> <li> Versioning integrations with LLMOps tools (e.g. Weave, LangSmith, ...)</li> <li> LLM Provider Auto-conversion</li> <li> Templates (<code>mirascope from_template pinecone_rag_openai_call my_call_name</code>)</li> </ul> </li> <li> Extracting structured information using LLMs</li> <li> Streaming extraction for tools (function calling)</li> <li> Additional template parsing for more complex messages<ul> <li> Chat History</li> <li> List + List[List] Convenience</li> <li> Additional Metadata</li> <li> Vision</li> </ul> </li> <li> Support for more LLM providers:<ul> <li> Anthropic</li> <li> Cohere</li> <li> Mistral</li> <li> Groq</li> <li> Gemini</li> <li> HuggingFace</li> </ul> </li> <li> Integrations<ul> <li> Weights &amp; Biases Trace</li> <li> Weave by Weights &amp; Biases</li> <li> LangChain / LangSmith</li> <li> \u2026 tell us what you\u2019d like integrated!</li> </ul> </li> <li> Evaluating prompts and their quality by version</li> </ul>"},{"location":"#versioning","title":"Versioning","text":"<p>Mirascope uses\u00a0Semantic Versioning.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the\u00a0MIT License.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use poetry as our package and dependency manager.</p> <p>To create a virtual environment for development, run the following in your shell:</p> <pre><code>pip install poetry\npoetry shell\npoetry install --with dev\n</code></pre> <p>Simply use <code>exit</code> to deactivate the environment. The next time you call <code>poetry shell</code> the environment will already be setup and ready to go.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout dev\ngit pull upstream dev\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run mypy .\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR. Make sure to fill in a detailed title and description. Submit your PR for review.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p> <p>Furthermore, we have a full coverage requirement, so all incoming code must have 100% coverage. This policy ensures that every line of code is run in our tests. However, while achieving full coverage is essential, it is not sufficient on its own. Coverage metrics ensure code execution but do not guarantee correctness under all conditions. Make sure to stress test beyond coverage to reduce bugs.</p> <p>We use a Codecov dashboard to monitor and track our coverage.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>mypy</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>mypy</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>mypy</code> errors.</p>"},{"location":"HELP/","title":"How to help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li> <p>Follow us on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow William Bakst on Twitter/X</p> <ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with William Bakst on LinkedIn</p> <ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how Mirascope has helped you and how you are using it.</p> </li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues / Discussions.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"api/","title":"mirascope api","text":"<p>mirascope package.</p>"},{"location":"api/enums/","title":"enums","text":"<p>Enum Classes for mirascope.</p>"},{"location":"api/enums/#mirascope.enums.MessageRole","title":"<code>MessageRole</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Roles that the <code>BasePrompt</code> messages parser can parse from the template.</p> <p>SYSTEM: A system message. USER: A user message. ASSISTANT: A message response from the assistant or chat client. MODEL: A message response from the assistant or chat client. Model is used by     Google's Gemini instead of assistant, which doesn't have system messages. CHATBOT: A message response from the chat client. Chatbot is used by Cohere instead     of assistant. TOOL: A message representing the output of calling a tool.</p> Source code in <code>mirascope/enums.py</code> <pre><code>class MessageRole(_Enum):\n    \"\"\"Roles that the `BasePrompt` messages parser can parse from the template.\n\n    SYSTEM: A system message.\n    USER: A user message.\n    ASSISTANT: A message response from the assistant or chat client.\n    MODEL: A message response from the assistant or chat client. Model is used by\n        Google's Gemini instead of assistant, which doesn't have system messages.\n    CHATBOT: A message response from the chat client. Chatbot is used by Cohere instead\n        of assistant.\n    TOOL: A message representing the output of calling a tool.\n    \"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    MODEL = \"model\"\n    CHATBOT = \"chatbot\"\n    TOOL = \"tool\"\n</code></pre>"},{"location":"api/anthropic/","title":"anthropic","text":"<p>A module for interacting with Anthropic models.</p>"},{"location":"api/anthropic/calls/","title":"anthropic.calls","text":"<p>A module for calling Anthropic's Claude API.</p>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall","title":"<code>AnthropicCall</code>","text":"<p>             Bases: <code>BaseCall[AnthropicCallResponse, AnthropicCallResponseChunk, AnthropicTool]</code></p> <p>A base class for calling Anthropic's Claude models.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; There are many great books to read, it ultimately depends...\n</code></pre> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>class AnthropicCall(\n    BaseCall[AnthropicCallResponse, AnthropicCallResponseChunk, AnthropicTool]\n):\n    \"\"\"A base class for calling Anthropic's Claude models.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend a {genre} book.\"\n\n        genre: str\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; There are many great books to read, it ultimately depends...\n    ```\n    \"\"\"\n\n    call_params: ClassVar[AnthropicCallParams] = AnthropicCallParams()\n\n    def messages(self) -&gt; list[MessageParam]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        return self._parse_messages(\n            [MessageRole.SYSTEM, MessageRole.USER, MessageRole.ASSISTANT]\n        )  # type: ignore\n\n    def call(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n        \"\"\"Makes a call to the model using this `AnthropicCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `AnthropicCallResponse` instance.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        create = client.messages.create\n        if tool_types:\n            create = client.beta.tools.messages.create  # type: ignore\n        if self.call_params.weave is not None:\n            create = self.call_params.weave(create)  # pragma: no cover\n        start_time = datetime.datetime.now().timestamp() * 1000\n        message = create(\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n        return AnthropicCallResponse(\n            response=message,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=anthropic_api_calculate_cost(message.usage, message.model),\n            response_format=self.call_params.response_format,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `AnthropicCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `AnthropicCallResponse` instance.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        create = client.messages.create\n        if tool_types:\n            create = client.beta.tools.messages.create  # type: ignore\n        if self.call_params.weave is not None:\n            create = self.call_params.weave(create)  # pragma: no cover\n        start_time = datetime.datetime.now().timestamp() * 1000\n        message = await create(\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n        return AnthropicCallResponse(\n            response=message,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=anthropic_api_calculate_cost(message.usage, message.model),\n            response_format=self.call_params.response_format,\n        )\n\n    def stream(\n        self, **kwargs: Any\n    ) -&gt; Generator[AnthropicCallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `AnthropicCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            An `AnthropicCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        with client.messages.stream(messages=messages, **kwargs) as stream:\n            for chunk in stream:\n                yield AnthropicCallResponseChunk(\n                    chunk=chunk,\n                    tool_types=tool_types,\n                    response_format=self.call_params.response_format,\n                )\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[AnthropicCallResponseChunk, None]:\n        \"\"\"Streams the response for an asynchronous call using this `AnthropicCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            An `AnthropicCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n        client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        async with client.messages.stream(messages=messages, **kwargs) as stream:\n            async for chunk in stream:\n                yield AnthropicCallResponseChunk(\n                    chunk=chunk,\n                    tool_types=tool_types,\n                    response_format=self.call_params.response_format,\n                )\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _setup_anthropic_kwargs(\n        self,\n        kwargs: dict[str, Any],\n    ) -&gt; tuple[\n        list[MessageParam],\n        dict[str, Any],\n        Optional[list[Type[AnthropicTool]]],\n    ]:\n        \"\"\"Overrides the `BaseCall._setup` for Anthropic specific setup.\"\"\"\n        kwargs, tool_types = self._setup(kwargs, AnthropicTool)\n        messages = self.messages()\n        system_message = \"\"\n        if \"system\" in kwargs and kwargs[\"system\"] is not None:\n            system_message += f'{kwargs.pop(\"system\")}'\n        if messages[0][\"role\"] == \"system\":\n            if system_message:\n                system_message += \"\\n\"\n            system_message += messages.pop(0)[\"content\"]\n        if self.call_params.response_format == \"json\":\n            if system_message:\n                system_message += \"\\n\\n\"\n            system_message += \"Response format: JSON.\"\n            messages.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": \"Here is the JSON requested with only the fields \"\n                    \"defined in the schema you provided:\\n{\",\n                }\n            )\n            if \"tools\" in kwargs:\n                tools = kwargs.pop(\"tools\")\n                messages[-1][\"content\"] = (\n                    \"For each JSON you output, output ONLY the fields defined by these \"\n                    \"schemas. Include a `tool_name` field that EXACTLY MATCHES the \"\n                    \"tool name found in the schema matching this tool:\"\n                    \"\\n{schemas}\\n{json_msg}\".format(\n                        schemas=\"\\n\\n\".join([str(tool) for tool in tools]),\n                        json_msg=messages[-1][\"content\"],\n                    )\n                )\n        if system_message:\n            kwargs[\"system\"] = system_message\n\n        return messages, kwargs, tool_types\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>AnthropicCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnthropicCallResponse</code> <p>A <code>AnthropicCallResponse</code> instance.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n    \"\"\"Makes a call to the model using this `AnthropicCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `AnthropicCallResponse` instance.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    create = client.messages.create\n    if tool_types:\n        create = client.beta.tools.messages.create  # type: ignore\n    if self.call_params.weave is not None:\n        create = self.call_params.weave(create)  # pragma: no cover\n    start_time = datetime.datetime.now().timestamp() * 1000\n    message = create(\n        messages=messages,\n        stream=False,\n        **kwargs,\n    )\n    return AnthropicCallResponse(\n        response=message,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=anthropic_api_calculate_cost(message.usage, message.model),\n        response_format=self.call_params.response_format,\n    )\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>AnthropicCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnthropicCallResponse</code> <p>A <code>AnthropicCallResponse</code> instance.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; AnthropicCallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `AnthropicCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `AnthropicCallResponse` instance.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    create = client.messages.create\n    if tool_types:\n        create = client.beta.tools.messages.create  # type: ignore\n    if self.call_params.weave is not None:\n        create = self.call_params.weave(create)  # pragma: no cover\n    start_time = datetime.datetime.now().timestamp() * 1000\n    message = await create(\n        messages=messages,\n        stream=False,\n        **kwargs,\n    )\n    return AnthropicCallResponse(\n        response=message,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=anthropic_api_calculate_cost(message.usage, message.model),\n        response_format=self.call_params.response_format,\n    )\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>def messages(self) -&gt; list[MessageParam]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    return self._parse_messages(\n        [MessageRole.SYSTEM, MessageRole.USER, MessageRole.ASSISTANT]\n    )  # type: ignore\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>AnthropicCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AnthropicCallResponseChunk</code> <p>An <code>AnthropicCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>def stream(\n    self, **kwargs: Any\n) -&gt; Generator[AnthropicCallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `AnthropicCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        An `AnthropicCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = Anthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    with client.messages.stream(messages=messages, **kwargs) as stream:\n        for chunk in stream:\n            yield AnthropicCallResponseChunk(\n                chunk=chunk,\n                tool_types=tool_types,\n                response_format=self.call_params.response_format,\n            )\n</code></pre>"},{"location":"api/anthropic/calls/#mirascope.anthropic.calls.AnthropicCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response for an asynchronous call using this <code>AnthropicCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[AnthropicCallResponseChunk, None]</code> <p>An <code>AnthropicCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/anthropic/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[AnthropicCallResponseChunk, None]:\n    \"\"\"Streams the response for an asynchronous call using this `AnthropicCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        An `AnthropicCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    messages, kwargs, tool_types = self._setup_anthropic_kwargs(kwargs)\n    client = AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    async with client.messages.stream(messages=messages, **kwargs) as stream:\n        async for chunk in stream:\n            yield AnthropicCallResponseChunk(\n                chunk=chunk,\n                tool_types=tool_types,\n                response_format=self.call_params.response_format,\n            )\n</code></pre>"},{"location":"api/anthropic/extractors/","title":"anthropic.extractors","text":"<p>A class for extracting structured information using Anthropic Claude models.</p>"},{"location":"api/anthropic/extractors/#mirascope.anthropic.extractors.AnthropicExtractor","title":"<code>AnthropicExtractor</code>","text":"<p>             Bases: <code>BaseExtractor[AnthropicCall, AnthropicTool, Any, T]</code>, <code>Generic[T]</code></p> <p>A class for extracting structured information using Anthropic Claude models.</p> <p>Example:</p> <pre><code>from typing import Literal, Type\n\nfrom mirascope.anthropic import AnthropicExtractor\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\n\nclass TaskExtractor(AnthropicExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n\n    prompt_template = \"\"\"\n    Please extract the task details:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask_description = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask = TaskExtractor(task=task_description).extract(retries=3)\nassert isinstance(task, TaskDetails)\nprint(task)\n#&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n</code></pre> Source code in <code>mirascope/anthropic/extractors.py</code> <pre><code>class AnthropicExtractor(\n    BaseExtractor[AnthropicCall, AnthropicTool, Any, T], Generic[T]\n):\n    '''A class for extracting structured information using Anthropic Claude models.\n\n    Example:\n\n    ```python\n    from typing import Literal, Type\n\n    from mirascope.anthropic import AnthropicExtractor\n    from pydantic import BaseModel\n\n\n    class TaskDetails(BaseModel):\n        title: str\n        priority: Literal[\"low\", \"normal\", \"high\"]\n        due_date: str\n\n\n    class TaskExtractor(AnthropicExtractor[TaskDetails]):\n        extract_schema: Type[TaskDetails] = TaskDetails\n\n        prompt_template = \"\"\"\n        Please extract the task details:\n        {task}\n        \"\"\"\n\n        task: str\n\n\n    task_description = \"Submit quarterly report by next Friday. Task is high priority.\"\n    task = TaskExtractor(task=task_description).extract(retries=3)\n    assert isinstance(task, TaskDetails)\n    print(task)\n    #&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n    ```\n    '''\n\n    call_params: ClassVar[AnthropicCallParams] = AnthropicCallParams()\n\n    def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Extracts `extract_schema` from the Anthropic call response.\n\n        The `extract_schema` is converted into an `AnthropicTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Anthropics's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs[\"system\"] = EXTRACT_SYSTEM_MESSAGE\n        return self._extract(AnthropicCall, AnthropicTool, retries, **kwargs)\n\n    async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Asynchronously extracts `extract_schema` from the Anthropic call response.\n\n        The `extract_schema` is converted into an `AnthropicTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Anthropic's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs[\"system\"] = EXTRACT_SYSTEM_MESSAGE\n        return await self._extract_async(\n            AnthropicCall, AnthropicTool, retries, **kwargs\n        )\n\n    def stream(self, retries: int = 0, **kwargs: Any) -&gt; Generator[T, None, None]:\n        \"\"\"Streams partial instances of `extract_schema` as the schema is streamed.\n\n        The `extract_schema` is converted into a `partial(AnthropicTool)`, which allows\n        for any field (i.e.function argument) in the tool to be `None`. This allows us\n        to stream partial results as we construct the tool from the streamed chunks.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword argument parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            The partial `extract_schema` instance from the current buffer.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        yield from self._stream(\n            AnthropicCall, AnthropicTool, AnthropicToolStream, retries, **kwargs\n        )\n\n    async def stream_async(\n        self, retries: int = 0, **kwargs: Any\n    ) -&gt; AsyncGenerator[T, None]:\n        \"\"\"Asynchronously streams partial instances of `extract_schema` as streamed.\n\n        The `extract_schema` is converted into a `partial(AnthropicTool)`, which allows\n        for any field (i.e.function argument) in the tool to be `None`. This allows us\n        to stream partial results as we construct the tool from the streamed chunks.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            The partial `extract_schema` instance from the current buffer.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        async for partial_tool in self._stream_async(\n            AnthropicCall, AnthropicTool, AnthropicToolStream, retries, **kwargs\n        ):\n            yield partial_tool\n</code></pre>"},{"location":"api/anthropic/extractors/#mirascope.anthropic.extractors.AnthropicExtractor.extract","title":"<code>extract(retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the Anthropic call response.</p> <p>The <code>extract_schema</code> is converted into an <code>AnthropicTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Anthropics's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> Source code in <code>mirascope/anthropic/extractors.py</code> <pre><code>def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Extracts `extract_schema` from the Anthropic call response.\n\n    The `extract_schema` is converted into an `AnthropicTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Anthropics's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n    \"\"\"\n    kwargs[\"system\"] = EXTRACT_SYSTEM_MESSAGE\n    return self._extract(AnthropicCall, AnthropicTool, retries, **kwargs)\n</code></pre>"},{"location":"api/anthropic/extractors/#mirascope.anthropic.extractors.AnthropicExtractor.extract_async","title":"<code>extract_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously extracts <code>extract_schema</code> from the Anthropic call response.</p> <p>The <code>extract_schema</code> is converted into an <code>AnthropicTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Anthropic's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> Source code in <code>mirascope/anthropic/extractors.py</code> <pre><code>async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Asynchronously extracts `extract_schema` from the Anthropic call response.\n\n    The `extract_schema` is converted into an `AnthropicTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Anthropic's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n    \"\"\"\n    kwargs[\"system\"] = EXTRACT_SYSTEM_MESSAGE\n    return await self._extract_async(\n        AnthropicCall, AnthropicTool, retries, **kwargs\n    )\n</code></pre>"},{"location":"api/anthropic/extractors/#mirascope.anthropic.extractors.AnthropicExtractor.stream","title":"<code>stream(retries=0, **kwargs)</code>","text":"<p>Streams partial instances of <code>extract_schema</code> as the schema is streamed.</p> <p>The <code>extract_schema</code> is converted into a <code>partial(AnthropicTool)</code>, which allows for any field (i.e.function argument) in the tool to be <code>None</code>. This allows us to stream partial results as we construct the tool from the streamed chunks.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword argument parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>T</code> <p>The partial <code>extract_schema</code> instance from the current buffer.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> Source code in <code>mirascope/anthropic/extractors.py</code> <pre><code>def stream(self, retries: int = 0, **kwargs: Any) -&gt; Generator[T, None, None]:\n    \"\"\"Streams partial instances of `extract_schema` as the schema is streamed.\n\n    The `extract_schema` is converted into a `partial(AnthropicTool)`, which allows\n    for any field (i.e.function argument) in the tool to be `None`. This allows us\n    to stream partial results as we construct the tool from the streamed chunks.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword argument parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        The partial `extract_schema` instance from the current buffer.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n    \"\"\"\n    yield from self._stream(\n        AnthropicCall, AnthropicTool, AnthropicToolStream, retries, **kwargs\n    )\n</code></pre>"},{"location":"api/anthropic/extractors/#mirascope.anthropic.extractors.AnthropicExtractor.stream_async","title":"<code>stream_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously streams partial instances of <code>extract_schema</code> as streamed.</p> <p>The <code>extract_schema</code> is converted into a <code>partial(AnthropicTool)</code>, which allows for any field (i.e.function argument) in the tool to be <code>None</code>. This allows us to stream partial results as we construct the tool from the streamed chunks.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[T, None]</code> <p>The partial <code>extract_schema</code> instance from the current buffer.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> Source code in <code>mirascope/anthropic/extractors.py</code> <pre><code>async def stream_async(\n    self, retries: int = 0, **kwargs: Any\n) -&gt; AsyncGenerator[T, None]:\n    \"\"\"Asynchronously streams partial instances of `extract_schema` as streamed.\n\n    The `extract_schema` is converted into a `partial(AnthropicTool)`, which allows\n    for any field (i.e.function argument) in the tool to be `None`. This allows us\n    to stream partial results as we construct the tool from the streamed chunks.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        The partial `extract_schema` instance from the current buffer.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n    \"\"\"\n    async for partial_tool in self._stream_async(\n        AnthropicCall, AnthropicTool, AnthropicToolStream, retries, **kwargs\n    ):\n        yield partial_tool\n</code></pre>"},{"location":"api/anthropic/tools/","title":"anthropic.tools","text":"<p>Classes for using tools with Anthropic's Claude API.</p>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool","title":"<code>AnthropicTool</code>","text":"<p>             Bases: <code>BaseTool[ToolUseBlock]</code></p> <p>A base class for easy use of tools with the Anthropic Claude client.</p> <p><code>AnthropicTool</code> internally handles the logic that allows you to use tools with simple calls such as <code>AnthropicCallResponse.tool</code> or <code>AnthropicTool.fn</code>, as seen in the example below.</p> <p>Example:</p> <pre><code>from mirascope import AnthropicCall, AnthropicCallParams\n\n\ndef animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n    \"\"\"Tells you your most likely favorite animal from personality traits.\n\n    Args:\n        fav_food: your favorite food.\n        fav_color: your favorite color.\n\n    Returns:\n        The animal most likely to be your favorite based on traits.\n    \"\"\"\n    return \"Your favorite animal is the best one, a frog.\"\n\n\nclass AnimalMatcher(AnthropicCall):\n    prompt_template = \"\"\"\n    Tell me my favorite animal if my favorite food is {food} and my\n    favorite color is {color}.\n    \"\"\"\n\n    food: str\n    color: str\n\n    call_params = AnthropicCallParams(tools=[animal_matcher])\n\n\nresponse = AnimalMatcher(food=\"pizza\", color=\"red\").call\ntool = response.tool\nprint(tool.fn(**tool.args))\n#&gt; Your favorite animal is the best one, a frog.\n</code></pre> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>class AnthropicTool(BaseTool[ToolUseBlock]):\n    '''A base class for easy use of tools with the Anthropic Claude client.\n\n    `AnthropicTool` internally handles the logic that allows you to use tools with\n    simple calls such as `AnthropicCallResponse.tool` or `AnthropicTool.fn`, as seen in\n    the example below.\n\n    Example:\n\n    ```python\n    from mirascope import AnthropicCall, AnthropicCallParams\n\n\n    def animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n        \"\"\"Tells you your most likely favorite animal from personality traits.\n\n        Args:\n            fav_food: your favorite food.\n            fav_color: your favorite color.\n\n        Returns:\n            The animal most likely to be your favorite based on traits.\n        \"\"\"\n        return \"Your favorite animal is the best one, a frog.\"\n\n\n    class AnimalMatcher(AnthropicCall):\n        prompt_template = \"\"\"\n        Tell me my favorite animal if my favorite food is {food} and my\n        favorite color is {color}.\n        \"\"\"\n\n        food: str\n        color: str\n\n        call_params = AnthropicCallParams(tools=[animal_matcher])\n\n\n    response = AnimalMatcher(food=\"pizza\", color=\"red\").call\n    tool = response.tool\n    print(tool.fn(**tool.args))\n    #&gt; Your favorite animal is the best one, a frog.\n    ```\n    '''\n\n    @classmethod\n    def tool_schema(cls) -&gt; ToolParam:\n        \"\"\"Constructs JSON tool schema for use with Anthropic's Claude API.\"\"\"\n        schema = super().tool_schema()\n        return ToolParam(\n            input_schema=schema[\"parameters\"],\n            name=schema[\"name\"],\n            description=schema[\"description\"],\n        )\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given the tool call contents in a `Message` from an Anthropic call response,\n        this method parses out the arguments of the tool call and creates an\n        `AnthropicTool` instance from them.\n\n        Args:\n            tool_call: The list of `TextBlock` contents.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        model_json = tool_call.input\n        model_json[\"tool_call\"] = tool_call.model_dump()  # type: ignore\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[AnthropicTool]:\n        \"\"\"Constructs a `AnthropicTool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, AnthropicTool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[AnthropicTool]:\n        \"\"\"Constructs a `AnthropicTool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, AnthropicTool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[AnthropicTool]:\n        \"\"\"Constructs a `AnthropicTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>AnthropicTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[AnthropicTool]:\n    \"\"\"Constructs a `AnthropicTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>AnthropicTool</code> type from a function.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[AnthropicTool]:\n    \"\"\"Constructs a `AnthropicTool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>AnthropicTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[AnthropicTool]:\n    \"\"\"Constructs a `AnthropicTool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, AnthropicTool)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given the tool call contents in a <code>Message</code> from an Anthropic call response, this method parses out the arguments of the tool call and creates an <code>AnthropicTool</code> instance from them.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ToolUseBlock</code> <p>The list of <code>TextBlock</code> contents.</p> required <p>Returns:</p> Type Description <code>AnthropicTool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given the tool call contents in a `Message` from an Anthropic call response,\n    this method parses out the arguments of the tool call and creates an\n    `AnthropicTool` instance from them.\n\n    Args:\n        tool_call: The list of `TextBlock` contents.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    model_json = tool_call.input\n    model_json[\"tool_call\"] = tool_call.model_dump()  # type: ignore\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/anthropic/tools/#mirascope.anthropic.tools.AnthropicTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs JSON tool schema for use with Anthropic's Claude API.</p> Source code in <code>mirascope/anthropic/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ToolParam:\n    \"\"\"Constructs JSON tool schema for use with Anthropic's Claude API.\"\"\"\n    schema = super().tool_schema()\n    return ToolParam(\n        input_schema=schema[\"parameters\"],\n        name=schema[\"name\"],\n        description=schema[\"description\"],\n    )\n</code></pre>"},{"location":"api/anthropic/types/","title":"anthropic.types","text":"<p>Type classes for interacting with Anthropics's Claude API.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallParams","title":"<code>AnthropicCallParams</code>","text":"<p>             Bases: <code>BaseCallParams[AnthropicTool]</code></p> <p>The parameters to use when calling d Claud API with a prompt.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall, AnthropicCallParams\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend some books.\"\n\n    call_params = AnthropicCallParams(\n        model=\"anthropic-3-opus-20240229\",\n    )\n</code></pre> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>class AnthropicCallParams(BaseCallParams[AnthropicTool]):\n    \"\"\"The parameters to use when calling d Claud API with a prompt.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall, AnthropicCallParams\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend some books.\"\n\n        call_params = AnthropicCallParams(\n            model=\"anthropic-3-opus-20240229\",\n        )\n    ```\n    \"\"\"\n\n    max_tokens: int = 1000\n    model: str = \"claude-3-haiku-20240307\"\n    metadata: Optional[Metadata] = None\n    stop_sequences: Optional[list[str]] = None\n    system: Optional[str] = None\n    temperature: Optional[float] = None\n    top_k: Optional[int] = None\n    top_p: Optional[float] = None\n    extra_headers: Optional[Headers] = None\n    extra_query: Optional[Query] = None\n    extra_body: Optional[Body] = None\n    timeout: Optional[Union[float, Timeout]] = 600\n\n    response_format: Optional[Literal[\"json\"]] = None\n\n    wrapper: Optional[Callable[[Anthropic], Anthropic]] = None\n    wrapper_async: Optional[Callable[[AsyncAnthropic], AsyncAnthropic]] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def kwargs(\n        self,\n        tool_type: Optional[Type[AnthropicTool]] = None,\n        exclude: Optional[set[str]] = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the keyword argument call parameters.\"\"\"\n        extra_exclude = {\"response_format\", \"wrapper\", \"wrapper_async\"}\n        exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n        return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallParams.kwargs","title":"<code>kwargs(tool_type=None, exclude=None)</code>","text":"<p>Returns the keyword argument call parameters.</p> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>def kwargs(\n    self,\n    tool_type: Optional[Type[AnthropicTool]] = None,\n    exclude: Optional[set[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the keyword argument call parameters.\"\"\"\n    extra_exclude = {\"response_format\", \"wrapper\", \"wrapper_async\"}\n    exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n    return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse","title":"<code>AnthropicCallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[Union[Message, ToolsBetaMessage], AnthropicTool]</code></p> <p>Convenience wrapper around the Anthropic Claude API.</p> <p>When using Mirascope's convenience wrappers to interact with Anthropic models via <code>AnthropicCall</code>, responses using <code>Anthropic.call()</code> will return an <code>AnthropicCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend some books.\"\n\n\nprint(BookRecommender().call())\n</code></pre> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>class AnthropicCallResponse(\n    BaseCallResponse[Union[Message, ToolsBetaMessage], AnthropicTool]\n):\n    \"\"\"Convenience wrapper around the Anthropic Claude API.\n\n    When using Mirascope's convenience wrappers to interact with Anthropic models via\n    `AnthropicCall`, responses using `Anthropic.call()` will return an\n    `AnthropicCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend some books.\"\n\n\n    print(BookRecommender().call())\n    ```\n    \"\"\"\n\n    response_format: Optional[Literal[\"json\"]] = None\n\n    @property\n    def tools(self) -&gt; Optional[list[AnthropicTool]]:\n        \"\"\"Returns the tools for the 0th choice message.\"\"\"\n        if not self.tool_types:\n            return None\n\n        if self.response.stop_reason != \"tool_use\":\n            raise RuntimeError(\n                \"Generation stopped with stop reason that is not `tool_use`. \"\n                \"This is likely due to a limit on output tokens that is too low. \"\n                \"Note that this could also indicate no tool is beind called, so we \"\n                \"recommend that you check the output of the call to confirm. \"\n                f\"Stop Reason: {self.response.stop_reason} \"\n            )\n\n        extracted_tools = []\n        for tool_call in self.response.content:\n            if tool_call.type != \"tool_use\":\n                continue\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type.__name__:\n                    tool = tool_type.from_tool_call(tool_call)\n                    extracted_tools.append(tool)\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[AnthropicTool]:\n        \"\"\"Returns the 0th tool for the 0th choice text block.\"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string text of the 0th text block.\"\"\"\n        block = self.response.content[0]\n        return block.text if block.type == \"text\" else \"\"\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": self.response.model_dump(),\n        }\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string text of the 0th text block.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.tool","title":"<code>tool: Optional[AnthropicTool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice text block.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.tools","title":"<code>tools: Optional[list[AnthropicTool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": self.response.model_dump(),\n    }\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponseChunk","title":"<code>AnthropicCallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[MessageStreamEvent, AnthropicTool]</code></p> <p>Convenience wrapper around the Anthropic API streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Anthropic models via <code>AnthropicCall</code>, responses using <code>AnthropicCall.stream()</code> will yield <code>AnthropicCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.anthropic import AnthropicCall\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend some books.\"\n\n\nfor chunk in BookRecommender().stream():\n    print(chunk, end=\"\")\n</code></pre> Source code in <code>mirascope/anthropic/types.py</code> <pre><code>class AnthropicCallResponseChunk(\n    BaseCallResponseChunk[MessageStreamEvent, AnthropicTool]\n):\n    \"\"\"Convenience wrapper around the Anthropic API streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Anthropic models via\n    `AnthropicCall`, responses using `AnthropicCall.stream()` will yield\n    `AnthropicCallResponseChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.anthropic import AnthropicCall\n\n\n    class BookRecommender(AnthropicCall):\n        prompt_template = \"Please recommend some books.\"\n\n\n    for chunk in BookRecommender().stream():\n        print(chunk, end=\"\")\n    ```\n    \"\"\"\n\n    response_format: Optional[Literal[\"json\"]] = None\n\n    @property\n    def type(\n        self,\n    ) -&gt; Literal[\n        \"message_start\",\n        \"message_delta\",\n        \"message_stop\",\n        \"content_block_start\",\n        \"content_block_delta\",\n        \"content_block_stop\",\n    ]:\n        \"\"\"Returns the type of the chunk.\"\"\"\n        return self.chunk.type\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string content of the 0th message.\"\"\"\n        if isinstance(self.chunk, ContentBlockStartEvent):\n            return self.chunk.content_block.text\n        if isinstance(self.chunk, ContentBlockDeltaEvent):\n            return self.chunk.delta.text\n        return \"\"\n</code></pre>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string content of the 0th message.</p>"},{"location":"api/anthropic/types/#mirascope.anthropic.types.AnthropicCallResponseChunk.type","title":"<code>type: Literal['message_start', 'message_delta', 'message_stop', 'content_block_start', 'content_block_delta', 'content_block_stop']</code>  <code>property</code>","text":"<p>Returns the type of the chunk.</p>"},{"location":"api/base/","title":"base","text":"<p>Base modules for the Mirascope library.</p>"},{"location":"api/base/calls/","title":"base.calls","text":"<p>A base abstract interface for calling LLMs.</p>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall","title":"<code>BaseCall</code>","text":"<p>             Bases: <code>BasePrompt</code>, <code>Generic[BaseCallResponseT, BaseCallResponseChunkT, BaseToolT]</code>, <code>ABC</code></p> <p>The base class abstract interface for calling LLMs.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>class BaseCall(\n    BasePrompt,\n    Generic[BaseCallResponseT, BaseCallResponseChunkT, BaseToolT],\n    ABC,\n):\n    \"\"\"The base class abstract interface for calling LLMs.\"\"\"\n\n    api_key: ClassVar[Optional[str]] = None\n    base_url: ClassVar[Optional[str]] = None\n    call_params: ClassVar[BaseCallParams] = BaseCallParams[BaseToolT](\n        model=\"gpt-3.5-turbo-0125\"\n    )\n\n    @abstractmethod\n    def call(self, **kwargs: Any) -&gt; BaseCallResponseT:\n        \"\"\"A call to an LLM.\n\n        An implementation of this function must return a response that extends\n        `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n        different model providers.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def call_async(self, **kwargs: Any) -&gt; BaseCallResponseT:\n        \"\"\"An asynchronous call to an LLM.\n\n        An implementation of this function must return a response that extends\n        `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n        different model providers.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def stream(self, **kwargs: Any) -&gt; Generator[BaseCallResponseChunkT, None, None]:\n        \"\"\"A call to an LLM that streams the response in chunks.\n\n        An implementation of this function must yield response chunks that extend\n        `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n        e.g. different model providers.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[BaseCallResponseChunkT, None]:\n        \"\"\"A asynchronous call to an LLM that streams the response in chunks.\n\n        An implementation of this function must yield response chunks that extend\n        `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n        e.g. different model providers.\"\"\"\n        yield ...  # type: ignore # pragma: no cover\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _setup(\n        self,\n        kwargs: dict[str, Any],\n        base_tool_type: Optional[Type[BaseToolT]] = None,\n    ) -&gt; tuple[dict[str, Any], Optional[list[Type[BaseToolT]]]]:\n        \"\"\"Returns the call params kwargs and tool types.\n\n        The tools in the call params first get converted into BaseToolT types. We then\n        need both the converted tools for the response (so it can construct actual tool\n        instances if present in the response) as well as the actual schemas injected\n        through kwargs. This function handles that setup.\n        \"\"\"\n        call_params = self.call_params.model_copy(update=kwargs)\n        kwargs = call_params.kwargs(tool_type=base_tool_type)\n        tool_types = None\n        if \"tools\" in kwargs and base_tool_type is not None:\n            tool_types = kwargs.pop(\"tools\")\n            kwargs[\"tools\"] = [tool_type.tool_schema() for tool_type in tool_types]\n        return kwargs, tool_types\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.call","title":"<code>call(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>A call to an LLM.</p> <p>An implementation of this function must return a response that extends <code>BaseCallResponse</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\ndef call(self, **kwargs: Any) -&gt; BaseCallResponseT:\n    \"\"\"A call to an LLM.\n\n    An implementation of this function must return a response that extends\n    `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n    different model providers.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>An asynchronous call to an LLM.</p> <p>An implementation of this function must return a response that extends <code>BaseCallResponse</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\nasync def call_async(self, **kwargs: Any) -&gt; BaseCallResponseT:\n    \"\"\"An asynchronous call to an LLM.\n\n    An implementation of this function must return a response that extends\n    `BaseCallResponse`. This ensures a consistent API and convenience across e.g.\n    different model providers.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.stream","title":"<code>stream(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>A call to an LLM that streams the response in chunks.</p> <p>An implementation of this function must yield response chunks that extend <code>BaseCallResponseChunk</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\ndef stream(self, **kwargs: Any) -&gt; Generator[BaseCallResponseChunkT, None, None]:\n    \"\"\"A call to an LLM that streams the response in chunks.\n\n    An implementation of this function must yield response chunks that extend\n    `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n    e.g. different model providers.\n    \"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/calls/#mirascope.base.calls.BaseCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>A asynchronous call to an LLM that streams the response in chunks.</p> <p>An implementation of this function must yield response chunks that extend <code>BaseCallResponseChunk</code>. This ensures a consistent API and convenience across e.g. different model providers.</p> Source code in <code>mirascope/base/calls.py</code> <pre><code>@abstractmethod\nasync def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[BaseCallResponseChunkT, None]:\n    \"\"\"A asynchronous call to an LLM that streams the response in chunks.\n\n    An implementation of this function must yield response chunks that extend\n    `BaseCallResponseChunk`. This ensures a consistent API and convenience across\n    e.g. different model providers.\"\"\"\n    yield ...  # type: ignore # pragma: no cover\n</code></pre>"},{"location":"api/base/extractors/","title":"base.extractors","text":"<p>A base abstract interface for extracting structured information using LLMs.</p>"},{"location":"api/base/extractors/#mirascope.base.extractors.BaseExtractor","title":"<code>BaseExtractor</code>","text":"<p>             Bases: <code>BasePrompt</code>, <code>Generic[BaseCallT, BaseToolT, BaseToolStreamT, ExtractedTypeT]</code>, <code>ABC</code></p> <p>The base abstract interface for extracting structured information using LLMs.</p> Source code in <code>mirascope/base/extractors.py</code> <pre><code>class BaseExtractor(\n    BasePrompt, Generic[BaseCallT, BaseToolT, BaseToolStreamT, ExtractedTypeT], ABC\n):\n    \"\"\"The base abstract interface for extracting structured information using LLMs.\"\"\"\n\n    extract_schema: ExtractionType\n\n    api_key: ClassVar[Optional[str]] = None\n    base_url: ClassVar[Optional[str]] = None\n    call_params: ClassVar[BaseCallParams] = BaseCallParams[BaseToolT](\n        model=\"gpt-3.5-turbo-0125\"\n    )\n\n    @abstractmethod\n    def extract(self, retries: int = 0) -&gt; ExtractedTypeT:\n        \"\"\"Extracts the `extraction_schema` from an LLM call.\"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def extract_async(self, retries: int = 0) -&gt; ExtractedTypeT:\n        \"\"\"Asynchronously extracts the `extraction_schema` from an LLM call.\"\"\"\n        ...  # pragma: no cover\n\n    # Note: only some model providers support streaming tools, so we only implement\n    # streaming for those providers and do not require all extractors to implement\n    # the `stream` and `stream_async` methods.\n    # @abstractmethod\n    # def stream(self, retries: int = 0) -&gt; Generator[ExtractedTypeT, None, None]:\n    #     \"\"\"Streams extracted partial `extraction_schema` instances.\"\"\"\n    #     ...  # pragma: no cover\n\n    # @abstractmethod\n    # async def stream_async(\n    #     self, retries: int = 0\n    # ) -&gt; AsyncGenerator[ExtractedTypeT, None]:\n    #     \"\"\"Asynchronously streams extracted partial `extraction_schema` instances.\"\"\"\n    #     ...  # pragma: no cover\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _extract(\n        self,\n        call_type: Type[BaseCallT],\n        tool_type: Type[BaseToolT],\n        retries: int,\n        **kwargs: Any,\n    ) -&gt; ExtractedTypeT:\n        \"\"\"Extracts `extract_schema` from the call response.\n\n        The `extract_schema` is converted into a tool, complete with a description of\n        the tool, all of the fields, and their types. This allows us to take advantage\n        of tools/function calling functionality to extract information from a prompt\n        according to the context provided by the `BaseModel` schema.\n\n        Args:\n            call_type: The type of call to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            tool_type: The type of tool to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            retries: The number of call attempts to make on `ValidationError` before\n                giving up and throwing the error to the user.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            An instance of the `extract_schema` with it's fields populated.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs, return_tool = self._setup(tool_type, kwargs)\n\n        class TempCall(call_type):  # type: ignore\n            prompt_template = self.prompt_template\n\n            base_url = self.base_url\n            api_key = self.api_key\n            call_params = self.call_params\n\n            model_config = ConfigDict(extra=\"allow\")\n\n        response = TempCall(**self.model_dump(exclude={\"extract_schema\"})).call(\n            **kwargs\n        )\n        try:\n            extracted_schema = self._extract_schema(\n                response.tool, self.extract_schema, return_tool, response=response\n            )\n            if extracted_schema is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            return extracted_schema\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return self._extract(call_type, tool_type, retries - 1, **kwargs)\n            raise  # re-raise if we have no retries left\n\n    async def _extract_async(\n        self,\n        call_type: Type[BaseCallT],\n        tool_type: Type[BaseToolT],\n        retries: int,\n        **kwargs: Any,\n    ) -&gt; ExtractedTypeT:\n        \"\"\"Extracts `extract_schema` from the asynchronous call response.\n\n        The `extract_schema` is converted into a tool, complete with a description of\n        the tool, all of the fields, and their types. This allows us to take advantage\n        of tools/function calling functionality to extract information from a prompt\n        according to the context provided by the `BaseModel` schema.\n\n        Args:\n            call_type: The type of call to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            tool_type: The type of tool to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            retries: The number of call attempts to make on `ValidationError` before\n                giving up and throwing the error to the user.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            An instance of the `extract_schema` with it's fields populated.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs, return_tool = self._setup(tool_type, kwargs)\n\n        class TempCall(call_type):  # type: ignore\n            prompt_template = self.prompt_template\n\n            base_url = self.base_url\n            api_key = self.api_key\n            call_params = self.call_params\n\n            model_config = ConfigDict(extra=\"allow\")\n\n        response = await TempCall(\n            **self.model_dump(exclude={\"extract_schema\"})\n        ).call_async(**kwargs)\n        try:\n            extracted_schema = self._extract_schema(\n                response.tool, self.extract_schema, return_tool, response=response\n            )\n            if extracted_schema is None:\n                raise AttributeError(\"No tool found in the completion.\")\n            return extracted_schema\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                return await self._extract_async(\n                    call_type, tool_type, retries - 1, **kwargs\n                )\n            raise  # re-raise if we have no retries left\n\n    def _stream(\n        self,\n        call_type: Type[BaseCallT],\n        tool_type: Type[BaseToolT],\n        tool_stream_type: Type[BaseToolStreamT],\n        retries: int,\n        **kwargs: Any,\n    ) -&gt; Generator[ExtractedTypeT, None, None]:\n        \"\"\"Streams partial `extract_schema` instances from the streamed chunks.\n\n        The `extract_schema` is converted into a partial tool, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of tools/function calling functionality to stream information\n        extracted from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            call_type: The type of call to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            tool_type: The type of tool to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            tool_stream_type: The type of tool stream to use for streaming tools. This\n                enables shared code across various model providers that have slight\n                variations but the same internal interfaces.\n            retries: The number of call attempts to make on `ValidationError` before\n                giving up and throwing the error to the user.\n            **kwargs: Additional keyword arguments.\n\n        Yields:\n            An instance of the partial `extract_schema` with it's available fields\n            populated.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs, return_tool = self._setup(tool_type, kwargs)\n\n        class TempCall(call_type):  # type: ignore\n            prompt_template = self.prompt_template\n\n            base_url = self.base_url\n            api_key = self.api_key\n            call_params = self.call_params\n\n            model_config = ConfigDict(extra=\"allow\")\n\n        setattr(TempCall, \"messages\", self.messages)\n        stream = TempCall(**self.model_dump(exclude={\"extract_schema\"})).stream(\n            **kwargs\n        )\n        tool_stream = tool_stream_type.from_stream(stream, allow_partial=True)\n        try:\n            yielded = False\n            for partial_tool in tool_stream:\n                extracted_schema = self._extract_schema(\n                    partial_tool, self.extract_schema, return_tool, response=None\n                )\n                if extracted_schema is None:\n                    break\n                yielded = True\n                yield extracted_schema\n\n            if not yielded:\n                raise AttributeError(\"No tool found in the completion.\")\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                yield from self._stream(\n                    call_type, tool_type, tool_stream_type, retries - 1, **kwargs\n                )\n            raise  # re-raise if we have no retries left\n\n    async def _stream_async(\n        self,\n        call_type: Type[BaseCallT],\n        tool_type: Type[BaseToolT],\n        tool_stream_type: Type[BaseToolStreamT],\n        retries: int,\n        **kwargs: Any,\n    ) -&gt; AsyncGenerator[ExtractedTypeT, None]:\n        \"\"\"Asynchronously streams partial `extract_schema`s from streamed chunks.\n\n        The `extract_schema` is converted into a partial tool, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of tools/function calling functionality to stream information\n        extracted from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            call_type: The type of call to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            tool_type: The type of tool to use for extraction. This enables shared code\n                across various model providers that have slight variations but the same\n                internal interfaces.\n            tool_stream_type: The type of tool stream to use for streaming tools. This\n                enables shared code across various model providers that have slight\n                variations but the same internal interfaces.\n            retries: The number of call attempts to make on `ValidationError` before\n                giving up and throwing the error to the user.\n            **kwargs: Additional keyword arguments.\n\n        Yields:\n            An instance of the partial `extract_schema` with it's available fields\n            populated.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n        \"\"\"\n        kwargs, return_tool = self._setup(tool_type, kwargs)\n\n        class TempCall(call_type):  # type: ignore\n            prompt_template = self.prompt_template\n\n            base_url = self.base_url\n            api_key = self.api_key\n            call_params = self.call_params\n\n            model_config = ConfigDict(extra=\"allow\")\n\n        setattr(TempCall, \"messages\", self.messages)\n        stream = TempCall(**self.model_dump(exclude={\"extract_schema\"})).stream_async(\n            **kwargs\n        )\n        tool_stream = tool_stream_type.from_async_stream(stream, allow_partial=True)\n        try:\n            yielded = False\n            async for partial_tool in tool_stream:\n                extracted_schema = self._extract_schema(\n                    partial_tool, self.extract_schema, return_tool, response=None\n                )\n                if extracted_schema is None:\n                    break\n                yielded = True\n                yield extracted_schema\n\n            if not yielded:\n                raise AttributeError(\"No tool found in the completion.\")\n        except (AttributeError, ValueError, ValidationError) as e:\n            if retries &gt; 0:\n                logging.info(f\"Retrying due to exception: {e}\")\n                # TODO: include failure in retry prompt.\n                async for partial_tool in self._stream_async(\n                    call_type, tool_type, tool_stream_type, retries - 1, **kwargs\n                ):\n                    yield partial_tool\n            raise  # re-raise if we have no retries left\n\n    def _extract_schema(\n        self,\n        tool: Optional[BaseToolT],\n        schema: ExtractedType,\n        return_tool: bool,\n        response: Optional[Any],\n    ) -&gt; Optional[ExtractedTypeT]:\n        \"\"\"Returns the extracted schema extracted depending on it's extraction type.\n\n        Due to mypy issues with all these generics, we have to type ignore a bunch\n        of stuff so it doesn't complain, but each conditional properly checks types\n        before doing anything specific to that type (it's just that mypy is annoying).\n        \"\"\"\n        if tool is None:\n            return None\n        if return_tool:\n            return tool  # type: ignore\n        if _is_base_type(schema):\n            return tool.value  # type: ignore\n        if response:\n            model = schema(**tool.model_dump())  # type: ignore\n            model._response = response\n        else:\n            schema = partial(schema)  # type: ignore\n            model = schema(**tool.model_dump())\n            model._tool_call = tool.tool_call  # type: ignore\n        return model\n\n    def _setup(\n        self, tool_type: Type[BaseToolT], kwargs: dict[str, Any]\n    ) -&gt; tuple[dict[str, Any], bool]:\n        \"\"\"Returns the call params kwargs and whether to return the tool directly.\"\"\"\n        call_params = self.call_params.model_copy(update=kwargs)\n        kwargs = call_params.kwargs(tool_type=tool_type)\n        if _is_base_type(self.extract_schema):\n            tool = tool_type.from_base_type(self.extract_schema)  # type: ignore\n            return_tool = False\n        elif not isclass(self.extract_schema):\n            tool = tool_type.from_fn(self.extract_schema)\n            return_tool = True\n        elif not issubclass(self.extract_schema, tool_type):\n            tool = tool_type.from_model(self.extract_schema)\n            return_tool = False\n        else:\n            tool = self.extract_schema\n            return_tool = True\n        kwargs[\"tools\"] = [tool]\n        return kwargs, return_tool\n</code></pre>"},{"location":"api/base/extractors/#mirascope.base.extractors.BaseExtractor.extract","title":"<code>extract(retries=0)</code>  <code>abstractmethod</code>","text":"<p>Extracts the <code>extraction_schema</code> from an LLM call.</p> Source code in <code>mirascope/base/extractors.py</code> <pre><code>@abstractmethod\ndef extract(self, retries: int = 0) -&gt; ExtractedTypeT:\n    \"\"\"Extracts the `extraction_schema` from an LLM call.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/extractors/#mirascope.base.extractors.BaseExtractor.extract_async","title":"<code>extract_async(retries=0)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Asynchronously extracts the <code>extraction_schema</code> from an LLM call.</p> Source code in <code>mirascope/base/extractors.py</code> <pre><code>@abstractmethod\nasync def extract_async(self, retries: int = 0) -&gt; ExtractedTypeT:\n    \"\"\"Asynchronously extracts the `extraction_schema` from an LLM call.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/prompts/","title":"base.prompts","text":"<p>A base class for writing prompts.</p>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The base class for working with prompts.</p> <p>This class is implemented as the base for all prompting needs across various model providers.</p> <p>Example:</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"A prompt for recommending a book.\"\"\"\n\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    USER: Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nprint(prompt.messages())\n#&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\nprint(prompt)\n#&gt; Please recommend a fantasy book.\n</code></pre> Source code in <code>mirascope/base/prompts.py</code> <pre><code>class BasePrompt(BaseModel):\n    '''The base class for working with prompts.\n\n    This class is implemented as the base for all prompting needs across various model\n    providers.\n\n    Example:\n\n    ```python\n    from mirascope import BasePrompt\n\n\n    class BookRecommendationPrompt(BasePrompt):\n        \"\"\"A prompt for recommending a book.\"\"\"\n\n        prompt_template = \"\"\"\n        SYSTEM: You are the world's greatest librarian.\n        USER: Please recommend a {genre} book.\n        \"\"\"\n\n        genre: str\n\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n    print(prompt.messages())\n    #&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\n    print(prompt)\n    #&gt; Please recommend a fantasy book.\n    ```\n    '''\n\n    tags: ClassVar[list[str]] = []\n    prompt_template: ClassVar[str] = \"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the formatted template.\"\"\"\n        return self._format_template(self.prompt_template)\n\n    def messages(self) -&gt; Union[list[Message], Any]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        message_type_by_role = {\n            MessageRole.SYSTEM: SystemMessage,\n            MessageRole.USER: UserMessage,\n            MessageRole.ASSISTANT: AssistantMessage,\n            MessageRole.MODEL: ModelMessage,\n            MessageRole.TOOL: ToolMessage,\n        }\n        return [\n            message_type_by_role[MessageRole(message[\"role\"])](\n                role=message[\"role\"], content=message[\"content\"]\n            )\n            for message in self._parse_messages(list(message_type_by_role.keys()))\n        ]\n\n    def dump(\n        self,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n        return {\n            \"tags\": self.tags,\n            \"template\": dedent(self.prompt_template).strip(\"\\n\"),\n            \"inputs\": self.model_dump(),\n        }\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _format_template(self, template: str):\n        \"\"\"Formats the given `template` with attributes matching template variables.\"\"\"\n        dedented_template = dedent(template).strip()\n        template_vars = [\n            var\n            for _, var, _, _ in Formatter().parse(dedented_template)\n            if var is not None\n        ]\n\n        values = {}\n        for var in template_vars:\n            attr = getattr(self, var)\n            if attr and isinstance(attr, list):\n                if isinstance(attr[0], list):\n                    values[var] = \"\\n\\n\".join(\n                        [\"\\n\".join([str(subitem) for subitem in item]) for item in attr]\n                    )\n                else:\n                    values[var] = \"\\n\".join([str(item) for item in attr])\n            else:\n                values[var] = str(attr)\n\n        return dedented_template.format(**values)\n\n    def _parse_messages(self, roles: list[str]) -&gt; list[Message]:\n        \"\"\"Returns messages parsed from the `template` ClassVar.\n\n        Raises:\n            ValueError: if the template contains an unknown role.\n        \"\"\"\n        messages = []\n        re_roles = \"|\".join(\n            [role.upper() for role in roles] + [\"MESSAGES\"] + [\"[A-Z]+\"]\n        )\n        for match in re.finditer(\n            rf\"({re_roles}):((.|\\n)+?)(?=({re_roles}):|\\Z)\",\n            self.prompt_template,\n        ):\n            role = match.group(1).lower()\n            if role == \"messages\":\n                template_var = [\n                    var\n                    for _, var, _, _ in Formatter().parse(match.group(2))\n                    if var is not None\n                ][0]\n                if not hasattr(self, template_var) or not isinstance(\n                    getattr(self, template_var), list\n                ):\n                    raise ValueError(\n                        f\"MESSAGES keyword used with attribute `{template_var}`, which \"\n                        \"is not a `list` of messages.\"\n                    )\n                messages += getattr(self, template_var)\n            else:\n                if role not in roles:\n                    raise ValueError(f\"Invalid role: {role}\")\n                content = self._format_template(match.group(2))\n                messages.append({\"role\": role, \"content\": content})\n        if len(messages) == 0:\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": self._format_template(self.prompt_template),\n                }\n            )\n        return messages\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the formatted template.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the formatted template.\"\"\"\n    return self._format_template(self.prompt_template)\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt.dump","title":"<code>dump()</code>","text":"<p>Dumps the contents of the prompt into a dictionary.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def dump(\n    self,\n) -&gt; dict[str, Any]:\n    \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n    return {\n        \"tags\": self.tags,\n        \"template\": dedent(self.prompt_template).strip(\"\\n\"),\n        \"inputs\": self.model_dump(),\n    }\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.BasePrompt.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def messages(self) -&gt; Union[list[Message], Any]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    message_type_by_role = {\n        MessageRole.SYSTEM: SystemMessage,\n        MessageRole.USER: UserMessage,\n        MessageRole.ASSISTANT: AssistantMessage,\n        MessageRole.MODEL: ModelMessage,\n        MessageRole.TOOL: ToolMessage,\n    }\n    return [\n        message_type_by_role[MessageRole(message[\"role\"])](\n            role=message[\"role\"], content=message[\"content\"]\n        )\n        for message in self._parse_messages(list(message_type_by_role.keys()))\n    ]\n</code></pre>"},{"location":"api/base/prompts/#mirascope.base.prompts.tags","title":"<code>tags(args)</code>","text":"<p>A decorator for adding tags to a <code>BasePrompt</code>.</p> <p>Adding this decorator to a <code>BasePrompt</code> updates the <code>_tags</code> class attribute to the given value. This is useful for adding metadata to a <code>BasePrompt</code> that can be used for logging or filtering.</p> <p>Example:</p> <pre><code>from mirascope import BasePrompt, tags\n\n\n@tags([\"book_recommendation\", \"entertainment\"])\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    I've recently read this book: {book_title}.\n    What should I read next?\n    \"\"\"\n\n    book_title: [str]\n\nprint(BookRecommendationPrompt.dump()[\"tags\"])\n#&gt; ['book_recommendation', 'entertainment']\n</code></pre> <p>Returns:</p> Type Description <code>Callable[[Type[BasePromptT]], Type[BasePromptT]]</code> <p>The decorated class with <code>tags</code> class attribute set.</p> Source code in <code>mirascope/base/prompts.py</code> <pre><code>def tags(args: list[str]) -&gt; Callable[[Type[BasePromptT]], Type[BasePromptT]]:\n    '''A decorator for adding tags to a `BasePrompt`.\n\n    Adding this decorator to a `BasePrompt` updates the `_tags` class attribute to the\n    given value. This is useful for adding metadata to a `BasePrompt` that can be used\n    for logging or filtering.\n\n    Example:\n\n    ```python\n    from mirascope import BasePrompt, tags\n\n\n    @tags([\"book_recommendation\", \"entertainment\"])\n    class BookRecommendationPrompt(BasePrompt):\n        prompt_template = \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        I've recently read this book: {book_title}.\n        What should I read next?\n        \"\"\"\n\n        book_title: [str]\n\n    print(BookRecommendationPrompt.dump()[\"tags\"])\n    #&gt; ['book_recommendation', 'entertainment']\n    ```\n\n    Returns:\n        The decorated class with `tags` class attribute set.\n    '''\n\n    def tags_fn(model_class: Type[BasePromptT]) -&gt; Type[BasePromptT]:\n        \"\"\"Updates the `tags` class attribute to the given value.\"\"\"\n        setattr(model_class, \"tags\", args)\n        return model_class\n\n    return tags_fn\n</code></pre>"},{"location":"api/base/tools/","title":"base.tools","text":"<p>A base interface for using tools (function calling) when calling LLMs.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool","title":"<code>BaseTool</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[ToolCallT]</code>, <code>ABC</code></p> <p>A base class for easy use of tools with prompts.</p> <p><code>BaseTool</code> is an abstract class interface and should not be used directly. When implementing a class that extends <code>BaseTool</code>, you must include the original <code>tool_call</code> from which this till was instantiated. Make sure to skip <code>tool_call</code> when generating the schema by annotating it with <code>SkipJsonSchema</code>.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>class BaseTool(BaseModel, Generic[ToolCallT], ABC):\n    \"\"\"A base class for easy use of tools with prompts.\n\n    `BaseTool` is an abstract class interface and should not be used directly. When\n    implementing a class that extends `BaseTool`, you must include the original\n    `tool_call` from which this till was instantiated. Make sure to skip `tool_call`\n    when generating the schema by annotating it with `SkipJsonSchema`.\n    \"\"\"\n\n    tool_call: SkipJsonSchema[ToolCallT]\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def args(self) -&gt; dict[str, Any]:\n        \"\"\"The arguments of the tool as a dictionary.\"\"\"\n        return self.model_dump(exclude={\"tool_call\"})\n\n    @property\n    def fn(self) -&gt; Callable:\n        \"\"\"Returns the function that the tool describes.\"\"\"\n        raise RuntimeError(\"Tool does not have an attached function.\")\n\n    @classmethod\n    def tool_schema(cls) -&gt; Any:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n\n        fn = {\n            \"name\": model_schema.pop(\"title\"),\n            \"description\": model_schema.pop(\"description\")\n            if \"description\" in model_schema\n            else DEFAULT_TOOL_DOCSTRING,\n        }\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return fn\n\n    @classmethod\n    @abstractmethod\n    def from_tool_call(cls, tool_call: ToolCallT) -&gt; BaseTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[BaseTool]:\n        \"\"\"Constructs a `BaseTool` type from a `BaseModel` type.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[BaseTool]:\n        \"\"\"Constructs a `BaseTool` type from a function.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[BaseTool]:\n        \"\"\"Constructs a `BaseTool` type from a `BaseType` type.\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.args","title":"<code>args: dict[str, Any]</code>  <code>property</code>","text":"<p>The arguments of the tool as a dictionary.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.fn","title":"<code>fn: Callable</code>  <code>property</code>","text":"<p>Returns the function that the tool describes.</p>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a <code>BaseTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[BaseTool]:\n    \"\"\"Constructs a `BaseTool` type from a `BaseType` type.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_fn","title":"<code>from_fn(fn)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a <code>BaseTool</code> type from a function.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[BaseTool]:\n    \"\"\"Constructs a `BaseTool` type from a function.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_model","title":"<code>from_model(model)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a <code>BaseTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[BaseTool]:\n    \"\"\"Constructs a `BaseTool` type from a `BaseModel` type.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_tool_call(cls, tool_call: ToolCallT) -&gt; BaseTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/tools/#mirascope.base.tools.BaseTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/base/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Any:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n\n    fn = {\n        \"name\": model_schema.pop(\"title\"),\n        \"description\": model_schema.pop(\"description\")\n        if \"description\" in model_schema\n        else DEFAULT_TOOL_DOCSTRING,\n    }\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return fn\n</code></pre>"},{"location":"api/base/types/","title":"base.types","text":"<p>Base types and abstract interfaces for typing LLM calls.</p>"},{"location":"api/base/types/#mirascope.base.types.AssistantMessage","title":"<code>AssistantMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>assistant</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['assistant']]</code> <p>The role of the message's author, in this case <code>assistant</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class AssistantMessage(TypedDict, total=False):\n    \"\"\"A message with the `assistant` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `assistant`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"assistant\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallParams","title":"<code>BaseCallParams</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[BaseToolT]</code></p> <p>The parameters with which to make a call.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallParams(BaseModel, Generic[BaseToolT]):\n    \"\"\"The parameters with which to make a call.\"\"\"\n\n    model: str\n    tools: Optional[list[Union[Callable, Type[BaseToolT]]]] = None\n    weave: Optional[Callable[[T], T]] = None\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    def kwargs(\n        self,\n        tool_type: Optional[Type[BaseToolT]] = None,\n        exclude: Optional[set[str]] = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the call as a keyword arguments dictionary.\"\"\"\n        extra_exclude = {\"tools\", \"weave\"}\n        exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n        kwargs = {\n            key: value\n            for key, value in self.model_dump(exclude=exclude).items()\n            if value is not None\n        }\n        if not self.tools or tool_type is None:\n            return kwargs\n        kwargs[\"tools\"] = [\n            tool if isclass(tool) else convert_function_to_tool(tool, tool_type)\n            for tool in self.tools\n        ]\n        return kwargs\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallParams.kwargs","title":"<code>kwargs(tool_type=None, exclude=None)</code>","text":"<p>Returns all parameters for the call as a keyword arguments dictionary.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>def kwargs(\n    self,\n    tool_type: Optional[Type[BaseToolT]] = None,\n    exclude: Optional[set[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the call as a keyword arguments dictionary.\"\"\"\n    extra_exclude = {\"tools\", \"weave\"}\n    exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n    kwargs = {\n        key: value\n        for key, value in self.model_dump(exclude=exclude).items()\n        if value is not None\n    }\n    if not self.tools or tool_type is None:\n        return kwargs\n    kwargs[\"tools\"] = [\n        tool if isclass(tool) else convert_function_to_tool(tool, tool_type)\n        for tool in self.tools\n    ]\n    return kwargs\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse","title":"<code>BaseCallResponse</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[ResponseT, BaseToolT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM call responses.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>ResponseT</code> <p>The original response from whichever model response this wraps.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallResponse(BaseModel, Generic[ResponseT, BaseToolT], ABC):\n    \"\"\"A base abstract interface for LLM call responses.\n\n    Attributes:\n        response: The original response from whichever model response this wraps.\n    \"\"\"\n\n    response: ResponseT\n    tool_types: Optional[list[Type[BaseToolT]]] = None\n    start_time: float  # The start time of the completion in ms\n    end_time: float  # The end time of the completion in ms\n    cost: Optional[float] = None  # The cost of the completion in dollars\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def tools(self) -&gt; Optional[list[BaseToolT]]:\n        \"\"\"Returns the tools for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def tool(self) -&gt; Optional[BaseToolT]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response.\n\n        If there are multiple choices in a response, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response.</p> <p>If there are multiple choices in a response, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse.tool","title":"<code>tool: Optional[BaseToolT]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponse.tools","title":"<code>tools: Optional[list[BaseToolT]]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponseChunk","title":"<code>BaseCallResponseChunk</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[ChunkT, BaseToolT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM streaming response chunks.</p> <p>Attributes:</p> Name Type Description <code>response</code> <p>The original response chunk from whichever model response this wraps.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class BaseCallResponseChunk(BaseModel, Generic[ChunkT, BaseToolT], ABC):\n    \"\"\"A base abstract interface for LLM streaming response chunks.\n\n    Attributes:\n        response: The original response chunk from whichever model response this wraps.\n    \"\"\"\n\n    chunk: ChunkT\n    tool_types: Optional[list[Type[BaseToolT]]] = None\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response chunk.\n\n        If there are multiple choices in a chunk, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.BaseCallResponseChunk.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response chunk.</p> <p>If there are multiple choices in a chunk, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/base/types/#mirascope.base.types.ModelMessage","title":"<code>ModelMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>model</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['model']]</code> <p>The role of the message's author, in this case <code>model</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class ModelMessage(TypedDict, total=False):\n    \"\"\"A message with the `model` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `model`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"model\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.SystemMessage","title":"<code>SystemMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>system</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['system']]</code> <p>The role of the message's author, in this case <code>system</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class SystemMessage(TypedDict, total=False):\n    \"\"\"A message with the `system` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `system`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"system\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>tool</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['tool']]</code> <p>The role of the message's author, in this case <code>tool</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class ToolMessage(TypedDict, total=False):\n    \"\"\"A message with the `tool` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `tool`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"tool\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/types/#mirascope.base.types.UserMessage","title":"<code>UserMessage</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A message with the <code>user</code> role.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Required[Literal['user']]</code> <p>The role of the message's author, in this case <code>user</code>.</p> <code>content</code> <code>Required[str]</code> <p>The contents of the message.</p> Source code in <code>mirascope/base/types.py</code> <pre><code>class UserMessage(TypedDict, total=False):\n    \"\"\"A message with the `user` role.\n\n    Attributes:\n        role: The role of the message's author, in this case `user`.\n        content: The contents of the message.\n    \"\"\"\n\n    role: Required[Literal[\"user\"]]\n    content: Required[str]\n</code></pre>"},{"location":"api/base/utils/","title":"base.utils","text":"<p>Base utility functions.</p>"},{"location":"api/base/utils/#mirascope.base.utils.convert_base_model_to_tool","title":"<code>convert_base_model_to_tool(schema, base)</code>","text":"<p>Converts a <code>BaseModel</code> schema to a <code>BaseToolT</code> type.</p> <p>By adding a docstring (if needed) and passing on fields and field information in dictionary format, a Pydantic <code>BaseModel</code> can be converted into an <code>BaseToolT</code> for performing extraction.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[BaseModel]</code> <p>The <code>BaseModel</code> schema to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseToolT]</code> <p>The constructed <code>BaseToolT</code> type.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def convert_base_model_to_tool(\n    schema: Type[BaseModel], base: Type[BaseToolT]\n) -&gt; Type[BaseToolT]:\n    \"\"\"Converts a `BaseModel` schema to a `BaseToolT` type.\n\n    By adding a docstring (if needed) and passing on fields and field information in\n    dictionary format, a Pydantic `BaseModel` can be converted into an `BaseToolT` for\n    performing extraction.\n\n    Args:\n        schema: The `BaseModel` schema to convert.\n\n    Returns:\n        The constructed `BaseToolT` type.\n    \"\"\"\n    field_definitions = {\n        field_name: (field_info.annotation, field_info)\n        for field_name, field_info in schema.model_fields.items()\n    }\n    return create_model(\n        f\"{schema.__name__}\",\n        __base__=base,\n        __doc__=schema.__doc__ if schema.__doc__ else DEFAULT_TOOL_DOCSTRING,\n        **cast(dict[str, Any], field_definitions),\n    )\n</code></pre>"},{"location":"api/base/utils/#mirascope.base.utils.convert_base_type_to_tool","title":"<code>convert_base_type_to_tool(schema, base)</code>","text":"<p>Converts a <code>BaseType</code> to a <code>BaseToolT</code> type.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def convert_base_type_to_tool(\n    schema: Type[BaseType], base: Type[BaseToolT]\n) -&gt; Type[BaseToolT]:\n    \"\"\"Converts a `BaseType` to a `BaseToolT` type.\"\"\"\n    if get_origin(schema) == Annotated:\n        schema.__name__ = get_args(schema)[0].__name__\n    return create_model(\n        f\"{schema.__name__.title()}\",\n        __base__=base,\n        __doc__=DEFAULT_TOOL_DOCSTRING,\n        value=(schema, ...),\n    )\n</code></pre>"},{"location":"api/base/utils/#mirascope.base.utils.convert_function_to_tool","title":"<code>convert_function_to_tool(fn, base)</code>","text":"<p>Constructs a <code>BaseToolT</code> type from the given function.</p> <p>This method expects all function parameters to be properly documented in identical order with identical variable names, as well as descriptions of each parameter. Errors will be raised if any of these conditions are not met.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to convert.</p> required <p>Returns:</p> Type Description <code>Type[BaseToolT]</code> <p>The constructed <code>BaseToolT</code> type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the given function doesn't have a docstring.</p> <code>ValueError</code> <p>if the given function's parameters don't have type annotations.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but the name doesn't match the docstring's parameter name.</p> <code>ValueError</code> <p>if a given function's parameter is in the docstring args section but doesn't have a dosctring description.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def convert_function_to_tool(fn: Callable, base: Type[BaseToolT]) -&gt; Type[BaseToolT]:\n    \"\"\"Constructs a `BaseToolT` type from the given function.\n\n    This method expects all function parameters to be properly documented in identical\n    order with identical variable names, as well as descriptions of each parameter.\n    Errors will be raised if any of these conditions are not met.\n\n    Args:\n        fn: The function to convert.\n\n    Returns:\n        The constructed `BaseToolT` type.\n\n    Raises:\n        ValueError: if the given function doesn't have a docstring.\n        ValueError: if the given function's parameters don't have type annotations.\n        ValueError: if a given function's parameter is in the docstring args section but\n            the name doesn't match the docstring's parameter name.\n        ValueError: if a given function's parameter is in the docstring args section but\n            doesn't have a dosctring description.\n    \"\"\"\n    if not fn.__doc__:\n        raise ValueError(\"Function must have a docstring.\")\n\n    docstring = parse(fn.__doc__)\n\n    doc = \"\"\n    if docstring.short_description:\n        doc = docstring.short_description\n    if docstring.long_description:\n        doc += \"\\n\\n\" + docstring.long_description\n    if docstring.returns and docstring.returns.description:\n        doc += \"\\n\\n\" + \"Returns:\\n    \" + docstring.returns.description\n\n    field_definitions = {}\n    hints = get_type_hints(fn)\n    for i, parameter in enumerate(signature(fn).parameters.values()):\n        if parameter.name == \"self\" or parameter.name == \"cls\":\n            continue\n        if parameter.annotation == Parameter.empty:\n            raise ValueError(\"All parameters must have a type annotation.\")\n\n        docstring_description = None\n        if i &lt; len(docstring.params):\n            docstring_param = docstring.params[i]\n            if docstring_param.arg_name != parameter.name:\n                raise ValueError(\n                    f\"Function parameter name {parameter.name} does not match docstring \"\n                    f\"parameter name {docstring_param.arg_name}. Make sure that the \"\n                    \"parameter names match exactly.\"\n                )\n            if not docstring_param.description:\n                raise ValueError(\"All parameters must have a description.\")\n            docstring_description = docstring_param.description\n\n        field_info = FieldInfo(annotation=hints[parameter.name])\n        if parameter.default != Parameter.empty:\n            field_info.default = parameter.default\n        if docstring_description:  # we check falsy here because this comes from docstr\n            field_info.description = docstring_description\n\n        param_name = parameter.name\n        if param_name.startswith(\"model_\"):  # model_ is a BaseModel reserved namespace\n            param_name = \"aliased_\" + param_name\n            field_info.alias = parameter.name\n            field_info.validation_alias = parameter.name\n            field_info.serialization_alias = parameter.name\n\n        field_definitions[param_name] = (\n            hints[parameter.name],\n            field_info,\n        )\n\n    model = create_model(\n        \"\".join(word.title() for word in fn.__name__.split(\"_\")),\n        __base__=base,\n        __doc__=doc,\n        **cast(dict[str, Any], field_definitions),\n    )\n    return tool_fn(fn)(model)\n</code></pre>"},{"location":"api/base/utils/#mirascope.base.utils.tool_fn","title":"<code>tool_fn(fn)</code>","text":"<p>A decorator for adding a function to a tool class.</p> <p>Adding this decorator will add an <code>fn</code> property to the tool class that returns the function that the tool describes. This is convenient for calling the function given an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to add to the tool class.</p> required <p>Returns:</p> Type Description <code>Callable[[Type[BaseToolT]], Type[BaseToolT]]</code> <p>The decorated tool class.</p> Source code in <code>mirascope/base/utils.py</code> <pre><code>def tool_fn(fn: Callable) -&gt; Callable[[Type[BaseToolT]], Type[BaseToolT]]:\n    \"\"\"A decorator for adding a function to a tool class.\n\n    Adding this decorator will add an `fn` property to the tool class that returns the\n    function that the tool describes. This is convenient for calling the function given\n    an instance of the tool.\n\n    Args:\n        fn: The function to add to the tool class.\n\n    Returns:\n        The decorated tool class.\n    \"\"\"\n\n    def decorator(cls: Type[BaseToolT]) -&gt; Type[BaseToolT]:\n        \"\"\"A decorator for adding a function to a tool class.\"\"\"\n        setattr(cls, \"fn\", property(lambda self: fn))\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/chroma/","title":"chroma","text":"<p>A module for interacting with Chroma vectorstores.</p>"},{"location":"api/chroma/types/","title":"chroma.types","text":"<p>Types for interacting with Chroma using Mirascope.</p>"},{"location":"api/chroma/types/#mirascope.chroma.types.ChromaSettings","title":"<code>ChromaSettings</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>mirascope/chroma/types.py</code> <pre><code>class ChromaSettings(BaseModel):\n    mode: Literal[\"http\", \"persistent\", \"ephemeral\"] = \"persistent\"\n    path: str = \"./chroma\"\n    host: str = \"localhost\"\n    port: int = 8000\n    ssl: bool = False\n    headers: Optional[dict[str, str]] = None\n    settings: Optional[Settings] = None\n    tenant: str = DEFAULT_TENANT\n    database: str = DEFAULT_DATABASE\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        if self.mode == \"http\":\n            exclude = {\"mode\", \"path\"}\n        elif self.mode == \"persistent\":\n            exclude = {\"mode\", \"host\", \"port\", \"ssl\", \"headers\"}\n        elif self.mode == \"ephemeral\":\n            exclude = {\"mode\", \"host\", \"port\", \"ssl\", \"headers\", \"path\"}\n        kwargs = {\n            key: value\n            for key, value in self.model_dump(exclude=exclude).items()\n            if value is not None\n        }\n        return kwargs\n</code></pre>"},{"location":"api/chroma/types/#mirascope.chroma.types.ChromaSettings.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/chroma/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    if self.mode == \"http\":\n        exclude = {\"mode\", \"path\"}\n    elif self.mode == \"persistent\":\n        exclude = {\"mode\", \"host\", \"port\", \"ssl\", \"headers\"}\n    elif self.mode == \"ephemeral\":\n        exclude = {\"mode\", \"host\", \"port\", \"ssl\", \"headers\", \"path\"}\n    kwargs = {\n        key: value\n        for key, value in self.model_dump(exclude=exclude).items()\n        if value is not None\n    }\n    return kwargs\n</code></pre>"},{"location":"api/chroma/vectorstores/","title":"chroma.vectorstores","text":"<p>A module for calling Chroma's Client and Collection.</p>"},{"location":"api/chroma/vectorstores/#mirascope.chroma.vectorstores.ChromaVectorStore","title":"<code>ChromaVectorStore</code>","text":"<p>             Bases: <code>BaseVectorStore</code></p> <p>A vectorstore for Chroma.</p> <p>Example:</p> <pre><code>from mirascope.chroma import ChromaSettings, ChromaVectorStore\nfrom mirascope.openai import OpenAIEmbedder\nfrom mirascope.rag import TextChunker\n\n\nclass MyStore(ChromaVectorStore):\n    embedder = OpenAIEmbedder()\n    chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n    index_name = \"my-store-0001\"\n    client_settings = ChromaSettings()\n\nmy_store = MyStore()\nwith open(f\"{PATH_TO_FILE}\") as file:\n    data = file.read()\n    my_store.add(data)\ndocuments = my_store.retrieve(\"my question\").documents\nprint(documents)\n</code></pre> Source code in <code>mirascope/chroma/vectorstores.py</code> <pre><code>class ChromaVectorStore(BaseVectorStore):\n    \"\"\"A vectorstore for Chroma.\n\n    Example:\n\n    ```python\n    from mirascope.chroma import ChromaSettings, ChromaVectorStore\n    from mirascope.openai import OpenAIEmbedder\n    from mirascope.rag import TextChunker\n\n\n    class MyStore(ChromaVectorStore):\n        embedder = OpenAIEmbedder()\n        chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n        index_name = \"my-store-0001\"\n        client_settings = ChromaSettings()\n\n    my_store = MyStore()\n    with open(f\"{PATH_TO_FILE}\") as file:\n        data = file.read()\n        my_store.add(data)\n    documents = my_store.retrieve(\"my question\").documents\n    print(documents)\n    ```\n    \"\"\"\n\n    vectorstore_params = ChromaParams(get_or_create=True)\n    client_settings: ClassVar[ChromaSettings] = ChromaSettings(mode=\"persistent\")\n\n    def retrieve(\n        self, text: Optional[Union[str, list[str]]] = None, **kwargs: Any\n    ) -&gt; ChromaQueryResult:\n        \"\"\"Queries the vectorstore for closest match\"\"\"\n        if text:\n            if isinstance(text, str):\n                text = [text]\n            query_result = self._index.query(query_texts=text, **kwargs)\n        else:\n            query_result = self._index.query(**kwargs)\n\n        return ChromaQueryResult.model_validate(query_result)\n\n    def add(self, text: Union[str, list[Document]], **kwargs: Any) -&gt; None:\n        \"\"\"Takes unstructured data and upserts into vectorstore\"\"\"\n        documents: list[Document]\n        if isinstance(text, str):\n            chunk = self.chunker.chunk\n            if self.vectorstore_params.weave is not None and not isinstance(\n                self.chunker, weave.Op\n            ):\n                chunk = self.vectorstore_params.weave(\n                    self.chunker.chunk\n                )  # pragma: no cover\n            documents = chunk(text)\n        else:\n            documents = text\n\n        return self._index.upsert(\n            ids=[document.id for document in documents],\n            documents=[document.text for document in documents],\n            **kwargs,\n        )\n\n    ############################# PRIVATE PROPERTIES #################################\n\n    @cached_property\n    def _client(self) -&gt; ClientAPI:\n        if self.client_settings.mode == \"persistent\":\n            return PersistentClient(**self.client_settings.kwargs())\n        elif self.client_settings.mode == \"http\":\n            return HttpClient(**self.client_settings.kwargs())\n        elif self.client_settings.mode == \"ephemeral\":\n            return EphemeralClient(**self.client_settings.kwargs())\n\n    @cached_property\n    def _index(self) -&gt; Collection:\n        vectorstore_params = self.vectorstore_params\n        if self.index_name:\n            vectorstore_params = self.vectorstore_params.model_copy(\n                update={\"name\": self.index_name}\n            )\n        create_collection = self._client.create_collection\n        if self.vectorstore_params.weave is not None:\n            create_collection = self.vectorstore_params.weave(\n                self._client.create_collection\n            )  # pragma: no cover\n\n        return create_collection(\n            **vectorstore_params.kwargs(),\n            embedding_function=self.embedder,  # type: ignore\n        )\n</code></pre>"},{"location":"api/chroma/vectorstores/#mirascope.chroma.vectorstores.ChromaVectorStore.add","title":"<code>add(text, **kwargs)</code>","text":"<p>Takes unstructured data and upserts into vectorstore</p> Source code in <code>mirascope/chroma/vectorstores.py</code> <pre><code>def add(self, text: Union[str, list[Document]], **kwargs: Any) -&gt; None:\n    \"\"\"Takes unstructured data and upserts into vectorstore\"\"\"\n    documents: list[Document]\n    if isinstance(text, str):\n        chunk = self.chunker.chunk\n        if self.vectorstore_params.weave is not None and not isinstance(\n            self.chunker, weave.Op\n        ):\n            chunk = self.vectorstore_params.weave(\n                self.chunker.chunk\n            )  # pragma: no cover\n        documents = chunk(text)\n    else:\n        documents = text\n\n    return self._index.upsert(\n        ids=[document.id for document in documents],\n        documents=[document.text for document in documents],\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/chroma/vectorstores/#mirascope.chroma.vectorstores.ChromaVectorStore.retrieve","title":"<code>retrieve(text=None, **kwargs)</code>","text":"<p>Queries the vectorstore for closest match</p> Source code in <code>mirascope/chroma/vectorstores.py</code> <pre><code>def retrieve(\n    self, text: Optional[Union[str, list[str]]] = None, **kwargs: Any\n) -&gt; ChromaQueryResult:\n    \"\"\"Queries the vectorstore for closest match\"\"\"\n    if text:\n        if isinstance(text, str):\n            text = [text]\n        query_result = self._index.query(query_texts=text, **kwargs)\n    else:\n        query_result = self._index.query(**kwargs)\n\n    return ChromaQueryResult.model_validate(query_result)\n</code></pre>"},{"location":"api/gemini/","title":"gemini","text":"<p>A module for interacting with Google's Gemini models.</p>"},{"location":"api/gemini/calls/","title":"gemini.calls","text":"<p>A module for calling Google's Gemini Chat API.</p>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall","title":"<code>GeminiCall</code>","text":"<p>             Bases: <code>BaseCall[GeminiCallResponse, GeminiCallResponseChunk, GeminiTool]</code></p> <p>A class for prompting Google's Gemini Chat API.</p> <p>This prompt supports the message types: USER, MODEL, TOOL</p> <p>Example:</p> <pre><code>from google.generativeai import configure  # type: ignore\nfrom mirascope.gemini import GeminiPrompt\n\nconfigure(api_key=\"YOUR_API_KEY\")\n\n\nclass BookRecommender(GeminiPrompt):\n    prompt_template = \"\"\"\n    USER: You're the world's greatest librarian.\n    MODEL: Ok, I understand I'm the world's greatest librarian. How can I help?\n    USER: Please recommend some {genre} books.\n\n    genre: str\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.call())\n#&gt; As the world's greatest librarian, I am delighted to recommend...\n</code></pre> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>class GeminiCall(BaseCall[GeminiCallResponse, GeminiCallResponseChunk, GeminiTool]):\n    '''A class for prompting Google's Gemini Chat API.\n\n    This prompt supports the message types: USER, MODEL, TOOL\n\n    Example:\n\n    ```python\n    from google.generativeai import configure  # type: ignore\n    from mirascope.gemini import GeminiPrompt\n\n    configure(api_key=\"YOUR_API_KEY\")\n\n\n    class BookRecommender(GeminiPrompt):\n        prompt_template = \"\"\"\n        USER: You're the world's greatest librarian.\n        MODEL: Ok, I understand I'm the world's greatest librarian. How can I help?\n        USER: Please recommend some {genre} books.\n\n        genre: str\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.call())\n    #&gt; As the world's greatest librarian, I am delighted to recommend...\n    ```\n    '''\n\n    call_params: ClassVar[GeminiCallParams] = GeminiCallParams()\n\n    def messages(self) -&gt; ContentsType:\n        \"\"\"Returns the `ContentsType` messages for Gemini `generate_content`.\n\n        Raises:\n            ValueError: if the docstring contains an unknown role.\n        \"\"\"\n        return [\n            {\"role\": message[\"role\"], \"parts\": [message[\"content\"]]}\n            for message in self._parse_messages(\n                [MessageRole.MODEL, MessageRole.USER, MessageRole.TOOL]\n            )\n        ]\n\n    def call(self, **kwargs: Any) -&gt; GeminiCallResponse:\n        \"\"\"Makes an call to the model using this `GeminiCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments that will be used for generating the\n                response. These will override any existing argument settings in call\n                params.\n\n        Returns:\n            A `GeminiCallResponse` instance.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        if self.call_params.weave is not None:\n            generate_content = self.call_params.weave(\n                gemini_pro_model.generate_content\n            )  # pragma: no cover\n        else:\n            generate_content = gemini_pro_model.generate_content\n        start_time = datetime.datetime.now().timestamp() * 1000\n        response = generate_content(\n            self.messages(),\n            stream=False,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        return GeminiCallResponse(\n            response=response,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=None,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; GeminiCallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `GeminiCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments that will be used for generating the\n                response. These will override any existing argument settings in call\n                params.\n\n        Returns:\n            A `GeminiCallResponse` instance.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        if self.call_params.weave is not None:\n            generate_content_async = self.call_params.weave(\n                gemini_pro_model.generate_content_async\n            )  # pragma: no cover\n        else:\n            generate_content_async = gemini_pro_model.generate_content_async\n        start_time = datetime.datetime.now().timestamp() * 1000\n        response = await generate_content_async(\n            self.messages(),\n            stream=False,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        return GeminiCallResponse(\n            response=response,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=None,\n        )\n\n    def stream(self, **kwargs: Any) -&gt; Generator[GeminiCallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `GeminiCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `GeminiCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        stream = gemini_pro_model.generate_content(\n            self.messages(),\n            stream=True,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        for chunk in stream:\n            yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[GeminiCallResponseChunk, None]:\n        \"\"\"Streams the response asynchronously for a call using this `GeminiCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `GeminiCallResponseChunk` for each chunk of the response.\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, GeminiTool)\n        gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n        stream = await gemini_pro_model.generate_content_async(\n            self.messages(),\n            stream=True,\n            tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes an call to the model using this <code>GeminiCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that will be used for generating the response. These will override any existing argument settings in call params.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeminiCallResponse</code> <p>A <code>GeminiCallResponse</code> instance.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; GeminiCallResponse:\n    \"\"\"Makes an call to the model using this `GeminiCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments that will be used for generating the\n            response. These will override any existing argument settings in call\n            params.\n\n    Returns:\n        A `GeminiCallResponse` instance.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    if self.call_params.weave is not None:\n        generate_content = self.call_params.weave(\n            gemini_pro_model.generate_content\n        )  # pragma: no cover\n    else:\n        generate_content = gemini_pro_model.generate_content\n    start_time = datetime.datetime.now().timestamp() * 1000\n    response = generate_content(\n        self.messages(),\n        stream=False,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    return GeminiCallResponse(\n        response=response,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=None,\n    )\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>GeminiCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that will be used for generating the response. These will override any existing argument settings in call params.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeminiCallResponse</code> <p>A <code>GeminiCallResponse</code> instance.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; GeminiCallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `GeminiCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments that will be used for generating the\n            response. These will override any existing argument settings in call\n            params.\n\n    Returns:\n        A `GeminiCallResponse` instance.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    if self.call_params.weave is not None:\n        generate_content_async = self.call_params.weave(\n            gemini_pro_model.generate_content_async\n        )  # pragma: no cover\n    else:\n        generate_content_async = gemini_pro_model.generate_content_async\n    start_time = datetime.datetime.now().timestamp() * 1000\n    response = await generate_content_async(\n        self.messages(),\n        stream=False,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    return GeminiCallResponse(\n        response=response,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=None,\n    )\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.messages","title":"<code>messages()</code>","text":"<p>Returns the <code>ContentsType</code> messages for Gemini <code>generate_content</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the docstring contains an unknown role.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>def messages(self) -&gt; ContentsType:\n    \"\"\"Returns the `ContentsType` messages for Gemini `generate_content`.\n\n    Raises:\n        ValueError: if the docstring contains an unknown role.\n    \"\"\"\n    return [\n        {\"role\": message[\"role\"], \"parts\": [message[\"content\"]]}\n        for message in self._parse_messages(\n            [MessageRole.MODEL, MessageRole.USER, MessageRole.TOOL]\n        )\n    ]\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>GeminiCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>GeminiCallResponseChunk</code> <p>A <code>GeminiCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>def stream(self, **kwargs: Any) -&gt; Generator[GeminiCallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `GeminiCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `GeminiCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    stream = gemini_pro_model.generate_content(\n        self.messages(),\n        stream=True,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    for chunk in stream:\n        yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/gemini/calls/#mirascope.gemini.calls.GeminiCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response asynchronously for a call using this <code>GeminiCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[GeminiCallResponseChunk, None]</code> <p>A <code>GeminiCallResponseChunk</code> for each chunk of the response.</p> Source code in <code>mirascope/gemini/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[GeminiCallResponseChunk, None]:\n    \"\"\"Streams the response asynchronously for a call using this `GeminiCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `GeminiCallResponseChunk` for each chunk of the response.\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, GeminiTool)\n    gemini_pro_model = GenerativeModel(model_name=kwargs.pop(\"model\"))\n    stream = await gemini_pro_model.generate_content_async(\n        self.messages(),\n        stream=True,\n        tools=kwargs.pop(\"tools\") if \"tools\" in kwargs else None,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield GeminiCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/gemini/extractors/","title":"gemini.extractors","text":""},{"location":"api/gemini/extractors/#mirascope.gemini.extractors.GeminiExtractor","title":"<code>GeminiExtractor</code>","text":"<p>             Bases: <code>BaseExtractor[GeminiCall, GeminiTool, Any, T]</code>, <code>Generic[T]</code></p> <p>A class for extracting structured information using Google's Gemini Chat models.</p> <p>Example:</p> <pre><code>from typing import Literal, Type\nfrom pydantic import BaseModel\nfrom mirascope.gemini import GeminiExtractor\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\nclass TaskExtractor(GeminiExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n\n    prompt_template = \"\"\"\n    USER: I need to extract task details.\n    MODEL: Sure, please provide the task description.\n    USER: {task}\n    \"\"\"\n\n    task: str\n\ntask_description = \"Prepare the budget report by next Monday. It's a high priority task.\"\ntask = TaskExtractor(task=task_description).extract(retries=3)\nassert isinstance(task, TaskDetails)\nprint(task)\n#&gt; title='Prepare the budget report' priority='high' due_date='next Monday'\n</code></pre> Source code in <code>mirascope/gemini/extractors.py</code> <pre><code>class GeminiExtractor(BaseExtractor[GeminiCall, GeminiTool, Any, T], Generic[T]):\n    '''A class for extracting structured information using Google's Gemini Chat models.\n\n    Example:\n\n    ```python\n    from typing import Literal, Type\n    from pydantic import BaseModel\n    from mirascope.gemini import GeminiExtractor\n\n    class TaskDetails(BaseModel):\n        title: str\n        priority: Literal[\"low\", \"normal\", \"high\"]\n        due_date: str\n\n    class TaskExtractor(GeminiExtractor[TaskDetails]):\n        extract_schema: Type[TaskDetails] = TaskDetails\n\n        prompt_template = \"\"\"\n        USER: I need to extract task details.\n        MODEL: Sure, please provide the task description.\n        USER: {task}\n        \"\"\"\n\n        task: str\n\n    task_description = \"Prepare the budget report by next Monday. It's a high priority task.\"\n    task = TaskExtractor(task=task_description).extract(retries=3)\n    assert isinstance(task, TaskDetails)\n    print(task)\n    #&gt; title='Prepare the budget report' priority='high' due_date='next Monday'\n    ```\n    '''\n\n    call_params: ClassVar[GeminiCallParams] = GeminiCallParams()\n\n    def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Extracts `extract_schema` from the Gemini call response.\n\n        The `extract_schema` is converted into a `GeminiTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Gemini's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            GeminiError: raises any Gemini errors.\n        \"\"\"\n        return self._extract(GeminiCall, GeminiTool, retries, **kwargs)\n\n    async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Asynchronously extracts `extract_schema` from the Gemini call response.\n\n        The `extract_schema` is converted into a `GeminiTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Gemini's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            GeminiError: raises any Gemini errors.\n        \"\"\"\n        return await self._extract_async(GeminiCall, GeminiTool, retries, **kwargs)\n</code></pre>"},{"location":"api/gemini/extractors/#mirascope.gemini.extractors.GeminiExtractor.extract","title":"<code>extract(retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the Gemini call response.</p> <p>The <code>extract_schema</code> is converted into a <code>GeminiTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Gemini's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>GeminiError</code> <p>raises any Gemini errors.</p> Source code in <code>mirascope/gemini/extractors.py</code> <pre><code>def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Extracts `extract_schema` from the Gemini call response.\n\n    The `extract_schema` is converted into a `GeminiTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Gemini's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        GeminiError: raises any Gemini errors.\n    \"\"\"\n    return self._extract(GeminiCall, GeminiTool, retries, **kwargs)\n</code></pre>"},{"location":"api/gemini/extractors/#mirascope.gemini.extractors.GeminiExtractor.extract_async","title":"<code>extract_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously extracts <code>extract_schema</code> from the Gemini call response.</p> <p>The <code>extract_schema</code> is converted into a <code>GeminiTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Gemini's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>GeminiError</code> <p>raises any Gemini errors.</p> Source code in <code>mirascope/gemini/extractors.py</code> <pre><code>async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Asynchronously extracts `extract_schema` from the Gemini call response.\n\n    The `extract_schema` is converted into a `GeminiTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Gemini's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        GeminiError: raises any Gemini errors.\n    \"\"\"\n    return await self._extract_async(GeminiCall, GeminiTool, retries, **kwargs)\n</code></pre>"},{"location":"api/gemini/tools/","title":"gemini.tools","text":"<p>Classes for using tools with Google's Gemini API.</p>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool","title":"<code>GeminiTool</code>","text":"<p>             Bases: <code>BaseTool[FunctionCall]</code></p> <p>A base class for easy use of tools with the Gemini API.</p> <p><code>GeminiTool</code> internally handles the logic that allows you to use tools with simple calls such as <code>GeminiCompletion.tool</code> or <code>GeminiTool.fn</code>, as seen in the examples below.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall, GeminiCallParams, GeminiTool\n\n\nclass CurrentWeather(GeminiTool):\n    \"\"\"A tool for getting the current weather in a location.\"\"\"\n\n    location: str\n\n\nclass WeatherForecast(GeminiPrompt):\n    prompt_template = \"What is the current weather in {city}?\"\n\n    city: str\n\n    call_params = GeminiCallParams(\n        model=\"gemini-pro\",\n        tools=[CurrentWeather],\n    )\n\n\nprompt = WeatherPrompt()\nforecast = WeatherForecast(city=\"Tokyo\").call().tool\nprint(forecast.location)\n#&gt; Tokyo\n</code></pre> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>class GeminiTool(BaseTool[FunctionCall]):\n    '''A base class for easy use of tools with the Gemini API.\n\n    `GeminiTool` internally handles the logic that allows you to use tools with simple\n    calls such as `GeminiCompletion.tool` or `GeminiTool.fn`, as seen in the\n    examples below.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall, GeminiCallParams, GeminiTool\n\n\n    class CurrentWeather(GeminiTool):\n        \"\"\"A tool for getting the current weather in a location.\"\"\"\n\n        location: str\n\n\n    class WeatherForecast(GeminiPrompt):\n        prompt_template = \"What is the current weather in {city}?\"\n\n        city: str\n\n        call_params = GeminiCallParams(\n            model=\"gemini-pro\",\n            tools=[CurrentWeather],\n        )\n\n\n    prompt = WeatherPrompt()\n    forecast = WeatherForecast(city=\"Tokyo\").call().tool\n    print(forecast.location)\n    #&gt; Tokyo\n    ```\n    '''\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a tool schema for use with the Gemini API.\n\n        A Mirascope `GeminiTool` is deconstructed into a `Tool` schema for use with the\n        Gemini API.\n\n        Returns:\n            The constructed `Tool` schema.\n        \"\"\"\n        tool_schema = super().tool_schema()\n        if \"parameters\" in tool_schema:\n            if \"$defs\" in tool_schema[\"parameters\"]:\n                raise ValueError(\n                    \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                    \"with $defs.\"\n                )\n            tool_schema[\"parameters\"][\"properties\"] = {\n                prop: {\n                    key: value for key, value in prop_schema.items() if key != \"title\"\n                }\n                for prop, prop_schema in tool_schema[\"parameters\"][\"properties\"].items()\n            }\n        return Tool(function_declarations=[FunctionDeclaration(**tool_schema)])\n\n    @classmethod\n    def from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given a `GenerateContentResponse` from a Gemini chat completion response, this\n        method extracts the tool call and constructs an instance of the tool.\n\n        Args:\n            tool_call: The `GenerateContentResponse` from which to extract the tool.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValueError: if the tool call doesn't have any arguments.\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        if not tool_call.args:\n            raise ValueError(\"Tool call doesn't have any arguments.\")\n        model_json = {key: value for key, value in tool_call.args.items()}\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, GeminiTool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, GeminiTool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[GeminiTool]:\n        \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a function.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>GeminiTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[GeminiTool]:\n    \"\"\"Constructs a `GeminiTool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, GeminiTool)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given a <code>GenerateContentResponse</code> from a Gemini chat completion response, this method extracts the tool call and constructs an instance of the tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>FunctionCall</code> <p>The <code>GenerateContentResponse</code> from which to extract the tool.</p> required <p>Returns:</p> Type Description <code>GeminiTool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the tool call doesn't have any arguments.</p> <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given a `GenerateContentResponse` from a Gemini chat completion response, this\n    method extracts the tool call and constructs an instance of the tool.\n\n    Args:\n        tool_call: The `GenerateContentResponse` from which to extract the tool.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValueError: if the tool call doesn't have any arguments.\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json = {key: value for key, value in tool_call.args.items()}\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/gemini/tools/#mirascope.gemini.tools.GeminiTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the Gemini API.</p> <p>A Mirascope <code>GeminiTool</code> is deconstructed into a <code>Tool</code> schema for use with the Gemini API.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>The constructed <code>Tool</code> schema.</p> Source code in <code>mirascope/gemini/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a tool schema for use with the Gemini API.\n\n    A Mirascope `GeminiTool` is deconstructed into a `Tool` schema for use with the\n    Gemini API.\n\n    Returns:\n        The constructed `Tool` schema.\n    \"\"\"\n    tool_schema = super().tool_schema()\n    if \"parameters\" in tool_schema:\n        if \"$defs\" in tool_schema[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n        tool_schema[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value for key, value in prop_schema.items() if key != \"title\"\n            }\n            for prop, prop_schema in tool_schema[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**tool_schema)])\n</code></pre>"},{"location":"api/gemini/types/","title":"gemini.types","text":"<p>Types for interacting with Google's Gemini models using Mirascope.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallParams","title":"<code>GeminiCallParams</code>","text":"<p>             Bases: <code>BaseCallParams[GeminiTool]</code></p> <p>The parameters to use when calling the Gemini API calls.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall, GeminiCallParams\n\n\nclass BookRecommendation(GeminiPrompt):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n    call_params = GeminiCallParams(\n        model=\"gemini-1.0-pro-001\",\n        generation_config={\"candidate_count\": 2},\n    )\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Name of the Wind\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallParams(BaseCallParams[GeminiTool]):\n    \"\"\"The parameters to use when calling the Gemini API calls.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall, GeminiCallParams\n\n\n    class BookRecommendation(GeminiPrompt):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n        call_params = GeminiCallParams(\n            model=\"gemini-1.0-pro-001\",\n            generation_config={\"candidate_count\": 2},\n        )\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Name of the Wind\n    ```\n    \"\"\"\n\n    model: str = \"gemini-1.0-pro\"\n    generation_config: Optional[dict[str, Any]] = {\"candidate_count\": 1}\n    safety_settings: Optional[Any] = None\n    request_options: Optional[dict[str, Any]] = None\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse","title":"<code>GeminiCallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[Union[GenerateContentResponse, AsyncGenerateContentResponse], GeminiTool]</code></p> <p>Convenience wrapper around Gemini's <code>GenerateContentResponse</code>.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.call()</code> will return a <code>GeminiCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiPrompt\n\n\nclass BookRecommender(GeminiPrompt):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Lord of the Rings\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallResponse(\n    BaseCallResponse[\n        Union[GenerateContentResponse, AsyncGenerateContentResponse], GeminiTool\n    ]\n):\n    \"\"\"Convenience wrapper around Gemini's `GenerateContentResponse`.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.call()` will return a\n    `GeminiCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiPrompt\n\n\n    class BookRecommender(GeminiPrompt):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Lord of the Rings\n    ```\n    \"\"\"\n\n    @property\n    def tools(self) -&gt; Optional[list[GeminiTool]]:\n        \"\"\"Returns the list of tools for the 0th candidate's 0th content part.\"\"\"\n        if self.tool_types is None:\n            return None\n\n        if self.response.candidates[0].finish_reason != 1:  # STOP = 1\n            raise RuntimeError(\n                \"Generation stopped before the stop sequence. \"\n                \"This is likely due to a limit on output tokens that is too low. \"\n                \"Note that this could also indicate no tool is beind called, so we \"\n                \"recommend that you check the output of the call to confirm.\"\n                f\"Finish Reason: {self.response.candidates[0].finish_reason}\"\n            )\n\n        tool_calls = [\n            part.function_call for part in self.response.candidates[0].content.parts\n        ]\n\n        extracted_tools = []\n        for tool_call in tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[GeminiTool]:\n        \"\"\"Returns the 0th tool for the 0th candidate's 0th content part.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.response.candidates[0].content.parts[0].text\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": str(self.response),\n            \"cost\": self.cost,\n        }\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the contained string content for the 0th choice.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.tool","title":"<code>tool: Optional[GeminiTool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.tools","title":"<code>tools: Optional[list[GeminiTool]]</code>  <code>property</code>","text":"<p>Returns the list of tools for the 0th candidate's 0th content part.</p>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/gemini/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": str(self.response),\n        \"cost\": self.cost,\n    }\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponseChunk","title":"<code>GeminiCallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[GenerateContentResponse, GeminiTool]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.stream()</code> will return a <code>GeminiCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall\n\n\nclass BookRecommender(GeminiCall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n\nfor chunk in BookRecommender(genre=\"science fiction\").stream():\n    print(chunk)\n\n#&gt; D\n#  u\n#\n#  ne\n#\n#  by F\n#  r\n#  an\n#  k\n#  .\n</code></pre> Source code in <code>mirascope/gemini/types.py</code> <pre><code>class GeminiCallResponseChunk(\n    BaseCallResponseChunk[GenerateContentResponse, GeminiTool]\n):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.stream()` will return a\n    `GeminiCallResponseChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall\n\n\n    class BookRecommender(GeminiCall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n\n    for chunk in BookRecommender(genre=\"science fiction\").stream():\n        print(chunk)\n\n    #&gt; D\n    #  u\n    #\n    #  ne\n    #\n    #  by F\n    #  r\n    #  an\n    #  k\n    #  .\n    ```\n    \"\"\"\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.chunk.candidates[0].content.parts[0].text\n</code></pre>"},{"location":"api/gemini/types/#mirascope.gemini.types.GeminiCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the chunk content for the 0th choice.</p>"},{"location":"api/mistral/","title":"mistral","text":"<p>A module for interacting with Mistral models.</p>"},{"location":"api/mistral/calls/","title":"mistral.calls","text":"<p>A module for prompting Mistral API.</p>"},{"location":"api/mistral/calls/#mirascope.mistral.calls.MistralCall","title":"<code>MistralCall</code>","text":"<p>             Bases: <code>BaseCall[MistralCallResponse, MistralCallResponseChunk, MistralTool]</code></p> <p>A class for\" prompting Mistral's chat API.</p> <p>Example:</p> <pre><code>from mirascope.mistral import MistralCall\n\nclass BookRecommender(MistralCall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; There are many great books to read, it ultimately depends...\n</code></pre> Source code in <code>mirascope/mistral/calls.py</code> <pre><code>class MistralCall(BaseCall[MistralCallResponse, MistralCallResponseChunk, MistralTool]):\n    \"\"\"A class for\" prompting Mistral's chat API.\n\n    Example:\n\n    ```python\n    from mirascope.mistral import MistralCall\n\n    class BookRecommender(MistralCall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; There are many great books to read, it ultimately depends...\n    ```\n    \"\"\"\n\n    call_params: ClassVar[MistralCallParams] = MistralCallParams()\n\n    def messages(self) -&gt; list[Message]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        return self._parse_messages(\n            [MessageRole.SYSTEM, MessageRole.USER, MessageRole.ASSISTANT]\n        )\n\n    def call(self, **kwargs: Any) -&gt; MistralCallResponse:\n        \"\"\"Makes a call to the model using this `MistralCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `MistralCallResponse` instance.\n\n        Raises:\n            MistralException: raises any Mistral errors, see:\n                https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, MistralTool)\n        client = MistralClient(\n            api_key=self.api_key,\n            endpoint=self.base_url if self.base_url else ENDPOINT,\n        )\n        if self.call_params.weave is not None:\n            chat = self.call_params.weave(client.chat)  # pragma: no cover\n        else:\n            chat = client.chat\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = chat(messages=self.messages(), **kwargs)\n        return MistralCallResponse(\n            response=completion,\n            tool_types=tool_types,\n            start_time=start_time,\n            cost=mistral_api_calculate_cost(completion.usage, completion.model),\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; MistralCallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `MistralCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `MistralCallResponse` instance.\n\n        Raises:\n            MistralException: raises any Mistral errors, see:\n                https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, MistralTool)\n        client = MistralAsyncClient(\n            api_key=self.api_key,\n            endpoint=self.base_url if self.base_url else ENDPOINT,\n        )\n        if self.call_params.weave is not None:\n            chat = self.call_params.weave(client.chat)  # pragma: no cover\n        else:\n            chat = client.chat\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = await chat(messages=self.messages(), **kwargs)\n        return MistralCallResponse(\n            response=completion,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=mistral_api_calculate_cost(completion.usage, completion.model),\n        )\n\n    def stream(self, **kwargs: Any) -&gt; Generator[MistralCallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `MistralCall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `MistralCallResponseChunk` for each chunk of the response.\n\n        Raises:\n            MistralException: raises any Mistral errors, see:\n                https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, MistralTool)\n        client = MistralClient(\n            api_key=self.api_key,\n            endpoint=self.base_url if self.base_url else ENDPOINT,\n        )\n        stream = client.chat_stream(messages=self.messages(), **kwargs)\n        for chunk in stream:\n            yield MistralCallResponseChunk(chunk=chunk, tool_types=tool_types)\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[MistralCallResponseChunk, None]:\n        \"\"\"Streams the response for an asynchronous call using this `MistralCall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `MistralCallResponseChunk` for each chunk of the response.\n\n        Raises:\n            MistralException: raises any Mistral errors, see:\n                https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n        \"\"\"\n        kwargs, tool_types = self._setup(kwargs, MistralTool)\n        client = MistralAsyncClient(\n            api_key=self.api_key,\n            endpoint=self.base_url if self.base_url else ENDPOINT,\n        )\n        stream = client.chat_stream(messages=self.messages(), **kwargs)\n        async for chunk in stream:\n            yield MistralCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/mistral/calls/#mirascope.mistral.calls.MistralCall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>MistralCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>MistralCallResponse</code> <p>A <code>MistralCallResponse</code> instance.</p> <p>Raises:</p> Type Description <code>MistralException</code> <p>raises any Mistral errors, see: https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py</p> Source code in <code>mirascope/mistral/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; MistralCallResponse:\n    \"\"\"Makes a call to the model using this `MistralCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `MistralCallResponse` instance.\n\n    Raises:\n        MistralException: raises any Mistral errors, see:\n            https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, MistralTool)\n    client = MistralClient(\n        api_key=self.api_key,\n        endpoint=self.base_url if self.base_url else ENDPOINT,\n    )\n    if self.call_params.weave is not None:\n        chat = self.call_params.weave(client.chat)  # pragma: no cover\n    else:\n        chat = client.chat\n    start_time = datetime.datetime.now().timestamp() * 1000\n    completion = chat(messages=self.messages(), **kwargs)\n    return MistralCallResponse(\n        response=completion,\n        tool_types=tool_types,\n        start_time=start_time,\n        cost=mistral_api_calculate_cost(completion.usage, completion.model),\n        end_time=datetime.datetime.now().timestamp() * 1000,\n    )\n</code></pre>"},{"location":"api/mistral/calls/#mirascope.mistral.calls.MistralCall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>MistralCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>MistralCallResponse</code> <p>A <code>MistralCallResponse</code> instance.</p> <p>Raises:</p> Type Description <code>MistralException</code> <p>raises any Mistral errors, see: https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py</p> Source code in <code>mirascope/mistral/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; MistralCallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `MistralCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `MistralCallResponse` instance.\n\n    Raises:\n        MistralException: raises any Mistral errors, see:\n            https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, MistralTool)\n    client = MistralAsyncClient(\n        api_key=self.api_key,\n        endpoint=self.base_url if self.base_url else ENDPOINT,\n    )\n    if self.call_params.weave is not None:\n        chat = self.call_params.weave(client.chat)  # pragma: no cover\n    else:\n        chat = client.chat\n    start_time = datetime.datetime.now().timestamp() * 1000\n    completion = await chat(messages=self.messages(), **kwargs)\n    return MistralCallResponse(\n        response=completion,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=mistral_api_calculate_cost(completion.usage, completion.model),\n    )\n</code></pre>"},{"location":"api/mistral/calls/#mirascope.mistral.calls.MistralCall.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/mistral/calls.py</code> <pre><code>def messages(self) -&gt; list[Message]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    return self._parse_messages(\n        [MessageRole.SYSTEM, MessageRole.USER, MessageRole.ASSISTANT]\n    )\n</code></pre>"},{"location":"api/mistral/calls/#mirascope.mistral.calls.MistralCall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>MistralCall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Generator[MistralCallResponseChunk, None, None]</code> <p>A <code>MistralCallResponseChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>MistralException</code> <p>raises any Mistral errors, see: https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py</p> Source code in <code>mirascope/mistral/calls.py</code> <pre><code>def stream(self, **kwargs: Any) -&gt; Generator[MistralCallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `MistralCall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `MistralCallResponseChunk` for each chunk of the response.\n\n    Raises:\n        MistralException: raises any Mistral errors, see:\n            https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, MistralTool)\n    client = MistralClient(\n        api_key=self.api_key,\n        endpoint=self.base_url if self.base_url else ENDPOINT,\n    )\n    stream = client.chat_stream(messages=self.messages(), **kwargs)\n    for chunk in stream:\n        yield MistralCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/mistral/calls/#mirascope.mistral.calls.MistralCall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response for an asynchronous call using this <code>MistralCall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[MistralCallResponseChunk, None]</code> <p>A <code>MistralCallResponseChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>MistralException</code> <p>raises any Mistral errors, see: https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py</p> Source code in <code>mirascope/mistral/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[MistralCallResponseChunk, None]:\n    \"\"\"Streams the response for an asynchronous call using this `MistralCall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `MistralCallResponseChunk` for each chunk of the response.\n\n    Raises:\n        MistralException: raises any Mistral errors, see:\n            https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n    \"\"\"\n    kwargs, tool_types = self._setup(kwargs, MistralTool)\n    client = MistralAsyncClient(\n        api_key=self.api_key,\n        endpoint=self.base_url if self.base_url else ENDPOINT,\n    )\n    stream = client.chat_stream(messages=self.messages(), **kwargs)\n    async for chunk in stream:\n        yield MistralCallResponseChunk(chunk=chunk, tool_types=tool_types)\n</code></pre>"},{"location":"api/mistral/extractors/","title":"mistral.extractors","text":"<p>A class for extracting structured information using Mistral chat models.</p>"},{"location":"api/mistral/extractors/#mirascope.mistral.extractors.MistralExtractor","title":"<code>MistralExtractor</code>","text":"<p>             Bases: <code>BaseExtractor[MistralCall, MistralTool, Any, T]</code>, <code>Generic[T]</code></p> <p>A class for extracting structured information using Mistral Chat models.</p> <p>Example:</p> <pre><code>from mirascope.mistral import MistralExtractor\nfrom pydantic import BaseModel\nfrom typing import Literal, Type\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\nclass TaskExtractor(MistralExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    call_params = MistralCallParams(model=\"mistral-large-latest\")\n\n    prompt_template = \"\"\"\n    Prepare the budget report by next Monday. It's a high priority task.\n    \"\"\"\n\n\ntask = TaskExtractor().extract(retries=3)\nassert isinstance(task, TaskDetails)\nprint(task)\n# &gt; title='Prepare the budget report' priority='high' due_date='next Monday'\n</code></pre> Source code in <code>mirascope/mistral/extractors.py</code> <pre><code>class MistralExtractor(BaseExtractor[MistralCall, MistralTool, Any, T], Generic[T]):\n    '''A class for extracting structured information using Mistral Chat models.\n\n    Example:\n\n    ```python\n    from mirascope.mistral import MistralExtractor\n    from pydantic import BaseModel\n    from typing import Literal, Type\n\n    class TaskDetails(BaseModel):\n        title: str\n        priority: Literal[\"low\", \"normal\", \"high\"]\n        due_date: str\n\n    class TaskExtractor(MistralExtractor[TaskDetails]):\n        extract_schema: Type[TaskDetails] = TaskDetails\n        call_params = MistralCallParams(model=\"mistral-large-latest\")\n\n        prompt_template = \"\"\"\n        Prepare the budget report by next Monday. It's a high priority task.\n        \"\"\"\n\n\n    task = TaskExtractor().extract(retries=3)\n    assert isinstance(task, TaskDetails)\n    print(task)\n    # &gt; title='Prepare the budget report' priority='high' due_date='next Monday'\n    ```\n    '''\n\n    call_params: ClassVar[MistralCallParams] = MistralCallParams()\n\n    def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Extracts `extract_schema` from the Mistral call response.\n\n        The `extract_schema` is converted into an `MistralTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Mistrals's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            MistralException: raises any Mistral exceptions, see:\n                https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n        \"\"\"\n        return self._extract(MistralCall, MistralTool, retries, **kwargs)\n\n    async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Asynchronously extracts `extract_schema` from the Mistral call response.\n\n        The `extract_schema` is converted into an `MistralTool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of Mistrals's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            MistralException: raises any Mistral exceptions, see:\n                https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n        \"\"\"\n        return await self._extract_async(MistralCall, MistralTool, retries, **kwargs)\n</code></pre>"},{"location":"api/mistral/extractors/#mirascope.mistral.extractors.MistralExtractor.extract","title":"<code>extract(retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the Mistral call response.</p> <p>The <code>extract_schema</code> is converted into an <code>MistralTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Mistrals's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>MistralException</code> <p>raises any Mistral exceptions, see: https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py</p> Source code in <code>mirascope/mistral/extractors.py</code> <pre><code>def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Extracts `extract_schema` from the Mistral call response.\n\n    The `extract_schema` is converted into an `MistralTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Mistrals's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        MistralException: raises any Mistral exceptions, see:\n            https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n    \"\"\"\n    return self._extract(MistralCall, MistralTool, retries, **kwargs)\n</code></pre>"},{"location":"api/mistral/extractors/#mirascope.mistral.extractors.MistralExtractor.extract_async","title":"<code>extract_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously extracts <code>extract_schema</code> from the Mistral call response.</p> <p>The <code>extract_schema</code> is converted into an <code>MistralTool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of Mistrals's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>Schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>MistralException</code> <p>raises any Mistral exceptions, see: https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py</p> Source code in <code>mirascope/mistral/extractors.py</code> <pre><code>async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Asynchronously extracts `extract_schema` from the Mistral call response.\n\n    The `extract_schema` is converted into an `MistralTool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of Mistrals's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        MistralException: raises any Mistral exceptions, see:\n            https://github.com/mistralai/client-python/blob/main/src/mistralai/exceptions.py\n    \"\"\"\n    return await self._extract_async(MistralCall, MistralTool, retries, **kwargs)\n</code></pre>"},{"location":"api/mistral/tools/","title":"mistral.tools","text":"<p>Classes for using tools with Mistral Chat APIs</p>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool","title":"<code>MistralTool</code>","text":"<p>             Bases: <code>BaseTool[ToolCall]</code></p> <p>A base class for easy use of tools with the Mistral client.</p> <p><code>MistralTool</code> internally handles the logic that allows you to use tools with simple calls such as <code>MistralCallResponse.tool</code> or <code>MistralTool.fn</code>, as seen in the  examples below.</p> <p>Example:</p> <p>```python import os</p> <p>from mirascope.mistral import MistralCall, MistralCallParams</p> <p>def animal_matcher(fav_food: str, fav_color: str) -&gt; str:     \"\"\"Tells you your most likely favorite animal from personality traits.</p> <pre><code>Args:\n    fav_food: your favorite food.\n    fav_color: your favorite color.\n\nReturns:\n    The animal most likely to be your favorite based on traits.\n\"\"\"\nreturn \"Your favorite animal is the best one, a frog.\"\n</code></pre> <p>class AnimalMatcher(MistralCall):     prompt_template = \"\"\"\\         Tell me my favorite animal if my favorite food is {food} and my         favorite color is {color}.     \"\"\"</p> <pre><code>food: str\ncolor: str\n\napi_key = os.getenv(\"MISTRAL_API_KEY\")\ncall_params = MistralCallParams(\n    model=\"mistral-large-latest\", tools=[animal_matcher]\n)\n</code></pre> <p>prompt = AnimalMatcher(food=\"pizza\", color=\"green\") response = prompt.call()</p> <p>if tools := response.tools:     for tool in tools:         print(tool.fn(**tool.args))</p>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool--your-favorite-animal-is-the-best-one-a-frog","title":"&gt; Your favorite animal is the best one, a frog.","text":"Source code in <code>mirascope/mistral/tools.py</code> <pre><code>class MistralTool(BaseTool[ToolCall]):\n    '''A base class for easy use of tools with the Mistral client.\n\n    `MistralTool` internally handles the logic that allows you to use tools with simple\n    calls such as `MistralCallResponse.tool` or `MistralTool.fn`, as seen in the \n    examples below.\n\n    Example:\n\n    ```python\n    import os\n\n    from mirascope.mistral import MistralCall, MistralCallParams\n\n\n    def animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n        \"\"\"Tells you your most likely favorite animal from personality traits.\n\n        Args:\n            fav_food: your favorite food.\n            fav_color: your favorite color.\n\n        Returns:\n            The animal most likely to be your favorite based on traits.\n        \"\"\"\n        return \"Your favorite animal is the best one, a frog.\"\n\n\n    class AnimalMatcher(MistralCall):\n        prompt_template = \"\"\"\\\\\n            Tell me my favorite animal if my favorite food is {food} and my\n            favorite color is {color}.\n        \"\"\"\n\n        food: str\n        color: str\n\n        api_key = os.getenv(\"MISTRAL_API_KEY\")\n        call_params = MistralCallParams(\n            model=\"mistral-large-latest\", tools=[animal_matcher]\n        )\n\n\n    prompt = AnimalMatcher(food=\"pizza\", color=\"green\")\n    response = prompt.call()\n\n    if tools := response.tools:\n        for tool in tools:\n            print(tool.fn(**tool.args))\n    #&gt; Your favorite animal is the best one, a frog.\n    '''\n\n    @classmethod\n    def tool_schema(cls) -&gt; dict[str, Any]:\n        \"\"\"Constructs a tool schema for use with the Mistral Chat client.\n\n        A Mirascope `MistralTool` is deconstructed into a JSON schema, and relevant keys\n        are renamed to match the Mistral API schema used to make functional/tool calls\n        in Mistral API.\n\n        Returns:\n            The constructed tool schema.\n        \"\"\"\n        fn = super().tool_schema()\n        return {\"type\": \"function\", \"function\": fn}\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given `ToolCall` from a Mistral chat completion response, takes its function\n        arguments and creates a `MistralTool` instance from it.\n\n        Args:\n            tool_call: The Mistral `ToolCall` to extract the tool from.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValueError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        try:\n            model_json = json.loads(tool_call.function.arguments)\n        except json.JSONDecodeError as e:\n            raise ValueError() from e\n\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[MistralTool]:\n        \"\"\"Constructs a `MistralTool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, MistralTool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[MistralTool]:\n        \"\"\"Constructs a `MistralTool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, MistralTool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[MistralTool]:\n        \"\"\"Constructs a `MistralTool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, MistralTool)\n</code></pre>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>MistralTool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/mistral/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[MistralTool]:\n    \"\"\"Constructs a `MistralTool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, MistralTool)\n</code></pre>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>MistralTool</code> type from a function.</p> Source code in <code>mirascope/mistral/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[MistralTool]:\n    \"\"\"Constructs a `MistralTool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, MistralTool)\n</code></pre>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>MistralTool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/mistral/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[MistralTool]:\n    \"\"\"Constructs a `MistralTool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, MistralTool)\n</code></pre>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given <code>ToolCall</code> from a Mistral chat completion response, takes its function arguments and creates a <code>MistralTool</code> instance from it.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ToolCall</code> <p>The Mistral <code>ToolCall</code> to extract the tool from.</p> required <p>Returns:</p> Type Description <code>MistralTool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/mistral/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given `ToolCall` from a Mistral chat completion response, takes its function\n    arguments and creates a `MistralTool` instance from it.\n\n    Args:\n        tool_call: The Mistral `ToolCall` to extract the tool from.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValueError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    try:\n        model_json = json.loads(tool_call.function.arguments)\n    except json.JSONDecodeError as e:\n        raise ValueError() from e\n\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/mistral/tools/#mirascope.mistral.tools.MistralTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the Mistral Chat client.</p> <p>A Mirascope <code>MistralTool</code> is deconstructed into a JSON schema, and relevant keys are renamed to match the Mistral API schema used to make functional/tool calls in Mistral API.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The constructed tool schema.</p> Source code in <code>mirascope/mistral/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; dict[str, Any]:\n    \"\"\"Constructs a tool schema for use with the Mistral Chat client.\n\n    A Mirascope `MistralTool` is deconstructed into a JSON schema, and relevant keys\n    are renamed to match the Mistral API schema used to make functional/tool calls\n    in Mistral API.\n\n    Returns:\n        The constructed tool schema.\n    \"\"\"\n    fn = super().tool_schema()\n    return {\"type\": \"function\", \"function\": fn}\n</code></pre>"},{"location":"api/mistral/types/","title":"mistral.types","text":"<p>Types for working with Mistral prompts.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallParams","title":"<code>MistralCallParams</code>","text":"<p>             Bases: <code>BaseCallParams[MistralTool]</code></p> <p>The parameters to use when calling the Mistral API.</p> Source code in <code>mirascope/mistral/types.py</code> <pre><code>class MistralCallParams(BaseCallParams[MistralTool]):\n    \"\"\"The parameters to use when calling the Mistral API.\"\"\"\n\n    model: str = \"open-mixtral-8x7b\"\n    endpoint: Optional[str] = None\n    temperature: Optional[float] = None\n    max_tokens: Optional[int] = None\n    top_p: Optional[float] = None\n    random_seed: Optional[int] = None\n    safe_mode: Optional[bool] = None\n    safe_prompt: Optional[bool] = None\n    tool_choice: Optional[ToolChoice] = None\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse","title":"<code>MistralCallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[ChatCompletionResponse, MistralTool]</code></p> <p>Convenience wrapper for Mistral's chat model completions.</p> <p>When using Mirascope's convenience wrappers to interact with Mistral models via <code>MistralCall</code>, responses using <code>MistralCall.call()</code> will return a <code>MistralCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.mistral import MistralCall\n\nclass BookRecommender(MistralCall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\nresponse = Bookrecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Name of the Wind\n\nprint(response.message)\n#&gt; ChatMessage(content='The Name of the Wind', role='assistant',\n#  function_call=None, tool_calls=None)\n\nprint(response.choices)\n#&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n#  message=ChatMessage(content='The Name of the Wind', role='assistant',\n#  function_call=None, tool_calls=None))]\n</code></pre> Source code in <code>mirascope/mistral/types.py</code> <pre><code>class MistralCallResponse(BaseCallResponse[ChatCompletionResponse, MistralTool]):\n    \"\"\"Convenience wrapper for Mistral's chat model completions.\n\n    When using Mirascope's convenience wrappers to interact with Mistral models via\n    `MistralCall`, responses using `MistralCall.call()` will return a\n    `MistralCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.mistral import MistralCall\n\n    class BookRecommender(MistralCall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n    response = Bookrecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Name of the Wind\n\n    print(response.message)\n    #&gt; ChatMessage(content='The Name of the Wind', role='assistant',\n    #  function_call=None, tool_calls=None)\n\n    print(response.choices)\n    #&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n    #  message=ChatMessage(content='The Name of the Wind', role='assistant',\n    #  function_call=None, tool_calls=None))]\n    ```\n\n    \"\"\"\n\n    @property\n    def choices(self) -&gt; list[ChatCompletionResponseChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; ChatCompletionResponseChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"The content of the chat completion for the 0th choice.\"\"\"\n        content = self.message.content\n        # We haven't seen the `list[str]` response type in practice, so for now we\n        # return the first item in the list\n        return content if isinstance(content, str) else content[0]\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ToolCall]]:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @property\n    def tools(self) -&gt; Optional[list[MistralTool]]:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls or len(self.tool_calls) == 0:\n            return None\n\n        if self.choices[0].finish_reason != \"tool_calls\":\n            raise RuntimeError(\n                \"Finish reason was not `tool_call`, indicating no or failed tool use.\"\n                \"This is likely due to a limit on output tokens that is too low. \"\n                \"Note that this could also indicate no tool is beind called, so we \"\n                \"recommend that you check the output of the call to confirm. \"\n                f\"Finish Reason: {self.choices[0].finish_reason}\"\n            )\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[MistralTool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": self.response.model_dump(),\n            \"cost\": self.cost,\n        }\n</code></pre>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.choice","title":"<code>choice: ChatCompletionResponseChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.choices","title":"<code>choices: list[ChatCompletionResponseChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>The content of the chat completion for the 0th choice.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.message","title":"<code>message: ChatMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.tool","title":"<code>tool: Optional[MistralTool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.tool_calls","title":"<code>tool_calls: Optional[list[ToolCall]]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.tools","title":"<code>tools: Optional[list[MistralTool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/mistral/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": self.response.model_dump(),\n        \"cost\": self.cost,\n    }\n</code></pre>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponseChunk","title":"<code>MistralCallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[ChatCompletionStreamResponse, MistralTool]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Mistral models via <code>MistralCall.stream</code>, responses will return an <code>MistralCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.mistral import MistralCall\n\n\nclass Math(MistralCall):\n    prompt_template = \"What is 1 + 2?\"\n\n\nfor chunk in MistralCall().stream():\n    print(chunk.content)\n\n#&gt; 1\n#  +\n#  2\n#   equals\n#\n#  3\n#  .\n</code></pre> Source code in <code>mirascope/mistral/types.py</code> <pre><code>class MistralCallResponseChunk(\n    BaseCallResponseChunk[ChatCompletionStreamResponse, MistralTool]\n):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Mistral models via\n    `MistralCall.stream`, responses will return an `MistralCallResponseChunk`, whereby\n    the implemented properties allow for simpler syntax and a convenient developer\n    experience.\n\n    Example:\n\n    ```python\n    from mirascope.mistral import MistralCall\n\n\n    class Math(MistralCall):\n        prompt_template = \"What is 1 + 2?\"\n\n\n    for chunk in MistralCall().stream():\n        print(chunk.content)\n\n    #&gt; 1\n    #  +\n    #  2\n    #   equals\n    #\n    #  3\n    #  .\n    ```\n    \"\"\"\n\n    @property\n    def choices(self) -&gt; list[ChatCompletionResponseStreamChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChatCompletionResponseStreamChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def delta(self) -&gt; DeltaMessage:\n        \"\"\"Returns the delta of the 0th choice.\"\"\"\n        return self.choice.delta\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the delta.\"\"\"\n        return self.delta.content if self.delta.content is not None else \"\"\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ToolCall]]:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\"\"\"\n        return self.delta.tool_calls\n</code></pre>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponseChunk.choice","title":"<code>choice: ChatCompletionResponseStreamChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponseChunk.choices","title":"<code>choices: list[ChatCompletionResponseStreamChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the delta.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponseChunk.delta","title":"<code>delta: DeltaMessage</code>  <code>property</code>","text":"<p>Returns the delta of the 0th choice.</p>"},{"location":"api/mistral/types/#mirascope.mistral.types.MistralCallResponseChunk.tool_calls","title":"<code>tool_calls: Optional[list[ToolCall]]</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p>"},{"location":"api/openai/","title":"openai","text":"<p>A module for interacting with OpenAI models.</p>"},{"location":"api/openai/calls/","title":"openai.calls","text":"<p>A module for calling OpenAI's Chat Completion models.</p>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall","title":"<code>OpenAICall</code>","text":"<p>             Bases: <code>BaseCall[OpenAICallResponse, OpenAICallResponseChunk, OpenAITool]</code></p> <p>A base class for calling OpenAI's Chat Completion models.</p> <p>Example:</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; There are many great books to read, it ultimately depends...\n</code></pre> Source code in <code>mirascope/openai/calls.py</code> <pre><code>class OpenAICall(BaseCall[OpenAICallResponse, OpenAICallResponseChunk, OpenAITool]):\n    \"\"\"A base class for calling OpenAI's Chat Completion models.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall\n\n\n    class BookRecommender(OpenAICall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n    response = BookRecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; There are many great books to read, it ultimately depends...\n    ```\n    \"\"\"\n\n    call_params: ClassVar[OpenAICallParams] = OpenAICallParams()\n\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        \"\"\"Returns the template as a formatted list of messages.\"\"\"\n        message_type_by_role = {\n            MessageRole.SYSTEM: ChatCompletionSystemMessageParam,\n            MessageRole.USER: ChatCompletionUserMessageParam,\n            MessageRole.ASSISTANT: ChatCompletionAssistantMessageParam,\n            MessageRole.TOOL: ChatCompletionToolMessageParam,\n        }\n        return [\n            message_type_by_role[MessageRole(message[\"role\"])](\n                role=message[\"role\"], content=message[\"content\"]\n            )\n            for message in self._parse_messages(list(message_type_by_role.keys()))\n        ]\n\n    def call(self, **kwargs: Any) -&gt; OpenAICallResponse:\n        \"\"\"Makes a call to the model using this `OpenAICall` instance.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            A `OpenAICallResponse` instance.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n        client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        messages = self._update_messages_if_json(self.messages(), tool_types)\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = client.chat.completions.create(\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n        return OpenAICallResponse(\n            response=completion,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=openai_api_calculate_cost(completion.usage, completion.model),\n            response_format=self.call_params.response_format,\n        )\n\n    async def call_async(self, **kwargs: Any) -&gt; OpenAICallResponse:\n        \"\"\"Makes an asynchronous call to the model using this `OpenAICall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            An `OpenAICallResponse` instance.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        messages = self._update_messages_if_json(self.messages(), tool_types)\n        start_time = datetime.datetime.now().timestamp() * 1000\n        completion = await client.chat.completions.create(\n            messages=messages,\n            stream=False,\n            **kwargs,\n        )\n        return OpenAICallResponse(\n            response=completion,\n            tool_types=tool_types,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n            cost=openai_api_calculate_cost(completion.usage, completion.model),\n            response_format=self.call_params.response_format,\n        )\n\n    def stream(self, **kwargs: Any) -&gt; Generator[OpenAICallResponseChunk, None, None]:\n        \"\"\"Streams the response for a call using this `OpenAICall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `OpenAICallResponseChunk` for each chunk of the response.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n        client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper is not None:\n            client = self.call_params.wrapper(client)\n        messages = self._update_messages_if_json(self.messages(), tool_types)\n        stream = client.chat.completions.create(\n            messages=messages,\n            stream=True,\n            **kwargs,\n        )\n        for chunk in stream:\n            yield OpenAICallResponseChunk(\n                chunk=chunk,\n                tool_types=tool_types,\n                response_format=self.call_params.response_format,\n            )\n\n    async def stream_async(\n        self, **kwargs: Any\n    ) -&gt; AsyncGenerator[OpenAICallResponseChunk, None]:\n        \"\"\"Streams the response for an asynchronous call using this `OpenAICall`.\n\n        Args:\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            A `OpenAICallResponseChunk` for each chunk of the response.\n\n        Raises:\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n        if self.call_params.wrapper_async is not None:\n            client = self.call_params.wrapper_async(client)\n        messages = self._update_messages_if_json(self.messages(), tool_types)\n        stream = await client.chat.completions.create(\n            messages=messages,\n            stream=True,\n            **kwargs,\n        )\n        async for chunk in stream:\n            yield OpenAICallResponseChunk(\n                chunk=chunk,\n                tool_types=tool_types,\n                response_format=self.call_params.response_format,\n            )\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _setup_openai_kwargs(\n        self,\n        kwargs: dict[str, Any],\n    ) -&gt; tuple[\n        dict[str, Any],\n        Optional[list[Type[OpenAITool]]],\n    ]:\n        \"\"\"Overrides the `BaseCall._setup` for Anthropic specific setup.\"\"\"\n        kwargs, tool_types = self._setup(kwargs, OpenAITool)\n        if (\n            self.call_params.response_format == ResponseFormat(type=\"json_object\")\n            and tool_types\n        ):\n            kwargs.pop(\"tools\")\n        return kwargs, tool_types\n\n    def _update_messages_if_json(\n        self,\n        messages: list[ChatCompletionMessageParam],\n        tool_types: Optional[list[type[OpenAITool]]],\n    ) -&gt; list[ChatCompletionMessageParam]:\n        if (\n            self.call_params.response_format == ResponseFormat(type=\"json_object\")\n            and tool_types\n        ):\n            messages.append(\n                ChatCompletionUserMessageParam(\n                    role=\"user\", content=_json_mode_content(tool_type=tool_types[0])\n                )\n            )\n        return messages\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.call","title":"<code>call(**kwargs)</code>","text":"<p>Makes a call to the model using this <code>OpenAICall</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAICallResponse</code> <p>A <code>OpenAICallResponse</code> instance.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>def call(self, **kwargs: Any) -&gt; OpenAICallResponse:\n    \"\"\"Makes a call to the model using this `OpenAICall` instance.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        A `OpenAICallResponse` instance.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n    client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    messages = self._update_messages_if_json(self.messages(), tool_types)\n    start_time = datetime.datetime.now().timestamp() * 1000\n    completion = client.chat.completions.create(\n        messages=messages,\n        stream=False,\n        **kwargs,\n    )\n    return OpenAICallResponse(\n        response=completion,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=openai_api_calculate_cost(completion.usage, completion.model),\n        response_format=self.call_params.response_format,\n    )\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.call_async","title":"<code>call_async(**kwargs)</code>  <code>async</code>","text":"<p>Makes an asynchronous call to the model using this <code>OpenAICall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>OpenAICallResponse</code> <p>An <code>OpenAICallResponse</code> instance.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>async def call_async(self, **kwargs: Any) -&gt; OpenAICallResponse:\n    \"\"\"Makes an asynchronous call to the model using this `OpenAICall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        An `OpenAICallResponse` instance.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n    client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    messages = self._update_messages_if_json(self.messages(), tool_types)\n    start_time = datetime.datetime.now().timestamp() * 1000\n    completion = await client.chat.completions.create(\n        messages=messages,\n        stream=False,\n        **kwargs,\n    )\n    return OpenAICallResponse(\n        response=completion,\n        tool_types=tool_types,\n        start_time=start_time,\n        end_time=datetime.datetime.now().timestamp() * 1000,\n        cost=openai_api_calculate_cost(completion.usage, completion.model),\n        response_format=self.call_params.response_format,\n    )\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.messages","title":"<code>messages()</code>","text":"<p>Returns the template as a formatted list of messages.</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>def messages(self) -&gt; list[ChatCompletionMessageParam]:\n    \"\"\"Returns the template as a formatted list of messages.\"\"\"\n    message_type_by_role = {\n        MessageRole.SYSTEM: ChatCompletionSystemMessageParam,\n        MessageRole.USER: ChatCompletionUserMessageParam,\n        MessageRole.ASSISTANT: ChatCompletionAssistantMessageParam,\n        MessageRole.TOOL: ChatCompletionToolMessageParam,\n    }\n    return [\n        message_type_by_role[MessageRole(message[\"role\"])](\n            role=message[\"role\"], content=message[\"content\"]\n        )\n        for message in self._parse_messages(list(message_type_by_role.keys()))\n    ]\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.stream","title":"<code>stream(**kwargs)</code>","text":"<p>Streams the response for a call using this <code>OpenAICall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>OpenAICallResponseChunk</code> <p>A <code>OpenAICallResponseChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>def stream(self, **kwargs: Any) -&gt; Generator[OpenAICallResponseChunk, None, None]:\n    \"\"\"Streams the response for a call using this `OpenAICall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `OpenAICallResponseChunk` for each chunk of the response.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n    client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper is not None:\n        client = self.call_params.wrapper(client)\n    messages = self._update_messages_if_json(self.messages(), tool_types)\n    stream = client.chat.completions.create(\n        messages=messages,\n        stream=True,\n        **kwargs,\n    )\n    for chunk in stream:\n        yield OpenAICallResponseChunk(\n            chunk=chunk,\n            tool_types=tool_types,\n            response_format=self.call_params.response_format,\n        )\n</code></pre>"},{"location":"api/openai/calls/#mirascope.openai.calls.OpenAICall.stream_async","title":"<code>stream_async(**kwargs)</code>  <code>async</code>","text":"<p>Streams the response for an asynchronous call using this <code>OpenAICall</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[OpenAICallResponseChunk, None]</code> <p>A <code>OpenAICallResponseChunk</code> for each chunk of the response.</p> <p>Raises:</p> Type Description <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/calls.py</code> <pre><code>async def stream_async(\n    self, **kwargs: Any\n) -&gt; AsyncGenerator[OpenAICallResponseChunk, None]:\n    \"\"\"Streams the response for an asynchronous call using this `OpenAICall`.\n\n    Args:\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        A `OpenAICallResponseChunk` for each chunk of the response.\n\n    Raises:\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    kwargs, tool_types = self._setup_openai_kwargs(kwargs)\n    client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n    if self.call_params.wrapper_async is not None:\n        client = self.call_params.wrapper_async(client)\n    messages = self._update_messages_if_json(self.messages(), tool_types)\n    stream = await client.chat.completions.create(\n        messages=messages,\n        stream=True,\n        **kwargs,\n    )\n    async for chunk in stream:\n        yield OpenAICallResponseChunk(\n            chunk=chunk,\n            tool_types=tool_types,\n            response_format=self.call_params.response_format,\n        )\n</code></pre>"},{"location":"api/openai/embedders/","title":"openai.embedders","text":"<p>A module for calling OpenAI's Embeddings models.</p>"},{"location":"api/openai/embedders/#mirascope.openai.embedders.OpenAIEmbedder","title":"<code>OpenAIEmbedder</code>","text":"<p>             Bases: <code>BaseEmbedder[EmbeddingResponse]</code></p> <p>OpenAI Embedder</p> <p>Example:</p> <pre><code>import os\nfrom mirascope.openai import OpenAIEmbedder\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nopenai_embedder = OpenAIEmbedder()\nresponse = openai_embedder.embed([\"your text to embed\"])\nprint(response)\n</code></pre> Source code in <code>mirascope/openai/embedders.py</code> <pre><code>class OpenAIEmbedder(BaseEmbedder[EmbeddingResponse]):\n    \"\"\"OpenAI Embedder\n\n    Example:\n\n    ```python\n    import os\n    from mirascope.openai import OpenAIEmbedder\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n    openai_embedder = OpenAIEmbedder()\n    response = openai_embedder.embed([\"your text to embed\"])\n    print(response)\n    ```\n    \"\"\"\n\n    dimensions: Optional[int] = 1536\n    embedding_params: ClassVar[OpenAIEmbeddingParams] = OpenAIEmbeddingParams(\n        model=\"text-embedding-ada-002\"\n    )\n\n    def embed(self, inputs: list[str]) -&gt; list[EmbeddingResponse]:\n        \"\"\"Call the embedder with multiple inputs\"\"\"\n        embedding_responses: list[EmbeddingResponse] = []\n        for input in inputs:\n            embedding_responses.append(self._embed(input))\n        return embedding_responses\n\n    async def embed_async(self, inputs: list[str]) -&gt; list[EmbeddingResponse]:\n        \"\"\"Asynchronously call the embedder with multiple inputs\"\"\"\n        embedding_responses: list[EmbeddingResponse] = await asyncio.gather(\n            *[self._embed_async(input) for input in inputs]\n        )\n        return embedding_responses\n\n    def __call__(self, input: list[str]) -&gt; list[list[float]]:\n        \"\"\"Call the embedder with a input\n\n        Chroma expects parameter to be `input`.\n        \"\"\"\n        embeddings: list[Embedding] = [\n            embedding\n            for response in self.embed(input)\n            for embedding in response.response.data\n        ]\n        sorted_embeddings = sorted(embeddings, key=lambda e: e.index)\n        return [result.embedding for result in sorted_embeddings]\n\n    ############################## PRIVATE METHODS ###################################\n\n    def _embed(self, input: str) -&gt; EmbeddingResponse:\n        \"\"\"Call the embedder with a single input\"\"\"\n        client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n        embedding_params = self.embedding_params.model_copy(update={\"input\": input})\n        kwargs = embedding_params.kwargs()\n        if self.embedding_params.model != \"text-embedding-ada-002\":\n            kwargs[\"dimensions\"] = self.dimensions\n        start_time = datetime.datetime.now().timestamp() * 1000\n        embeddings = client.embeddings.create(**kwargs)\n        return EmbeddingResponse(\n            response=embeddings,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n\n    async def _embed_async(self, input: str) -&gt; EmbeddingResponse:\n        \"\"\"Asynchronously call the embedder with a single input\"\"\"\n        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)\n        embedding_params = self.embedding_params.model_copy(update={\"input\": input})\n        kwargs = embedding_params.kwargs()\n        if self.embedding_params.model != \"text-embedding-ada-002\":\n            kwargs[\"dimensions\"] = self.dimensions\n        start_time = datetime.datetime.now().timestamp() * 1000\n        embeddings = await client.embeddings.create(**kwargs)\n        return EmbeddingResponse(\n            response=embeddings,\n            start_time=start_time,\n            end_time=datetime.datetime.now().timestamp() * 1000,\n        )\n</code></pre>"},{"location":"api/openai/embedders/#mirascope.openai.embedders.OpenAIEmbedder.__call__","title":"<code>__call__(input)</code>","text":"<p>Call the embedder with a input</p> <p>Chroma expects parameter to be <code>input</code>.</p> Source code in <code>mirascope/openai/embedders.py</code> <pre><code>def __call__(self, input: list[str]) -&gt; list[list[float]]:\n    \"\"\"Call the embedder with a input\n\n    Chroma expects parameter to be `input`.\n    \"\"\"\n    embeddings: list[Embedding] = [\n        embedding\n        for response in self.embed(input)\n        for embedding in response.response.data\n    ]\n    sorted_embeddings = sorted(embeddings, key=lambda e: e.index)\n    return [result.embedding for result in sorted_embeddings]\n</code></pre>"},{"location":"api/openai/embedders/#mirascope.openai.embedders.OpenAIEmbedder.embed","title":"<code>embed(inputs)</code>","text":"<p>Call the embedder with multiple inputs</p> Source code in <code>mirascope/openai/embedders.py</code> <pre><code>def embed(self, inputs: list[str]) -&gt; list[EmbeddingResponse]:\n    \"\"\"Call the embedder with multiple inputs\"\"\"\n    embedding_responses: list[EmbeddingResponse] = []\n    for input in inputs:\n        embedding_responses.append(self._embed(input))\n    return embedding_responses\n</code></pre>"},{"location":"api/openai/embedders/#mirascope.openai.embedders.OpenAIEmbedder.embed_async","title":"<code>embed_async(inputs)</code>  <code>async</code>","text":"<p>Asynchronously call the embedder with multiple inputs</p> Source code in <code>mirascope/openai/embedders.py</code> <pre><code>async def embed_async(self, inputs: list[str]) -&gt; list[EmbeddingResponse]:\n    \"\"\"Asynchronously call the embedder with multiple inputs\"\"\"\n    embedding_responses: list[EmbeddingResponse] = await asyncio.gather(\n        *[self._embed_async(input) for input in inputs]\n    )\n    return embedding_responses\n</code></pre>"},{"location":"api/openai/extractors/","title":"openai.extractors","text":"<p>A class for extracting structured information using OpenAI chat models.</p>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor","title":"<code>OpenAIExtractor</code>","text":"<p>             Bases: <code>BaseExtractor[OpenAICall, OpenAITool, OpenAIToolStream, T]</code>, <code>Generic[T]</code></p> <p>A class for extracting structured information using OpenAI chat models.</p> <p>Example:</p> <pre><code>from typing import Literal, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n\n    prompt_template = \"\"\"\n    Please extract the task details:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask_description = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask = TaskExtractor(task=task_description).extract(retries=3)\nassert isinstance(task, TaskDetails)\nprint(task)\n#&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n</code></pre> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>class OpenAIExtractor(\n    BaseExtractor[OpenAICall, OpenAITool, OpenAIToolStream, T], Generic[T]\n):\n    '''A class for extracting structured information using OpenAI chat models.\n\n    Example:\n\n    ```python\n    from typing import Literal, Type\n\n    from mirascope.openai import OpenAIExtractor\n    from pydantic import BaseModel\n\n\n    class TaskDetails(BaseModel):\n        title: str\n        priority: Literal[\"low\", \"normal\", \"high\"]\n        due_date: str\n\n\n    class TaskExtractor(OpenAIExtractor[TaskDetails]):\n        extract_schema: Type[TaskDetails] = TaskDetails\n\n        prompt_template = \"\"\"\n        Please extract the task details:\n        {task}\n        \"\"\"\n\n        task: str\n\n\n    task_description = \"Submit quarterly report by next Friday. Task is high priority.\"\n    task = TaskExtractor(task=task_description).extract(retries=3)\n    assert isinstance(task, TaskDetails)\n    print(task)\n    #&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n    ```\n    '''\n\n    call_params: ClassVar[OpenAICallParams] = OpenAICallParams()\n\n    def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Extracts `extract_schema` from the OpenAI call response.\n\n        The `extract_schema` is converted into an `OpenAITool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of OpenAI's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `extract_schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        return self._extract(OpenAICall, OpenAITool, retries, **kwargs)\n\n    async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n        \"\"\"Asynchronously extracts `extract_schema` from the OpenAI call response.\n\n        The `extract_schema` is converted into an `OpenAITool`, complete with a\n        description of the tool, all of the fields, and their types. This allows us to\n        take advantage of OpenAI's tool/function calling functionality to extract\n        information from a prompt according to the context provided by the `BaseModel`\n        schema.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `extract_schema` instance extracted from the completion.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        return await self._extract_async(OpenAICall, OpenAITool, retries, **kwargs)\n\n    def stream(self, retries: int = 0, **kwargs: Any) -&gt; Generator[T, None, None]:\n        \"\"\"Streams partial instances of `extract_schema` as the schema is streamed.\n\n        The `extract_schema` is converted into a `partial(OpenAITool)`, which allows for\n        any field (i.e.function argument) in the tool to be `None`. This allows us to\n        stream partial results as we construct the tool from the streamed chunks.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword argument parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            The partial `extract_schema` instance from the current buffer.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        yield from self._stream(\n            OpenAICall, OpenAITool, OpenAIToolStream, retries, **kwargs\n        )\n\n    async def stream_async(\n        self, retries: int = 0, **kwargs: Any\n    ) -&gt; AsyncGenerator[T, None]:\n        \"\"\"Asynchronously streams partial instances of `extract_schema` as streamed.\n\n        The `extract_schema` is converted into a `partial(OpenAITool)`, which allows for\n        any field (i.e.function argument) in the tool to be `None`. This allows us to\n        stream partial results as we construct the tool from the streamed chunks.\n\n        Args:\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Yields:\n            The partial `extract_schema` instance from the current buffer.\n\n        Raises:\n            AttributeError: if there is no tool in the call creation.\n            ValidationError: if the schema cannot be instantiated from the completion.\n            OpenAIError: raises any OpenAI errors, see:\n                https://platform.openai.com/docs/guides/error-codes/api-errors\n        \"\"\"\n        async for partial_tool in self._stream_async(\n            OpenAICall, OpenAITool, OpenAIToolStream, retries, **kwargs\n        ):\n            yield partial_tool\n</code></pre>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor.extract","title":"<code>extract(retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the OpenAI call response.</p> <p>The <code>extract_schema</code> is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>extract_schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>def extract(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Extracts `extract_schema` from the OpenAI call response.\n\n    The `extract_schema` is converted into an `OpenAITool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of OpenAI's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `extract_schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    return self._extract(OpenAICall, OpenAITool, retries, **kwargs)\n</code></pre>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor.extract_async","title":"<code>extract_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously extracts <code>extract_schema</code> from the OpenAI call response.</p> <p>The <code>extract_schema</code> is converted into an <code>OpenAITool</code>, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of OpenAI's tool/function calling functionality to extract information from a prompt according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The <code>extract_schema</code> instance extracted from the completion.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>async def extract_async(self, retries: int = 0, **kwargs: Any) -&gt; T:\n    \"\"\"Asynchronously extracts `extract_schema` from the OpenAI call response.\n\n    The `extract_schema` is converted into an `OpenAITool`, complete with a\n    description of the tool, all of the fields, and their types. This allows us to\n    take advantage of OpenAI's tool/function calling functionality to extract\n    information from a prompt according to the context provided by the `BaseModel`\n    schema.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `extract_schema` instance extracted from the completion.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    return await self._extract_async(OpenAICall, OpenAITool, retries, **kwargs)\n</code></pre>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor.stream","title":"<code>stream(retries=0, **kwargs)</code>","text":"<p>Streams partial instances of <code>extract_schema</code> as the schema is streamed.</p> <p>The <code>extract_schema</code> is converted into a <code>partial(OpenAITool)</code>, which allows for any field (i.e.function argument) in the tool to be <code>None</code>. This allows us to stream partial results as we construct the tool from the streamed chunks.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword argument parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>T</code> <p>The partial <code>extract_schema</code> instance from the current buffer.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>def stream(self, retries: int = 0, **kwargs: Any) -&gt; Generator[T, None, None]:\n    \"\"\"Streams partial instances of `extract_schema` as the schema is streamed.\n\n    The `extract_schema` is converted into a `partial(OpenAITool)`, which allows for\n    any field (i.e.function argument) in the tool to be `None`. This allows us to\n    stream partial results as we construct the tool from the streamed chunks.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword argument parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        The partial `extract_schema` instance from the current buffer.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    yield from self._stream(\n        OpenAICall, OpenAITool, OpenAIToolStream, retries, **kwargs\n    )\n</code></pre>"},{"location":"api/openai/extractors/#mirascope.openai.extractors.OpenAIExtractor.stream_async","title":"<code>stream_async(retries=0, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously streams partial instances of <code>extract_schema</code> as streamed.</p> <p>The <code>extract_schema</code> is converted into a <code>partial(OpenAITool)</code>, which allows for any field (i.e.function argument) in the tool to be <code>None</code>. This allows us to stream partial results as we construct the tool from the streamed chunks.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[T, None]</code> <p>The partial <code>extract_schema</code> instance from the current buffer.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>if there is no tool in the call creation.</p> <code>ValidationError</code> <p>if the schema cannot be instantiated from the completion.</p> <code>OpenAIError</code> <p>raises any OpenAI errors, see: https://platform.openai.com/docs/guides/error-codes/api-errors</p> Source code in <code>mirascope/openai/extractors.py</code> <pre><code>async def stream_async(\n    self, retries: int = 0, **kwargs: Any\n) -&gt; AsyncGenerator[T, None]:\n    \"\"\"Asynchronously streams partial instances of `extract_schema` as streamed.\n\n    The `extract_schema` is converted into a `partial(OpenAITool)`, which allows for\n    any field (i.e.function argument) in the tool to be `None`. This allows us to\n    stream partial results as we construct the tool from the streamed chunks.\n\n    Args:\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Yields:\n        The partial `extract_schema` instance from the current buffer.\n\n    Raises:\n        AttributeError: if there is no tool in the call creation.\n        ValidationError: if the schema cannot be instantiated from the completion.\n        OpenAIError: raises any OpenAI errors, see:\n            https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n    async for partial_tool in self._stream_async(\n        OpenAICall, OpenAITool, OpenAIToolStream, retries, **kwargs\n    ):\n        yield partial_tool\n</code></pre>"},{"location":"api/openai/tools/","title":"openai.tools","text":"<p>Classes for using tools with OpenAI Chat APIs.</p>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>             Bases: <code>BaseTool[ChatCompletionMessageToolCall]</code></p> <p>A base class for easy use of tools with the OpenAI Chat client.</p> <p><code>OpenAITool</code> internally handles the logic that allows you to use tools with simple calls such as <code>OpenAICallResponse.tool</code> or <code>OpenAITool.fn</code>, as seen in the examples below.</p> <p>Example:</p> <pre><code>from mirascope.openai import OpenAICall, OpenAICallParams\n\n\ndef animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n    \"\"\"Tells you your most likely favorite animal from personality traits.\n\n    Args:\n        fav_food: your favorite food.\n        fav_color: your favorite color.\n\n    Returns:\n        The animal most likely to be your favorite based on traits.\n    \"\"\"\n    return \"Your favorite animal is the best one, a frog.\"\n\n\nclass AnimalMatcher(OpenAICall):\n    prompt_template = \"\"\"\n    Tell me my favorite animal if my favorite food is {food} and my\n    favorite color is {color}.\n    \"\"\"\n\n    food: str\n    color: str\n\n    call_params = OpenAICallParams(tools=[animal_matcher])\n\n\nresponse = AnimalMatcher(food=\"pizza\", color=\"red\").call\ntool = response.tool\nprint(tool.fn(**tool.args))\n#&gt; Your favorite animal is the best one, a frog.\n</code></pre> Source code in <code>mirascope/openai/tools.py</code> <pre><code>class OpenAITool(BaseTool[ChatCompletionMessageToolCall]):\n    '''A base class for easy use of tools with the OpenAI Chat client.\n\n    `OpenAITool` internally handles the logic that allows you to use tools with simple\n    calls such as `OpenAICallResponse.tool` or `OpenAITool.fn`, as seen in the\n    examples below.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall, OpenAICallParams\n\n\n    def animal_matcher(fav_food: str, fav_color: str) -&gt; str:\n        \"\"\"Tells you your most likely favorite animal from personality traits.\n\n        Args:\n            fav_food: your favorite food.\n            fav_color: your favorite color.\n\n        Returns:\n            The animal most likely to be your favorite based on traits.\n        \"\"\"\n        return \"Your favorite animal is the best one, a frog.\"\n\n\n    class AnimalMatcher(OpenAICall):\n        prompt_template = \"\"\"\n        Tell me my favorite animal if my favorite food is {food} and my\n        favorite color is {color}.\n        \"\"\"\n\n        food: str\n        color: str\n\n        call_params = OpenAICallParams(tools=[animal_matcher])\n\n\n    response = AnimalMatcher(food=\"pizza\", color=\"red\").call\n    tool = response.tool\n    print(tool.fn(**tool.args))\n    #&gt; Your favorite animal is the best one, a frog.\n    ```\n    '''\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n        A Mirascope `OpenAITool` is deconstructed into a JSON schema, and relevant keys\n        are renamed to match the OpenAI `ChatCompletionToolParam` schema used to make\n        function/tool calls in OpenAI API.\n\n        Returns:\n            The constructed `ChatCompletionToolParam` schema.\n        \"\"\"\n        fn = super().tool_schema()\n        return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n\n    @classmethod\n    def from_tool_call(\n        cls,\n        tool_call: ChatCompletionMessageToolCall,\n        allow_partial: bool = False,\n    ) -&gt; OpenAITool:\n        \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n        Given `ChatCompletionMessageToolCall` from an OpenAI chat completion response,\n        takes its function arguments and creates an `OpenAITool` instance from it.\n\n        Args:\n            tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n        Returns:\n            An instance of the tool constructed from the tool call.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool schema.\n        \"\"\"\n        if allow_partial:\n            model_json = from_json(tool_call.function.arguments, allow_partial=True)\n        else:\n            try:\n                model_json = json.loads(tool_call.function.arguments)\n            except json.JSONDecodeError as e:\n                raise ValueError() from e\n\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n\n    @classmethod\n    def from_model(cls, model: Type[BaseModel]) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a `BaseModel` type.\"\"\"\n        return convert_base_model_to_tool(model, OpenAITool)\n\n    @classmethod\n    def from_fn(cls, fn: Callable) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a function.\"\"\"\n        return convert_function_to_tool(fn, OpenAITool)\n\n    @classmethod\n    def from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[OpenAITool]:\n        \"\"\"Constructs a `OpenAITool` type from a `BaseType` type.\"\"\"\n        return convert_base_type_to_tool(base_type, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_base_type","title":"<code>from_base_type(base_type)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a <code>BaseType</code> type.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_base_type(cls, base_type: Type[BaseType]) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a `BaseType` type.\"\"\"\n    return convert_base_type_to_tool(base_type, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_fn","title":"<code>from_fn(fn)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a function.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_fn(cls, fn: Callable) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a function.\"\"\"\n    return convert_function_to_tool(fn, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Constructs a <code>OpenAITool</code> type from a <code>BaseModel</code> type.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_model(cls, model: Type[BaseModel]) -&gt; Type[OpenAITool]:\n    \"\"\"Constructs a `OpenAITool` type from a `BaseModel` type.\"\"\"\n    return convert_base_model_to_tool(model, OpenAITool)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call, allow_partial=False)</code>  <code>classmethod</code>","text":"<p>Extracts an instance of the tool constructed from a tool call response.</p> <p>Given <code>ChatCompletionMessageToolCall</code> from an OpenAI chat completion response, takes its function arguments and creates an <code>OpenAITool</code> instance from it.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The <code>ChatCompletionMessageToolCall</code> to extract the tool from.</p> required <p>Returns:</p> Type Description <code>OpenAITool</code> <p>An instance of the tool constructed from the tool call.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool schema.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef from_tool_call(\n    cls,\n    tool_call: ChatCompletionMessageToolCall,\n    allow_partial: bool = False,\n) -&gt; OpenAITool:\n    \"\"\"Extracts an instance of the tool constructed from a tool call response.\n\n    Given `ChatCompletionMessageToolCall` from an OpenAI chat completion response,\n    takes its function arguments and creates an `OpenAITool` instance from it.\n\n    Args:\n        tool_call: The `ChatCompletionMessageToolCall` to extract the tool from.\n\n    Returns:\n        An instance of the tool constructed from the tool call.\n\n    Raises:\n        ValidationError: if the tool call doesn't match the tool schema.\n    \"\"\"\n    if allow_partial:\n        model_json = from_json(tool_call.function.arguments, allow_partial=True)\n    else:\n        try:\n            model_json = json.loads(tool_call.function.arguments)\n        except json.JSONDecodeError as e:\n            raise ValueError() from e\n\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/openai/tools/#mirascope.openai.tools.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a tool schema for use with the OpenAI Chat client.</p> <p>A Mirascope <code>OpenAITool</code> is deconstructed into a JSON schema, and relevant keys are renamed to match the OpenAI <code>ChatCompletionToolParam</code> schema used to make function/tool calls in OpenAI API.</p> <p>Returns:</p> Type Description <code>ChatCompletionToolParam</code> <p>The constructed <code>ChatCompletionToolParam</code> schema.</p> Source code in <code>mirascope/openai/tools.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a tool schema for use with the OpenAI Chat client.\n\n    A Mirascope `OpenAITool` is deconstructed into a JSON schema, and relevant keys\n    are renamed to match the OpenAI `ChatCompletionToolParam` schema used to make\n    function/tool calls in OpenAI API.\n\n    Returns:\n        The constructed `ChatCompletionToolParam` schema.\n    \"\"\"\n    fn = super().tool_schema()\n    return cast(ChatCompletionToolParam, {\"type\": \"function\", \"function\": fn})\n</code></pre>"},{"location":"api/openai/types/","title":"openai.types","text":"<p>Types for interacting with OpenAI models using Mirascope.</p>"},{"location":"api/openai/types/#mirascope.openai.types.EmbeddingResponse","title":"<code>EmbeddingResponse</code>","text":"<p>             Bases: <code>BaseEmbeddingResponse[CreateEmbeddingResponse]</code></p> <p>A convenience wrapper around the OpenAI <code>CreateEmbeddingResponse</code> response.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>class EmbeddingResponse(BaseEmbeddingResponse[CreateEmbeddingResponse]):\n    \"\"\"A convenience wrapper around the OpenAI `CreateEmbeddingResponse` response.\"\"\"\n\n    @property\n    def embedding(self) -&gt; list[float]:\n        \"\"\"Returns the embedding for the 0th choice.\"\"\"\n        return self.response.data[0].embedding\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.EmbeddingResponse.embedding","title":"<code>embedding: list[float]</code>  <code>property</code>","text":"<p>Returns the embedding for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>             Bases: <code>BaseCallParams[OpenAITool]</code></p> <p>The parameters to use when calling the OpenAI API.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallParams(BaseCallParams[OpenAITool]):\n    \"\"\"The parameters to use when calling the OpenAI API.\"\"\"\n\n    model: str = \"gpt-3.5-turbo-0125\"\n    frequency_penalty: Optional[float] = None\n    logit_bias: Optional[dict[str, int]] = None\n    logprobs: Optional[bool] = None\n    max_tokens: Optional[int] = None\n    n: Optional[int] = None\n    presence_penalty: Optional[float] = None\n    response_format: Optional[ResponseFormat] = None\n    seed: Optional[int] = None\n    stop: Union[Optional[str], list[str]] = None\n    temperature: Optional[float] = None\n    tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None\n    top_logprobs: Optional[int] = None\n    top_p: Optional[float] = None\n    user: Optional[str] = None\n    # Values defined below take precedence over values defined elsewhere. Use these\n    # params to pass additional parameters to the API if necessary that aren't already\n    # available as params.\n    extra_headers: Optional[Headers] = None\n    extra_query: Optional[Query] = None\n    extra_body: Optional[Body] = None\n    timeout: Optional[Union[float, Timeout]] = None\n\n    wrapper: Optional[Callable[[OpenAI], OpenAI]] = None\n    wrapper_async: Optional[Callable[[AsyncOpenAI], AsyncOpenAI]] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def kwargs(\n        self,\n        tool_type: Optional[Type[OpenAITool]] = OpenAITool,\n        exclude: Optional[set[str]] = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Returns the keyword argument call parameters.\"\"\"\n        extra_exclude = {\"wrapper\", \"wrapper_async\"}\n        exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n        return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallParams.kwargs","title":"<code>kwargs(tool_type=OpenAITool, exclude=None)</code>","text":"<p>Returns the keyword argument call parameters.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def kwargs(\n    self,\n    tool_type: Optional[Type[OpenAITool]] = OpenAITool,\n    exclude: Optional[set[str]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the keyword argument call parameters.\"\"\"\n    extra_exclude = {\"wrapper\", \"wrapper_async\"}\n    exclude = extra_exclude if exclude is None else exclude.union(extra_exclude)\n    return super().kwargs(tool_type, exclude)\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse","title":"<code>OpenAICallResponse</code>","text":"<p>             Bases: <code>BaseCallResponse[ChatCompletion, OpenAITool]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletion</code> response.</p> <p>When using Mirascope's convenience wrappers to interact with OpenAI models via <code>OpenAICall</code>, responses using <code>OpenAICall.call()</code> will return a <code>OpenAICallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend a {genre} book\"\n\n    genre: str\n\n\nresponse = Bookrecommender(genre=\"fantasy\").call()\nprint(response.content)\n#&gt; The Name of the Wind\n\nprint(response.message)\n#&gt; ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n#  function_call=None, tool_calls=None)\n\nprint(response.choices)\n#&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n#  message=ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n#  function_call=None, tool_calls=None))]\n</code></pre> Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallResponse(BaseCallResponse[ChatCompletion, OpenAITool]):\n    \"\"\"A convenience wrapper around the OpenAI `ChatCompletion` response.\n\n    When using Mirascope's convenience wrappers to interact with OpenAI models via\n    `OpenAICall`, responses using `OpenAICall.call()` will return a\n    `OpenAICallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall\n\n\n    class BookRecommender(OpenAICall):\n        prompt_template = \"Please recommend a {genre} book\"\n\n        genre: str\n\n\n    response = Bookrecommender(genre=\"fantasy\").call()\n    print(response.content)\n    #&gt; The Name of the Wind\n\n    print(response.message)\n    #&gt; ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n    #  function_call=None, tool_calls=None)\n\n    print(response.choices)\n    #&gt; [Choice(finish_reason='stop', index=0, logprobs=None,\n    #  message=ChatCompletionMessage(content='The Name of the Wind', role='assistant',\n    #  function_call=None, tool_calls=None))]\n    ```\n    \"\"\"\n\n    response_format: Optional[ResponseFormat] = None\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChatCompletionMessageToolCall]]:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @property\n    def tools(self) -&gt; Optional[list[OpenAITool]]:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types:\n            return None\n\n        if self.response_format != ResponseFormat(type=\"json_object\"):\n            if not self.tool_calls:\n                return None\n\n            if self.choices[0].finish_reason not in [\"tool_calls\", \"function_call\"]:\n                raise RuntimeError(\n                    \"Finish reason was not `tool_calls` or `function_call`, indicating \"\n                    \"no or failed tool use. This is likely due to a limit on output \"\n                    \"tokens that is too low. Note that this could also indicate no \"\n                    \"tool is beind called, so we recommend that you check the output \"\n                    \"of the call to confirm. \"\n                    f\"Finish Reason: {self.choices[0].finish_reason}\"\n                )\n        else:\n            # Note: we only handle single tool calls in JSON mode.\n            tool_type = self.tool_types[0]\n            return [\n                tool_type.from_tool_call(\n                    ChatCompletionMessageToolCall(\n                        id=\"id\",\n                        function=Function(\n                            name=tool_type.__name__, arguments=self.content\n                        ),\n                        type=\"function\",\n                    )\n                )\n            ]\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type.__name__:\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @property\n    def tool(self) -&gt; Optional[OpenAITool]:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the response to a dictionary.\"\"\"\n        return {\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time,\n            \"output\": self.response.model_dump(),\n            \"cost\": self.cost,\n        }\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.tool","title":"<code>tool: Optional[OpenAITool]</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.tool_calls","title":"<code>tool_calls: Optional[list[ChatCompletionMessageToolCall]]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.tools","title":"<code>tools: Optional[list[OpenAITool]]</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponse.dump","title":"<code>dump()</code>","text":"<p>Dumps the response to a dictionary.</p> Source code in <code>mirascope/openai/types.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the response to a dictionary.\"\"\"\n    return {\n        \"start_time\": self.start_time,\n        \"end_time\": self.end_time,\n        \"output\": self.response.model_dump(),\n        \"cost\": self.cost,\n    }\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk","title":"<code>OpenAICallResponseChunk</code>","text":"<p>             Bases: <code>BaseCallResponseChunk[ChatCompletionChunk, OpenAITool]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with OpenAI models via <code>OpenAICall.stream</code>, responses will return an <code>OpenAICallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <p>```python from mirascope.openai import OpenAICall</p> <p>class Math(OpenAICall):     prompt_template = \"What is 1 + 2?\"</p> <p>for chunk in OpenAICall().stream():     print(chunk.content)</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--1","title":"&gt; 1","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--_1","title":"+","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--2","title":"2","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--equals","title":"equals","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--_2","title":"types","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--3","title":"3","text":""},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk--_3","title":".","text":"Source code in <code>mirascope/openai/types.py</code> <pre><code>class OpenAICallResponseChunk(BaseCallResponseChunk[ChatCompletionChunk, OpenAITool]):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with OpenAI models via\n    `OpenAICall.stream`, responses will return an `OpenAICallResponseChunk`, whereby\n    the implemented properties allow for simpler syntax and a convenient developer\n    experience.\n\n    Example:\n\n    ```python\n    from mirascope.openai import OpenAICall\n\n\n    class Math(OpenAICall):\n        prompt_template = \"What is 1 + 2?\"\n\n\n    for chunk in OpenAICall().stream():\n        print(chunk.content)\n\n    #&gt; 1\n    #  +\n    #  2\n    #   equals\n    #\n    #  3\n    #  .\n    \"\"\"\n\n    response_format: Optional[ResponseFormat] = None\n\n    @property\n    def choices(self) -&gt; list[ChunkChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChunkChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        return self.choices[0].delta\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return self.delta.content if self.delta.content is not None else \"\"\n\n    @property\n    def tool_calls(self) -&gt; Optional[list[ChoiceDeltaToolCall]]:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        return self.delta.tool_calls\n</code></pre>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.choice","title":"<code>choice: ChunkChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.choices","title":"<code>choices: list[ChunkChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.delta","title":"<code>delta: ChoiceDelta</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/openai/types/#mirascope.openai.types.OpenAICallResponseChunk.tool_calls","title":"<code>tool_calls: Optional[list[ChoiceDeltaToolCall]]</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/pinecone/","title":"pinecone","text":"<p>A module for interacting with Pinecone vectorstores.</p>"},{"location":"api/pinecone/types/","title":"pinecone.types","text":"<p>Types for interacting with Pinecone using Mirascope.</p>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeParams","title":"<code>PineconeParams</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The parameters for Pinecone create_index</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class PineconeParams(BaseModel):\n    \"\"\"The parameters for Pinecone create_index\"\"\"\n\n    metric: Optional[Literal[\"cosine\", \"dotproduct\", \"euclidean\"]] = \"cosine\"\n    timeout: Optional[int] = None\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        kwargs = {\n            key: value for key, value in self.model_dump().items() if value is not None\n        }\n        return kwargs\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeParams.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    kwargs = {\n        key: value for key, value in self.model_dump().items() if value is not None\n    }\n    return kwargs\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconePodParams","title":"<code>PineconePodParams</code>","text":"<p>             Bases: <code>PineconeParams</code>, <code>PodSpec</code>, <code>BaseVectorStoreParams</code></p> <p>The parameters for Pinecone create_index with pod spec and weave</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class PineconePodParams(PineconeParams, PodSpec, BaseVectorStoreParams):\n    \"\"\"The parameters for Pinecone create_index with pod spec and weave\"\"\"\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        pod_kwargs = PodSpec(**self.model_dump()).kwargs()\n        pinecone_kwargs = PineconeParams(**self.model_dump()).kwargs()\n        # print(pinecone_kwargs, serverless_kwargs)\n        return {**pinecone_kwargs, \"spec\": {**pod_kwargs}}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconePodParams.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    pod_kwargs = PodSpec(**self.model_dump()).kwargs()\n    pinecone_kwargs = PineconeParams(**self.model_dump()).kwargs()\n    # print(pinecone_kwargs, serverless_kwargs)\n    return {**pinecone_kwargs, \"spec\": {**pod_kwargs}}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeQueryResult","title":"<code>PineconeQueryResult</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The result of a Pinecone index query</p> <p>Example:</p> <pre><code>from mirascope.pinecone import (\n    PineconeServerlessParams,\n    PineconeSettings,\n    PineconeVectorStore,\n)\nfrom mirascope.openai import OpenAIEmbedder\nfrom mirascope.rag import TextChunker\n\n\nclass MyStore(ChromaVectorStore):\n    embedder = OpenAIEmbedder(dimensions=1536)\n    chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n    index_name = \"my-store-0001\"\n    api_key = settings.pinecone_api_key\n    client_settings = PineconeSettings()\n    vectorstore_params = PineconeServerlessParams(\n        cloud=\"aws\",\n        region=\"us-west-2\",\n    )\n\nmy_store = MyStore()\nwith open(f\"{PATH_TO_FILE}\") as file:\n    data = file.read()\n    my_store.add(data)\nquery_results = my_store.retrieve(\"my question\")\n#&gt; QueryResult(ids=['0'], documents=['my answer'],\n# scores=[0.9999999999999999], embeddings=[[0.0, 0.0, 0.0, ...]])\n</code></pre> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class PineconeQueryResult(BaseModel):\n    \"\"\"The result of a Pinecone index query\n\n    Example:\n\n    ```python\n    from mirascope.pinecone import (\n        PineconeServerlessParams,\n        PineconeSettings,\n        PineconeVectorStore,\n    )\n    from mirascope.openai import OpenAIEmbedder\n    from mirascope.rag import TextChunker\n\n\n    class MyStore(ChromaVectorStore):\n        embedder = OpenAIEmbedder(dimensions=1536)\n        chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n        index_name = \"my-store-0001\"\n        api_key = settings.pinecone_api_key\n        client_settings = PineconeSettings()\n        vectorstore_params = PineconeServerlessParams(\n            cloud=\"aws\",\n            region=\"us-west-2\",\n        )\n\n    my_store = MyStore()\n    with open(f\"{PATH_TO_FILE}\") as file:\n        data = file.read()\n        my_store.add(data)\n    query_results = my_store.retrieve(\"my question\")\n    #&gt; QueryResult(ids=['0'], documents=['my answer'],\n    # scores=[0.9999999999999999], embeddings=[[0.0, 0.0, 0.0, ...]])\n    ```\n    \"\"\"\n\n    ids: list[str]\n    documents: Optional[list[str]] = None\n    scores: Optional[list[float]] = None\n    embeddings: Optional[list[list[float]]] = None\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeServerlessParams","title":"<code>PineconeServerlessParams</code>","text":"<p>             Bases: <code>PineconeParams</code>, <code>ServerlessSpec</code>, <code>BaseVectorStoreParams</code></p> <p>The parameters for Pinecone create_index with serverless spec and weave</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class PineconeServerlessParams(PineconeParams, ServerlessSpec, BaseVectorStoreParams):\n    \"\"\"The parameters for Pinecone create_index with serverless spec and weave\"\"\"\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        serverless_kwargs = ServerlessSpec(**self.model_dump()).kwargs()\n        pinecone_kwargs = PineconeParams(**self.model_dump()).kwargs()\n        return {**pinecone_kwargs, \"spec\": {**serverless_kwargs}}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeServerlessParams.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    serverless_kwargs = ServerlessSpec(**self.model_dump()).kwargs()\n    pinecone_kwargs = PineconeParams(**self.model_dump()).kwargs()\n    return {**pinecone_kwargs, \"spec\": {**serverless_kwargs}}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeSettings","title":"<code>PineconeSettings</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Settings for Pinecone instance</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class PineconeSettings(BaseModel):\n    \"\"\"Settings for Pinecone instance\"\"\"\n\n    api_key: Optional[str] = None\n    host: Optional[str] = None\n    proxy_url: Optional[str] = None\n    proxy_headers: Optional[dict[str, str]] = None\n    ssl_ca_certs: Optional[str] = None\n    ssl_verify: Optional[bool] = None\n    config: Optional[Config] = None\n    additional_headers: Optional[dict[str, str]] = {}\n    pool_threads: Optional[int] = 1\n    index_api: Optional[ManageIndexesApi] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        kwargs = {\n            key: value for key, value in self.model_dump().items() if value is not None\n        }\n        return kwargs\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PineconeSettings.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    kwargs = {\n        key: value for key, value in self.model_dump().items() if value is not None\n    }\n    return kwargs\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PodSpec","title":"<code>PodSpec</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The parameters for Pinecone PodSpec</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class PodSpec(BaseModel):\n    \"\"\"The parameters for Pinecone PodSpec\"\"\"\n\n    environment: str\n    replicas: Optional[int] = None\n    shards: Optional[int] = None\n    pods: Optional[int] = None\n    pod_type: Optional[str] = \"p1.x1\"\n    metadata_config: Optional[dict] = {}\n    source_collection: Optional[str] = None\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        kwargs = {\n            key: value for key, value in self.model_dump().items() if value is not None\n        }\n        return {\"pod\": kwargs}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.PodSpec.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    kwargs = {\n        key: value for key, value in self.model_dump().items() if value is not None\n    }\n    return {\"pod\": kwargs}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.ServerlessSpec","title":"<code>ServerlessSpec</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The parameters for Pinecone ServerlessSpec</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>class ServerlessSpec(BaseModel):\n    \"\"\"The parameters for Pinecone ServerlessSpec\"\"\"\n\n    cloud: str\n    region: str\n\n    def kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n        kwargs = {\n            key: value for key, value in self.model_dump().items() if value is not None\n        }\n        return {\"serverless\": kwargs}\n</code></pre>"},{"location":"api/pinecone/types/#mirascope.pinecone.types.ServerlessSpec.kwargs","title":"<code>kwargs()</code>","text":"<p>Returns all parameters for the index as a keyword arguments dictionary.</p> Source code in <code>mirascope/pinecone/types.py</code> <pre><code>def kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Returns all parameters for the index as a keyword arguments dictionary.\"\"\"\n    kwargs = {\n        key: value for key, value in self.model_dump().items() if value is not None\n    }\n    return {\"serverless\": kwargs}\n</code></pre>"},{"location":"api/pinecone/vectorstores/","title":"pinecone.vectorstores","text":"<p>A module for calling Chroma's Client and Collection.</p>"},{"location":"api/pinecone/vectorstores/#mirascope.pinecone.vectorstores.PineconeVectorStore","title":"<code>PineconeVectorStore</code>","text":"<p>             Bases: <code>BaseVectorStore</code></p> <p>A vectorstore for Pinecone.</p> <p>Example:</p> <pre><code>from mirascope.pinecone import (\n    PineconeServerlessParams,\n    PineconeSettings,\n    PineconeVectorStore,\n)\nfrom mirascope.openai import OpenAIEmbedder\nfrom mirascope.rag import TextChunker\n\n\nclass MyStore(ChromaVectorStore):\n    embedder = OpenAIEmbedder(dimensions=1536)\n    chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n    index_name = \"my-store-0001\"\n    api_key = settings.pinecone_api_key\n    client_settings = PineconeSettings()\n    vectorstore_params = PineconeServerlessParams(\n        cloud=\"aws\",\n        region=\"us-west-2\",\n    )\n\nmy_store = MyStore()\nwith open(f\"{PATH_TO_FILE}\") as file:\n    data = file.read()\n    my_store.add(data)\ndocuments = my_store.retrieve(\"my question\").documents\nprint(documents)\n</code></pre> Source code in <code>mirascope/pinecone/vectorstores.py</code> <pre><code>class PineconeVectorStore(BaseVectorStore):\n    \"\"\"A vectorstore for Pinecone.\n\n    Example:\n\n    ```python\n    from mirascope.pinecone import (\n        PineconeServerlessParams,\n        PineconeSettings,\n        PineconeVectorStore,\n    )\n    from mirascope.openai import OpenAIEmbedder\n    from mirascope.rag import TextChunker\n\n\n    class MyStore(ChromaVectorStore):\n        embedder = OpenAIEmbedder(dimensions=1536)\n        chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n        index_name = \"my-store-0001\"\n        api_key = settings.pinecone_api_key\n        client_settings = PineconeSettings()\n        vectorstore_params = PineconeServerlessParams(\n            cloud=\"aws\",\n            region=\"us-west-2\",\n        )\n\n    my_store = MyStore()\n    with open(f\"{PATH_TO_FILE}\") as file:\n        data = file.read()\n        my_store.add(data)\n    documents = my_store.retrieve(\"my question\").documents\n    print(documents)\n    ```\n    \"\"\"\n\n    handle_add_text: Optional[Callable[[list[Document]], None]] = None\n    handle_retrieve_text: Optional[Callable[[list[float]], list[str]]] = None\n\n    vectorstore_params: ClassVar[\n        Union[PineconePodParams, PineconeServerlessParams]\n    ] = PineconeServerlessParams(cloud=\"aws\", region=\"us-east-1\")\n    client_settings: ClassVar[PineconeSettings] = PineconeSettings()\n\n    def retrieve(self, text: str, **kwargs: Any) -&gt; PineconeQueryResult:\n        \"\"\"Queries the vectorstore for closest match\"\"\"\n        embed = self.embedder.embed\n        if self.vectorstore_params.weave is not None and not isinstance(\n            self.chunker, weave.Op\n        ):\n            embed = self.vectorstore_params.weave(\n                self.embedder.embed\n            )  # pragma: no cover\n        text_embedding = embed([text])\n        if \"top_k\" not in kwargs:\n            kwargs[\"top_k\"] = 8\n\n        query_result: QueryResponse = self._index.query(\n            vector=text_embedding[0].embedding,\n            **{\"include_metadata\": True, \"include_values\": True, **kwargs},\n        )\n        ids: list[str] = []\n        scores: list[float] = []\n        documents: list[str] = []\n        embeddings: list[list[float]] = []\n        for match in query_result.matches:\n            ids.append(match.id)\n            scores.append(match.score)\n            documents.append(\n                self.handle_retrieve_text([match.values])[0]\n                if self.handle_retrieve_text\n                else match.metadata[\"text\"]\n            )\n            embeddings.append(match.values)\n\n        return PineconeQueryResult(\n            ids=ids,\n            scores=scores,\n            documents=documents,\n            embeddings=embeddings,\n        )\n\n    def add(\n        self,\n        text: Union[str, list[Document]],\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Takes unstructured data and upserts into vectorstore\"\"\"\n        documents: list[Document]\n        if isinstance(text, str):\n            chunk = self.chunker.chunk\n            if self.vectorstore_params.weave is not None and not isinstance(\n                self.chunker, weave.Op\n            ):\n                chunk = self.vectorstore_params.weave(\n                    self.chunker.chunk\n                )  # pragma: no cover\n            documents = chunk(text)\n        else:\n            documents = text\n        inputs = [document.text for document in documents]\n        embed = self.embedder.embed\n        if self.vectorstore_params.weave is not None and not isinstance(\n            self.chunker, weave.Op\n        ):\n            embed = self.vectorstore_params.weave(\n                self.embedder.embed\n            )  # pragma: no cover\n        embeddings = embed(inputs)\n        if self.handle_add_text:\n            self.handle_add_text(documents)\n\n        vectors = []\n        for i, embedding in enumerate(embeddings):\n            if documents[i] is not None:\n                metadata = documents[i].metadata or {}\n                metadata_text = (\n                    {\"text\": documents[i].text}\n                    if documents[i].text and not self.handle_add_text\n                    else {}\n                )\n                vectors.append(\n                    {\n                        \"id\": documents[i].id,\n                        \"values\": embedding.embedding,\n                        \"metadata\": {**metadata, **metadata_text},\n                    }\n                )\n        return self._index.upsert(vectors, **kwargs)\n\n    ############################# PRIVATE PROPERTIES #################################\n\n    @cached_property\n    def _client(self) -&gt; Pinecone:\n        return Pinecone(api_key=self.api_key, **self.client_settings.kwargs())\n\n    @cached_property\n    def _index(self) -&gt; Index:\n        if self.index_name not in self._client.list_indexes().names():\n            self._client.create_index(\n                name=self.index_name,\n                dimension=self.embedder.dimensions,\n                **self.vectorstore_params.kwargs(),\n            )\n        return self._client.Index(self.index_name)\n</code></pre>"},{"location":"api/pinecone/vectorstores/#mirascope.pinecone.vectorstores.PineconeVectorStore.add","title":"<code>add(text, **kwargs)</code>","text":"<p>Takes unstructured data and upserts into vectorstore</p> Source code in <code>mirascope/pinecone/vectorstores.py</code> <pre><code>def add(\n    self,\n    text: Union[str, list[Document]],\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Takes unstructured data and upserts into vectorstore\"\"\"\n    documents: list[Document]\n    if isinstance(text, str):\n        chunk = self.chunker.chunk\n        if self.vectorstore_params.weave is not None and not isinstance(\n            self.chunker, weave.Op\n        ):\n            chunk = self.vectorstore_params.weave(\n                self.chunker.chunk\n            )  # pragma: no cover\n        documents = chunk(text)\n    else:\n        documents = text\n    inputs = [document.text for document in documents]\n    embed = self.embedder.embed\n    if self.vectorstore_params.weave is not None and not isinstance(\n        self.chunker, weave.Op\n    ):\n        embed = self.vectorstore_params.weave(\n            self.embedder.embed\n        )  # pragma: no cover\n    embeddings = embed(inputs)\n    if self.handle_add_text:\n        self.handle_add_text(documents)\n\n    vectors = []\n    for i, embedding in enumerate(embeddings):\n        if documents[i] is not None:\n            metadata = documents[i].metadata or {}\n            metadata_text = (\n                {\"text\": documents[i].text}\n                if documents[i].text and not self.handle_add_text\n                else {}\n            )\n            vectors.append(\n                {\n                    \"id\": documents[i].id,\n                    \"values\": embedding.embedding,\n                    \"metadata\": {**metadata, **metadata_text},\n                }\n            )\n    return self._index.upsert(vectors, **kwargs)\n</code></pre>"},{"location":"api/pinecone/vectorstores/#mirascope.pinecone.vectorstores.PineconeVectorStore.retrieve","title":"<code>retrieve(text, **kwargs)</code>","text":"<p>Queries the vectorstore for closest match</p> Source code in <code>mirascope/pinecone/vectorstores.py</code> <pre><code>def retrieve(self, text: str, **kwargs: Any) -&gt; PineconeQueryResult:\n    \"\"\"Queries the vectorstore for closest match\"\"\"\n    embed = self.embedder.embed\n    if self.vectorstore_params.weave is not None and not isinstance(\n        self.chunker, weave.Op\n    ):\n        embed = self.vectorstore_params.weave(\n            self.embedder.embed\n        )  # pragma: no cover\n    text_embedding = embed([text])\n    if \"top_k\" not in kwargs:\n        kwargs[\"top_k\"] = 8\n\n    query_result: QueryResponse = self._index.query(\n        vector=text_embedding[0].embedding,\n        **{\"include_metadata\": True, \"include_values\": True, **kwargs},\n    )\n    ids: list[str] = []\n    scores: list[float] = []\n    documents: list[str] = []\n    embeddings: list[list[float]] = []\n    for match in query_result.matches:\n        ids.append(match.id)\n        scores.append(match.score)\n        documents.append(\n            self.handle_retrieve_text([match.values])[0]\n            if self.handle_retrieve_text\n            else match.metadata[\"text\"]\n        )\n        embeddings.append(match.values)\n\n    return PineconeQueryResult(\n        ids=ids,\n        scores=scores,\n        documents=documents,\n        embeddings=embeddings,\n    )\n</code></pre>"},{"location":"api/rag/","title":"rag","text":"<p>A module for interacting with Mirascope RAG.</p>"},{"location":"api/rag/embedders/","title":"rag.embedders","text":"<p>Embedders for the RAG module.</p>"},{"location":"api/rag/embedders/#mirascope.rag.embedders.BaseEmbedder","title":"<code>BaseEmbedder</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[BaseEmbeddingT]</code>, <code>ABC</code></p> <p>The base class abstract interface for interacting with LLM embeddings.</p> Source code in <code>mirascope/rag/embedders.py</code> <pre><code>class BaseEmbedder(BaseModel, Generic[BaseEmbeddingT], ABC):\n    \"\"\"The base class abstract interface for interacting with LLM embeddings.\"\"\"\n\n    api_key: ClassVar[Optional[str]] = None\n    base_url: ClassVar[Optional[str]] = None\n    embedding_params: ClassVar[BaseEmbeddingParams] = BaseEmbeddingParams(\n        model=\"text-embedding-ada-002\"\n    )\n    dimensions: Optional[int] = None\n\n    @abstractmethod\n    def embed(self, input: list[str]) -&gt; list[BaseEmbeddingT]:\n        \"\"\"A call to the embedder with a single input\"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    async def embed_async(self, input: list[str]) -&gt; list[BaseEmbeddingT]:\n        \"\"\"Asynchronously call the embedder with a single input\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/embedders/#mirascope.rag.embedders.BaseEmbedder.embed","title":"<code>embed(input)</code>  <code>abstractmethod</code>","text":"<p>A call to the embedder with a single input</p> Source code in <code>mirascope/rag/embedders.py</code> <pre><code>@abstractmethod\ndef embed(self, input: list[str]) -&gt; list[BaseEmbeddingT]:\n    \"\"\"A call to the embedder with a single input\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/embedders/#mirascope.rag.embedders.BaseEmbedder.embed_async","title":"<code>embed_async(input)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Asynchronously call the embedder with a single input</p> Source code in <code>mirascope/rag/embedders.py</code> <pre><code>@abstractmethod\nasync def embed_async(self, input: list[str]) -&gt; list[BaseEmbeddingT]:\n    \"\"\"Asynchronously call the embedder with a single input\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/vectorstores/","title":"rag.vectorstores","text":"<p>Vectorstores for the RAG module.</p>"},{"location":"api/rag/vectorstores/#mirascope.rag.vectorstores.BaseVectorStore","title":"<code>BaseVectorStore</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>Generic[BaseQueryResultsT]</code>, <code>ABC</code></p> <p>The base class abstract interface for interacting with vectorstores.</p> Source code in <code>mirascope/rag/vectorstores.py</code> <pre><code>class BaseVectorStore(BaseModel, Generic[BaseQueryResultsT], ABC):\n    \"\"\"The base class abstract interface for interacting with vectorstores.\"\"\"\n\n    api_key: ClassVar[Optional[str]] = None\n    index_name: ClassVar[Optional[str]] = None\n    chunker: ClassVar[BaseChunker] = TextChunker(chunk_size=1000, chunk_overlap=200)\n    embedder: ClassVar[BaseEmbedder]\n    vectorstore_params: ClassVar[BaseVectorStoreParams] = BaseVectorStoreParams()\n\n    @abstractmethod\n    def retrieve(self, text: str, **kwargs: Any) -&gt; BaseQueryResultsT:\n        \"\"\"Queries the vectorstore for closest match\"\"\"\n        ...  # pragma: no cover\n\n    @abstractmethod\n    def add(self, text: Union[str, list[Document]], **kwargs: Any) -&gt; None:\n        \"\"\"Takes unstructured data and upserts into vectorstore\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/vectorstores/#mirascope.rag.vectorstores.BaseVectorStore.add","title":"<code>add(text, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Takes unstructured data and upserts into vectorstore</p> Source code in <code>mirascope/rag/vectorstores.py</code> <pre><code>@abstractmethod\ndef add(self, text: Union[str, list[Document]], **kwargs: Any) -&gt; None:\n    \"\"\"Takes unstructured data and upserts into vectorstore\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/vectorstores/#mirascope.rag.vectorstores.BaseVectorStore.retrieve","title":"<code>retrieve(text, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Queries the vectorstore for closest match</p> Source code in <code>mirascope/rag/vectorstores.py</code> <pre><code>@abstractmethod\ndef retrieve(self, text: str, **kwargs: Any) -&gt; BaseQueryResultsT:\n    \"\"\"Queries the vectorstore for closest match\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/chunkers/","title":"rag.chunkers","text":""},{"location":"api/rag/chunkers/base_chunker/","title":"rag.chunkers.base_chunker","text":"<p>Chunkers for the RAG module.</p>"},{"location":"api/rag/chunkers/base_chunker/#mirascope.rag.chunkers.base_chunker.BaseChunker","title":"<code>BaseChunker</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class for chunkers.</p> <p>Example:</p> <pre><code>from mirascope.rag import BaseChunker, Document\n\n\nclass TextChunker(BaseChunker):\n    chunk_size: int\n    chunk_overlap: int\n\n    def chunk(self, text: str) -&gt; list[Document]:\n        chunks: list[Document] = []\n        start: int = 0\n        while start &lt; len(text):\n            end: int = min(start + self.chunk_size, len(text))\n            chunks.append(Document(text=text[start:end], id=str(uuid.uuid4())))\n            start += self.chunk_size - self.chunk_overlap\n        return chunks\n</code></pre> Source code in <code>mirascope/rag/chunkers/base_chunker.py</code> <pre><code>class BaseChunker(BaseModel, ABC):\n    \"\"\"Base class for chunkers.\n\n    Example:\n\n    ```python\n    from mirascope.rag import BaseChunker, Document\n\n\n    class TextChunker(BaseChunker):\n        chunk_size: int\n        chunk_overlap: int\n\n        def chunk(self, text: str) -&gt; list[Document]:\n            chunks: list[Document] = []\n            start: int = 0\n            while start &lt; len(text):\n                end: int = min(start + self.chunk_size, len(text))\n                chunks.append(Document(text=text[start:end], id=str(uuid.uuid4())))\n                start += self.chunk_size - self.chunk_overlap\n            return chunks\n    ```\n    \"\"\"\n\n    @abstractmethod\n    def chunk(self, text: str) -&gt; list[Document]:\n        \"\"\"Returns a Document that contains an id, text, and optionally metadata.\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/chunkers/base_chunker/#mirascope.rag.chunkers.base_chunker.BaseChunker.chunk","title":"<code>chunk(text)</code>  <code>abstractmethod</code>","text":"<p>Returns a Document that contains an id, text, and optionally metadata.</p> Source code in <code>mirascope/rag/chunkers/base_chunker.py</code> <pre><code>@abstractmethod\ndef chunk(self, text: str) -&gt; list[Document]:\n    \"\"\"Returns a Document that contains an id, text, and optionally metadata.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/rag/chunkers/text_chunker/","title":"rag.chunkers.text_chunker","text":"<p>Text chunker for the RAG module</p>"},{"location":"api/rag/chunkers/text_chunker/#mirascope.rag.chunkers.text_chunker.TextChunker","title":"<code>TextChunker</code>","text":"<p>             Bases: <code>BaseChunker</code></p> <p>A text chunker that splits a text into chunks of a certain size and overlaps.</p> <p>Example:</p> <pre><code>from mirascope.rag import TextChunker\n\ntext_chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\nchunks = text_chunker.chunk(\"This is a long text that I want to split into chunks.\")\nprint(chunks)\n</code></pre> Source code in <code>mirascope/rag/chunkers/text_chunker.py</code> <pre><code>class TextChunker(BaseChunker):\n    \"\"\"A text chunker that splits a text into chunks of a certain size and overlaps.\n\n    Example:\n\n    ```python\n    from mirascope.rag import TextChunker\n\n    text_chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n    chunks = text_chunker.chunk(\"This is a long text that I want to split into chunks.\")\n    print(chunks)\n    ```\n    \"\"\"\n\n    chunk_size: int\n    chunk_overlap: int\n\n    def chunk(self, text: str) -&gt; list[Document]:\n        chunks: list[Document] = []\n        start: int = 0\n        while start &lt; len(text):\n            end: int = min(start + self.chunk_size, len(text))\n            chunks.append(Document(text=text[start:end], id=str(uuid.uuid4())))\n            start += self.chunk_size - self.chunk_overlap\n        return chunks\n</code></pre>"},{"location":"api/wandb/","title":"wandb","text":"<p>Integrations with Weights &amp; Biases toolins (wandb, weave).</p>"},{"location":"api/wandb/wandb/","title":"wandb.wandb","text":"<p>Prompts with WandB and OpenAI integration to support logging functionality.</p>"},{"location":"api/wandb/wandb/#mirascope.wandb.wandb.WandbCallMixin","title":"<code>WandbCallMixin</code>","text":"<p>             Bases: <code>_WandbBaseCall</code>, <code>Generic[BaseCallResponseT]</code></p> <p>A mixin for integrating a call with Weights &amp; Biases.</p> <p>Use this class's built in <code>call_with_trace</code> method to log traces to WandB along with your calls to LLM. These calls will include all of the additional metadata information such as the prompt template, template variables, and more.</p> <p>Example:</p> <pre><code>import os\n\nfrom mirascope.openai import OpenAICall, OpenAICallResponse\nfrom mirascope.wandb import WandbCallMixin\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass BookRecommender(OpenAICall, WandbCallMixin[OpenAICallResponse]):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nrecommender = BookRecommender(span_type=\"llm\", genre=\"fantasy\")\nresponse, span = recommender.call_with_trace()\n#           ^ this is a `Span` returned from the trace (or trace error).\n</code></pre> Source code in <code>mirascope/wandb/wandb.py</code> <pre><code>class WandbCallMixin(_WandbBaseCall, Generic[BaseCallResponseT]):\n    '''A mixin for integrating a call with Weights &amp; Biases.\n\n    Use this class's built in `call_with_trace` method to log traces to WandB along with\n    your calls to LLM. These calls will include all of the additional metadata\n    information such as the prompt template, template variables, and more.\n\n    Example:\n\n    ```python\n    import os\n\n    from mirascope.openai import OpenAICall, OpenAICallResponse\n    from mirascope.wandb import WandbCallMixin\n    import wandb\n\n    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n    wandb.init(project=\"wandb_logged_chain\")\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n    class BookRecommender(OpenAICall, WandbCallMixin[OpenAICallResponse]):\n        prompt_template = \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        Please recommend a {genre} book.\n        \"\"\"\n\n        genre: str\n\n\n    recommender = BookRecommender(span_type=\"llm\", genre=\"fantasy\")\n    response, span = recommender.call_with_trace()\n    #           ^ this is a `Span` returned from the trace (or trace error).\n    ```\n    '''\n\n    span_type: Literal[\"tool\", \"llm\", \"chain\", \"agent\"]\n\n    def call_with_trace(\n        self,\n        parent: Optional[Trace] = None,\n        **kwargs: Any,\n    ) -&gt; tuple[Optional[BaseCallResponseT], Trace]:\n        \"\"\"Creates an LLM response and logs it via a W&amp;B `Trace`.\n\n        Args:\n            parent: The parent trace to connect to.\n\n        Returns:\n            A tuple containing the completion and its trace (which has been connected\n                to the parent).\n        \"\"\"\n        try:\n            start_time = datetime.datetime.now().timestamp() * 1000\n            response = self.call(**kwargs)\n            tool_type = None\n            if response.tool_types and len(response.tool_types) &gt; 0:\n                tool_type = response.tool_types[0].__bases__[0]  # type: ignore\n            span = trace(self, response, tool_type, parent, **kwargs)\n            return response, span  # type: ignore\n        except Exception as e:\n            return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/wandb/wandb/#mirascope.wandb.wandb.WandbCallMixin.call_with_trace","title":"<code>call_with_trace(parent=None, **kwargs)</code>","text":"<p>Creates an LLM response and logs it via a W&amp;B <code>Trace</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Optional[BaseCallResponseT], Trace]</code> <p>A tuple containing the completion and its trace (which has been connected to the parent).</p> Source code in <code>mirascope/wandb/wandb.py</code> <pre><code>def call_with_trace(\n    self,\n    parent: Optional[Trace] = None,\n    **kwargs: Any,\n) -&gt; tuple[Optional[BaseCallResponseT], Trace]:\n    \"\"\"Creates an LLM response and logs it via a W&amp;B `Trace`.\n\n    Args:\n        parent: The parent trace to connect to.\n\n    Returns:\n        A tuple containing the completion and its trace (which has been connected\n            to the parent).\n    \"\"\"\n    try:\n        start_time = datetime.datetime.now().timestamp() * 1000\n        response = self.call(**kwargs)\n        tool_type = None\n        if response.tool_types and len(response.tool_types) &gt; 0:\n            tool_type = response.tool_types[0].__bases__[0]  # type: ignore\n        span = trace(self, response, tool_type, parent, **kwargs)\n        return response, span  # type: ignore\n    except Exception as e:\n        return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/wandb/wandb/#mirascope.wandb.wandb.WandbExtractorMixin","title":"<code>WandbExtractorMixin</code>","text":"<p>             Bases: <code>_WandbBaseExtractor</code>, <code>Generic[T]</code></p> <p>A extractor mixin for integrating with Weights &amp; Biases.</p> <p>Use this class's built in <code>extract_with_trace</code> method to log traces to WandB along with your calls to the LLM. These calls will include all of the additional metadata information such as the prompt template, template variables, and more.</p> <p>Example:</p> <pre><code>import os\nfrom typing import Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom mirascope.wandb import WandbExtractorMixin\nfrom pydantic import BaseModel\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIExtractor[Book], WandbExtractorMixin[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Please recommend a {genre} book.\n    \"\"\"\n\n    genre: str\n\n\nrecommender = BookRecommender(span_type=\"tool\", genre=\"fantasy\")\nbook, span = recommender.extract_with_trace()\n#       ^ this is a `Span` returned from the trace (or trace error).\n</code></pre> Source code in <code>mirascope/wandb/wandb.py</code> <pre><code>class WandbExtractorMixin(_WandbBaseExtractor, Generic[T]):\n    '''A extractor mixin for integrating with Weights &amp; Biases.\n\n    Use this class's built in `extract_with_trace` method to log traces to WandB along\n    with your calls to the LLM. These calls will include all of the additional metadata\n    information such as the prompt template, template variables, and more.\n\n    Example:\n\n    ```python\n    import os\n    from typing import Type\n\n    from mirascope.openai import OpenAIExtractor\n    from mirascope.wandb import WandbExtractorMixin\n    from pydantic import BaseModel\n    import wandb\n\n    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n    wandb.init(project=\"wandb_logged_chain\")\n\n    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n    class Book(BaseModel):\n        title: str\n        author: str\n\n\n    class BookRecommender(OpenAIExtractor[Book], WandbExtractorMixin[Book]):\n        extract_schema: Type[Book] = Book\n        prompt_template = \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n\n        USER:\n        Please recommend a {genre} book.\n        \"\"\"\n\n        genre: str\n\n\n    recommender = BookRecommender(span_type=\"tool\", genre=\"fantasy\")\n    book, span = recommender.extract_with_trace()\n    #       ^ this is a `Span` returned from the trace (or trace error).\n    ```\n    '''\n\n    span_type: Literal[\"tool\", \"llm\", \"chain\", \"agent\"]\n\n    def extract_with_trace(\n        self,\n        parent: Optional[Trace] = None,\n        retries: int = 0,\n        **kwargs: Any,\n    ) -&gt; tuple[Optional[T], Trace]:\n        \"\"\"Extracts `extract_schema` from the LLM call response and traces it.\n\n        The `extract_schema` is converted into an tool, complete with a description of\n        the tool, all of the fields, and their types. This allows us to take advantage\n        of tool/function calling functionality to extract information from a response\n        according to the context provided by the `BaseModel` schema.\n\n        Args:\n            parent: The parent trace to connect to.\n            retries: The maximum number of times to retry the query on validation error.\n            **kwargs: Additional keyword arguments parameters to pass to the call. These\n                will override any existing arguments in `call_params`.\n\n        Returns:\n            The `Schema` instance extracted from the response and it's trace.\n        \"\"\"\n        try:\n            start_time = datetime.datetime.now().timestamp() * 1000\n            model = self.extract(retries=retries, **kwargs)\n            span = trace(self, model._response, parent, **kwargs)  # type: ignore\n            return model, span  # type: ignore\n        except Exception as e:\n            return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/wandb/wandb/#mirascope.wandb.wandb.WandbExtractorMixin.extract_with_trace","title":"<code>extract_with_trace(parent=None, retries=0, **kwargs)</code>","text":"<p>Extracts <code>extract_schema</code> from the LLM call response and traces it.</p> <p>The <code>extract_schema</code> is converted into an tool, complete with a description of the tool, all of the fields, and their types. This allows us to take advantage of tool/function calling functionality to extract information from a response according to the context provided by the <code>BaseModel</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> <code>None</code> <code>retries</code> <code>int</code> <p>The maximum number of times to retry the query on validation error.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments parameters to pass to the call. These will override any existing arguments in <code>call_params</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Optional[T], Trace]</code> <p>The <code>Schema</code> instance extracted from the response and it's trace.</p> Source code in <code>mirascope/wandb/wandb.py</code> <pre><code>def extract_with_trace(\n    self,\n    parent: Optional[Trace] = None,\n    retries: int = 0,\n    **kwargs: Any,\n) -&gt; tuple[Optional[T], Trace]:\n    \"\"\"Extracts `extract_schema` from the LLM call response and traces it.\n\n    The `extract_schema` is converted into an tool, complete with a description of\n    the tool, all of the fields, and their types. This allows us to take advantage\n    of tool/function calling functionality to extract information from a response\n    according to the context provided by the `BaseModel` schema.\n\n    Args:\n        parent: The parent trace to connect to.\n        retries: The maximum number of times to retry the query on validation error.\n        **kwargs: Additional keyword arguments parameters to pass to the call. These\n            will override any existing arguments in `call_params`.\n\n    Returns:\n        The `Schema` instance extracted from the response and it's trace.\n    \"\"\"\n    try:\n        start_time = datetime.datetime.now().timestamp() * 1000\n        model = self.extract(retries=retries, **kwargs)\n        span = trace(self, model._response, parent, **kwargs)  # type: ignore\n        return model, span  # type: ignore\n    except Exception as e:\n        return None, trace_error(self, e, parent, start_time, **kwargs)\n</code></pre>"},{"location":"api/wandb/wandb/#mirascope.wandb.wandb.trace","title":"<code>trace(call, response, tool_type, parent, **kwargs)</code>","text":"<p>Returns a trace connected to parent.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>BaseCallResponse</code> <p>The response to trace. Handles <code>BaseCallResponse</code> for call/stream, and <code>BaseModel</code> for extractions.</p> required <code>tool_type</code> <code>Optional[Type[BaseTool]]</code> <p>The <code>BaseTool</code> provider-specific tool type e.g. <code>OpenAITool</code></p> required <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> required <p>Returns:</p> Type Description <code>Trace</code> <p>The created trace, connected to the parent.</p> Source code in <code>mirascope/wandb/wandb.py</code> <pre><code>def trace(\n    call: Union[_WandbBaseCall, _WandbBaseExtractor],\n    response: BaseCallResponse,\n    tool_type: Optional[Type[BaseTool]],\n    parent: Optional[Trace],\n    **kwargs: Any,\n) -&gt; Trace:\n    \"\"\"Returns a trace connected to parent.\n\n    Args:\n        response: The response to trace. Handles `BaseCallResponse` for call/stream, and\n            `BaseModel` for extractions.\n        tool_type: The `BaseTool` provider-specific tool type e.g. `OpenAITool`\n        parent: The parent trace to connect to.\n\n    Returns:\n        The created trace, connected to the parent.\n    \"\"\"\n    tool = response.tool\n    if tool is not None:\n        outputs = {\n            \"assistant\": tool.model_dump(),\n            \"tool_output\": tool.fn(**tool.args),\n        }\n    else:\n        outputs = {\"assistant\": response.content}\n\n    metadata = {\n        \"call_params\": call.call_params.model_copy(update=kwargs).kwargs(tool_type)\n    }\n    if response.response.usage is not None:\n        metadata[\"usage\"] = response.response.usage.model_dump()\n    span = Trace(\n        name=call.__class__.__name__,\n        kind=call.span_type,\n        status_code=\"success\",\n        status_message=None,\n        metadata=metadata,\n        start_time_ms=round(response.start_time),\n        end_time_ms=round(response.end_time),\n        inputs={message[\"role\"]: message[\"content\"] for message in call.messages()},\n        outputs=outputs,\n    )\n    if parent:\n        parent.add_child(span)\n    return span\n</code></pre>"},{"location":"api/wandb/wandb/#mirascope.wandb.wandb.trace_error","title":"<code>trace_error(call, error, parent, start_time, **kwargs)</code>","text":"<p>Returns an error trace connected to parent.</p> <p>Start time is set to time of prompt creation, and end time is set to the time function is called.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The error to trace.</p> required <code>parent</code> <code>Optional[Trace]</code> <p>The parent trace to connect to.</p> required <code>start_time</code> <code>float</code> <p>The time the call to OpenAI was started.</p> required <p>Returns:</p> Type Description <code>Trace</code> <p>The created error trace, connected to the parent.</p> Source code in <code>mirascope/wandb/wandb.py</code> <pre><code>def trace_error(\n    call: Union[_WandbBaseCall, _WandbBaseExtractor],\n    error: Exception,\n    parent: Optional[Trace],\n    start_time: float,\n    **kwargs: Any,\n) -&gt; Trace:\n    \"\"\"Returns an error trace connected to parent.\n\n    Start time is set to time of prompt creation, and end time is set to the time\n    function is called.\n\n    Args:\n        error: The error to trace.\n        parent: The parent trace to connect to.\n        start_time: The time the call to OpenAI was started.\n\n    Returns:\n        The created error trace, connected to the parent.\n    \"\"\"\n    span = Trace(\n        name=call.__class__.__name__,\n        kind=call.span_type,\n        status_code=\"error\",\n        status_message=str(error),\n        metadata={\"call_params\": call.call_params.model_copy(update=kwargs).kwargs()},\n        start_time_ms=round(start_time),\n        end_time_ms=round(datetime.datetime.now().timestamp() * 1000),\n        inputs={message[\"role\"]: message[\"content\"] for message in call.messages()},\n        outputs=None,\n    )\n    if parent:\n        parent.add_child(span)\n    return span\n</code></pre>"},{"location":"api/wandb/weave/","title":"wandb.weave","text":"<p>Integration with Weave from Weights &amp; Biases</p>"},{"location":"api/wandb/weave/#mirascope.wandb.weave.with_weave","title":"<code>with_weave(cls)</code>","text":"<p>Wraps base classes to automatically use weave.</p> <p>Supported base classes: <code>BaseCall</code>, <code>BaseExtractor</code>, <code>BaseVectorStore</code>, <code>BaseChunker</code>, <code>BaseEmbedder</code></p> <p>Example:</p> <pre><code>import weave\n\nfrom mirascope.openai import OpenAICall\nfrom mirascope.wandb import with_weave\n\nweave.init(\"my-project\")\n\n\n@with_weave\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend some {genre} books\"\n\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()  # this will automatically get logged with weave\nprint(response.content)\n</code></pre> Source code in <code>mirascope/wandb/weave.py</code> <pre><code>def with_weave(cls):\n    \"\"\"Wraps base classes to automatically use weave.\n\n    Supported base classes: `BaseCall`, `BaseExtractor`, `BaseVectorStore`,\n    `BaseChunker`, `BaseEmbedder`\n\n    Example:\n\n    ```python\n    import weave\n\n    from mirascope.openai import OpenAICall\n    from mirascope.wandb import with_weave\n\n    weave.init(\"my-project\")\n\n\n    @with_weave\n    class BookRecommender(OpenAICall):\n        prompt_template = \"Please recommend some {genre} books\"\n\n        genre: str\n\n\n    recommender = BookRecommender(genre=\"fantasy\")\n    response = recommender.call()  # this will automatically get logged with weave\n    print(response.content)\n    ```\n    \"\"\"\n    if hasattr(cls, \"call\"):\n        setattr(cls, \"call\", weave.op()(cls.call))\n    if hasattr(cls, \"call_async\"):\n        setattr(cls, \"call_async\", weave.op()(cls.call_async))\n\n    # VectorStore\n    if hasattr(cls, \"retrieve\"):\n        setattr(cls, \"retrieve\", weave.op()(cls.retrieve))\n    if hasattr(cls, \"add\"):\n        setattr(cls, \"add\", weave.op()(cls.add))\n\n    # Chunker\n    if hasattr(cls, \"chunk\"):\n        setattr(cls, \"chunk\", weave.op()(cls.chunk))\n\n    # Embedder\n    if hasattr(cls, \"embed\"):\n        setattr(cls, \"embed\", weave.op()(cls.embed))\n    if hasattr(cls, \"embed_async\"):\n        setattr(cls, \"embed_async\", weave.op()(cls.embed_async))\n\n    # It appears Weave does not yet support streaming or does it in a different way? :(\n    # Our calls will be tracked, but the sub-calls don't since the streaming happens\n    # when iterating through the generator after the call.\n    if hasattr(cls, \"stream\"):\n        setattr(cls, \"stream\", weave.op()(cls.stream))\n    if hasattr(cls, \"stream_async\"):\n        setattr(cls, \"stream_async\", weave.op()(cls.stream_async))\n\n    if hasattr(cls, \"extract\"):\n        setattr(cls, \"extract\", weave.op()(cls.extract))\n    if hasattr(cls, \"extract_async\"):\n        setattr(cls, \"extract_async\", weave.op()(cls.extract_async))\n\n    if hasattr(cls, \"call_params\"):\n        cls.call_params.weave = weave.op()\n    if hasattr(cls, \"vectorstore_params\"):\n        cls.vectorstore_params.weave = weave.op()\n    return cls\n</code></pre>"},{"location":"concepts/attaching_and_calling_tool_functions/","title":"Attaching tool functions to Mirascope Calls","text":""},{"location":"concepts/attaching_and_calling_tool_functions/#using-mirascope-openai-tool","title":"Using Mirascope OpenAI Tool","text":"<p>Create your call and pass in your <code>OpenAITool</code>:</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\n\nfrom mirascope.base import tool_fn\nfrom mirascope.openai import OpenAICall, OpenAITool\n\n@tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\nclass TodaysForecast(OpenAICall):\n    prompt_template = \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\", tools=[GetCurrentWeather]\n    )\n</code></pre> <p>The tools are attached to the <code>call_params</code> attribute in a Mirascope Call. For more information, check out why colocation is so important and how combining it with the Mirascope CLI makes engineering better prompts and calls easy.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#using-a-function-properly-documented-with-a-docstring","title":"Using a function properly documented with a docstring","text":"<p>Create your call and pass in your function:</p> <pre><code>import json\n\nfrom typing import Literal\n\nfrom mirascope.openai import OpenAICall\n\n\ndef get_current_weather(\n        location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON object containing the location, temperature, and unit.\n    \"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\nclass TodaysForecast(OpenAICall):\n    prompt_template = \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo-1106\", tools=[get_current_weather]\n    )\n</code></pre>"},{"location":"concepts/attaching_and_calling_tool_functions/#calling-tools","title":"Calling Tools","text":"<p>Generate content by calling the <code>call</code> method:</p> <pre><code># using same code as above\nforecast = TodaysForecast()\nresponse = forecast.call()\nif tools := response.tools:\n    for tool in tools:\n        print(tool.fn(**tool.args))\n\n#&gt; {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n#&gt; {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n</code></pre> <p>The\u00a0<code>response.tools</code>\u00a0property returns an actual instance of the tool.</p>"},{"location":"concepts/attaching_and_calling_tool_functions/#async","title":"Async","text":"<p>All of the examples above also work with\u00a0<code>async</code>\u00a0by replacing <code>call</code> with <code>call_async</code> or <code>stream</code> with <code>stream_async</code>.</p>"},{"location":"concepts/chat_history/","title":"Chat History","text":"<p>Large Language Models (LLMs) are inherently stateless, meaning they lack a built-in mechanism to retain information from one interaction to the next. However, incorporating chat history introduces a stateful element, enabling LLMs to recall past interactions with a user, thus personalizing the interaction experience. Mirascope provides a seamless solution to implement state and effectively manage scenarios where context limitations might otherwise be an issue.</p>"},{"location":"concepts/chat_history/#messages-keyword","title":"MESSAGES keyword","text":"<p>Let us take a simple chat application as our example. Every time the user makes a call to the LLM, the question and response is stored for the next call. To do this, use the <code>MESSAGES</code> keyword:</p> <pre><code>import os\n\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nclass Librarian(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    MESSAGES: {history}\n    USER: {question}\n    \"\"\"\n\n    question: str\n    history: list[ChatCompletionMessageParam] = []\n\nlibrarian = Librarian(question=\"\", history=[])\nwhile True:\n    librarian.question = input(\"(User): \")\n    response = librarian.call()\n    librarian.history += [\n        {\"role\": \"user\", \"content\": librarian.question},\n        {\"role\": \"assistant\", \"content\": response.content},\n    ]\n    print(f\"(Assistant): {response.content}\")\n\n#&gt; (User): What fantasy book should I read?\n#&gt; (Assistant): Have you read the Name of the Wind?\n#&gt; (User): I have! What do you like about it?\n#&gt; (Assistant): I love the intricate world-building...\n</code></pre> <p>This will insert the history or context between the <code>SYSTEM</code> role and the <code>USER</code> role as additional messages in the messages array passed to the LLM.</p> <p>Note</p> <p>Different model providers have constraints on their roles, so make sure you follow them when injecting <code>MESSAGES</code>. For example, Anthropic requires a back-and-forth between single user and assistant messages, and supplying two sequential user messages will throw an error.</p>"},{"location":"concepts/chat_history/#overriding-messages-function","title":"Overriding messages function","text":"<p>Alternatively, if you do not want to use the prompt_template parser, you can override the <code>messages</code> function instead.</p> <pre><code>import os\n\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nclass Librarian(OpenAICall):\n    question: str\n    history: list[ChatCompletionMessageParam] = []\n\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\"role\": \"system\", \"content\": \"You are the world's greatest librarian.\"},\n            *self.history,\n            {\"role\": \"user\", \"content\": f\"{self.question}\"},\n        ]\n\nlibrarian = Librarian(question=\"\", history=[])\nwhile True:\n    librarian.question = input(\"(User): \")\n    response = librarian.call()\n    librarian.history += [\n        {\"role\": \"user\", \"content\": librarian.question},\n        {\"role\": \"assistant\", \"content\": response.content},\n    ]\n    print(f\"(Assistant): {response.content}\")\n\n#&gt; (User): What fantasy book should I read?\n#&gt; (Assistant): Have you read the Name of the Wind?\n#&gt; (User): I have! What do you like about it?\n#&gt; (Assistant): I love the intricate world-building...\n</code></pre>"},{"location":"concepts/chat_history/#overcoming-context-limits-with-rag","title":"Overcoming context limits with RAG","text":"<p>As your chat gets longer and longer, you will soon approach the context limit for the particular model. One not so great solution is to remove the oldest messages to stay within the context limit. For example:</p> <pre><code>import os\n\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nclass Librarian(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    MESSAGES: {history}\n    USER: {question}\n    \"\"\"\n\n    question: str\n    history: list[ChatCompletionMessageParam] = []\n\nlibrarian = Librarian(question=\"\", history=[])\nwhile True:\n    librarian.question = input(\"(User): \")\n    response = librarian.call()\n    librarian.history += [\n        {\"role\": \"user\", \"content\": librarian.question},\n        {\"role\": \"assistant\", \"content\": response.content},\n    ]\n    # Limit to only the last 10 messages -- i.e. short term memory loss\n    librarian.history = librarian.history[-10:]\n    print(f\"(Assistant): {response.content}\")\n\n#&gt; (User): What fantasy book should I read?\n#&gt; (Assistant): Have you read the Name of the Wind?\n#&gt; (User): I have! What do you like about it?\n#&gt; (Assistant): I love the intricate world-building...\n</code></pre> <p>Better would be to implement a RAG (Retrieval-Augmented Generation) system for storing all chat history and querying for relevant previous messages for each interaction.</p> <p>When the user makes a call, a search is made to find the most relevant information, which is then inserted as context to LLM, like so:</p> <pre><code>import os\nfrom your_repo.stores import LibrarianKnowledge\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Librarian(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    MESSAGES: {context}\n    USER: {question}\n    \"\"\"\n\n    question: str = \"\"\n    knowledge: LibrarianKnowledge = LibrarianKnowledge()\n\n    @property\n    def context(self):\n        return self.store.retrieve(self.question).documents\n\n\nlibrarian = Librarian()\nwhile True:\n    librarian.question = input(\"(User): \")\n    response = librarian.call()\n    content = f\"(Assistant): {response.content}\"\n    librarian.knowledge.add([librarian.question, content])\n    print(content)\n</code></pre> <p>Check out Mirascope RAG for a more in-depth look on creating a RAG application with Mirascope.</p>"},{"location":"concepts/defining_and_extracting_schemas/","title":"Defining and extracting schemas","text":"<p>Mirascope's extraction functionality is built on top of Pydantic. We will walk through the high-level concepts you need to know to get started extracting structured information with LLMs. We recommend reading their docs for more detailed explanations of everything that you can do with Pydantic.</p>"},{"location":"concepts/defining_and_extracting_schemas/#model","title":"Model","text":"<p>Defining the schema for extraction is done via models, which are classes that inherit from <code>pydantic.BaseModel</code>. We can then use an extractor to extract this schema:</p> <pre><code>from typing import Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"The Name of the Wind by Patrick Rothfuss.\"\n\n\nbook = BookExtractor().extract()\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>You can use tool classes like <code>OpenAITool</code> directly if you want to extract a single tool instead of just a schema (which is useful for calling attached functions).</p>"},{"location":"concepts/defining_and_extracting_schemas/#field","title":"Field","text":"<p>You can also use <code>pydantic.Fields</code> to add additional information for each field in your schema. Again, this information will be included in the prompt, and we can take advantage of that:</p> <pre><code>from typing import Type\n\nfrom mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel, Field\n\n\nclass Book(BaseModel):\n    title: str\n    author: str = Field(..., description=\"Last, First\")\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"The Name of the Wind by Patrick Rothfuss.\"\n\n\nbook = BookExtractor().extract()\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='The Name of the Wind' author='Rothfuss, Patrick'\n</code></pre> <p>Notice how instead of \u201cPatrick Rothfuss\u201d the extracted author is \u201cRothfuss, Patrick\u201d as desired.</p>"},{"location":"concepts/defining_and_extracting_schemas/#retries","title":"Retries","text":"<p>Sometimes the model will fail to extract the schema. This can often be a result of the prompt; however, sometimes it\u2019s simply a failure of the model. If you want to retry the extraction some number of times, you can set <code>retries</code> equal to however many retries you want to run (defaults to 0).</p> <pre><code>book = BookExtractor().extract(retries=3)  # will retry up to 3 times \n</code></pre>"},{"location":"concepts/defining_and_extracting_schemas/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<p>In the above examples, we\u2019re extracting information present in the prompt text into structured form. We can also use <code>extract</code> to generate structured information from a prompt:</p> <pre><code>from mirascope.openai import OpenAIPrompt\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"A science fiction book.\"\"\"\n\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIPrompt[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a book.\"\n\nbook = BookRecommender().extract()\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='Dune' author='Frank Herbert'\n</code></pre> <p>Notice that the docstring for the <code>Book</code> schema specified a science fiction book, which resulted in the model recommending a science fiction book. The docstring gets included with the prompt as part of the schema definition, and you can use this to your advantage for better prompting.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/","title":"Defining tools (function calls)","text":"<p>Tools are extremely useful when you want the model to intelligently choose to output the arguments to call one or more functions. With Mirascope it is extremely easy to use tools.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/#using-tools-in-mirascope","title":"Using tools in Mirascope","text":"<p>Mirascope will automatically convert any function properly documented with a docstring into a tool. This means that you can use any such function as a tool with no additional work. The function below is taken from OpenAI documentation with Google style python docstrings:</p> <pre><code>import json\n\nfrom typing import Literal\n\n\ndef get_current_weather(\n    location: str, unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA.\n        unit: The unit for the temperature.\n\n    Returns:\n        A JSON object containing the location, temperature, and unit.\n    \"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n</code></pre> <p>Note</p> <p>We support Google, ReST, Numpydoc, and Epydoc style docstrings.</p> <p>You can also define your own\u00a0<code>OpenAITool</code>\u00a0class. This is necessary when the function you want to use as a tool does not have a docstring. Additionally, the\u00a0<code>OpenAITool</code>\u00a0class makes it easy to further update the descriptions, which is useful when you want to further engineer your prompt:</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\n\nfrom mirascope.base import tool_fn\nfrom mirascope.openai import OpenAITool\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    # Assume this function does not have a docstring\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n\n\n@tool_fn(get_current_weather)\nclass GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n</code></pre> <p>Using the\u00a0tool_fn\u00a0decorator will attach the function defined by the tool to the tool for easier calling of the function. This happens automatically when using the function directly.</p>"},{"location":"concepts/defining_tools_%28function_calls%29/#tools-with-openai-api-only","title":"Tools with OpenAI API only","text":"<p>Using the same OpenAI docs, the function call is defined as such:</p> <pre><code>import json\n\n\ndef get_current_weather(location, unit=\"fahrenheit\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if \"tokyo\" in location.lower():\n        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n    elif \"san francisco\" in location.lower():\n        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n    elif \"paris\" in location.lower():\n        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n    else:\n        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n</code></pre> <p>OpenAI uses JSON Schema to define the tool call:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n</code></pre> <p>You can quickly see how bloated OpenAI tools become when defining multiple tools:</p> <pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                },\n                \"required\": [\"location\", \"format\"],\n            },\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_n_day_weather_forecast\",\n            \"description\": \"Get an N-day weather forecast\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                    },\n                    \"num_days\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The number of days to forecast\",\n                    }\n                },\n                \"required\": [\"location\", \"format\", \"num_days\"]\n            },\n        }\n    },\n]\n</code></pre> <p>With Mirascope, it will look like this:</p> <pre><code>class GetCurrentWeather(OpenAITool):\n    \"\"\"Get the current weather in a given location.\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    unit: Literal[\"celsius\", \"fahrenheit\"] = \"fahrenheit\"\n\n\nclass GetNDayWeatherForecast(GetCurrentWeather):\n    \"\"\"Get an N-day weather forecast\"\"\"\n\n    num_days: int = Field(..., description=\"The number of days to forecast\")\n</code></pre> <p>We can take advantage of class inheritance and reduce repetition. </p>"},{"location":"concepts/defining_tools_%28function_calls%29/#other-providers","title":"Other Providers","text":"<p>If you are using a function property documented with a docstring, you do not need to make any code changes when using other providers. Mirascope will automatically convert these functions to their proper format for you under the hood.</p> <p>For classes, simply replace <code>OpenAITool</code> with your provider of choice e.g. <code>GeminiTool</code> to match your choice of call.</p>"},{"location":"concepts/dumping_prompts_and_calls/","title":"Dumping prompts and calls","text":"<p>The <code>.dump()</code> function can be called from prompts, calls, and responses to output a dictionary of associated data.</p>"},{"location":"concepts/dumping_prompts_and_calls/#dumping-from-the-prompt-and-response","title":"Dumping from the Prompt and Response","text":"<p>When called from <code>BasePrompt</code> or any of its subclasses like <code>BaseCall</code>, <code>.dump()</code> will give you:</p> <ul> <li>the prompt template</li> <li>inputs used to construct the prompt</li> <li>the prompt\u2019s tags</li> <li>any parameters specific to the model provider\u2019s API call, if they are not None:</li> </ul> <p>The returned <code>BaseCallResponse</code> will also have a <code>.dump()</code> method, which includes:</p> <ul> <li>start and end times in ms of its affiliated completion, if it has happened</li> <li>output of the underlying LLM provider</li> <li>cost in dollars (Gemini not supported)</li> </ul> <pre><code>import os\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\n@tags([\"recommendation_project\", \"version:0001\"])\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n\nrecommender = BookRecommender(topic=\"how to bake a cake\")\nresponse = recommender.call()\nprint(recommender.dump() | response.dump())\n\n\"\"\"\nOutput:\n{\n    \"tags\": [\"recommendation_project\", \"version:0001\"],\n    \"template\": \"Can you recommend some books on {topic}?\",\n    \"inputs\": {\"topic\": \"how to bake a cake\"},\n    \"start_time\": 1709847166609.473,\n    \"end_time\": 1709847169424.146,\n    \"output\": {\n        \"id\": \"chatcmpl-9F8U8TbPJ2abpSXXyIURQr1KRiILw\",\n        \"choices\": [\n            {\n                \"finish_reason\": \"stop\",\n                \"index\": 0,\n                \"logprobs\": null,\n                \"message\": {\n                    \"content\": \"...\",\n                    \"role\": \"assistant\",\n                    \"function_call\": null,\n                    \"tool_calls\": null\n                }\n            }\n        ],\n        \"created\": 1713394564,\n        \"model\": \"gpt-3.5-turbo-0125\",\n        \"object\": \"chat.completion\",\n        \"system_fingerprint\": \"fp_c2295e73ad\",\n        \"usage\": {\n            \"completion_tokens\": 177,\n            \"prompt_tokens\": 19,\n            \"total_tokens\": 196\n        }\n    }\n    }\n    \"cost\": 0.0001235,\n\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/dumping_prompts_and_calls/#logging","title":"Logging","text":"<p>Now that you have the JSON dump, it can be useful to log your responses:</p> <pre><code>\"\"\"A basic example on how to log the data from a prompt and a chat completion.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Optional\n\nimport pandas as pd\nfrom sqlalchemy import JSON, Float, Integer, MetaData, String, create_engine\nfrom sqlalchemy.dialects.postgresql import JSONB\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, sessionmaker\n\nfrom mirascope import tags\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\nlogger = logging.getLogger(\"mirascope\")\nTABLE_NAME = \"openai_call_responses\"\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass OpenAICallResponseTable(Base):\n    __tablename__ = TABLE_NAME\n    id: Mapped[int] = mapped_column(\n        Integer(), primary_key=True, autoincrement=True, nullable=False\n    )\n    template: Mapped[str] = mapped_column(String(), nullable=False)\n    inputs: Mapped[Optional[dict]] = mapped_column(JSONB)\n    tags: Mapped[Optional[list[str]]] = mapped_column(JSON)\n    call_params: Mapped[Optional[dict]] = mapped_column(JSONB)\n    start_time: Mapped[Optional[float]] = mapped_column(Float(), nullable=False)\n    end_time: Mapped[Optional[float]] = mapped_column(Float(), nullable=False)\n    output: Mapped[Optional[dict]] = mapped_column(JSONB)\n    cost: Mapped[Optional[float]] = mapped_column(Float())\n\n\n@tags([\"recommendation_project\"])\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n\nUSERNAME = \"root\"\nPASSWORD = \"\"\nHOST = \"localhost\"\nPORT = \"5432\"\nDB_NAME = \"mirascope\"\nengine = create_engine(f\"postgresql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DB_NAME}\")\n\n\ndef create_database():\n    \"\"\"Create the database and table for the OpenAI call response.\"\"\"\n    metadata = MetaData()\n    table_objects = [Base.metadata.tables[TABLE_NAME]]\n    metadata.create_all(engine, tables=table_objects)\n\n\ndef log_to_database(recommender_response: dict[str, Any]):\n    \"\"\"Create a call response and log it to the database.\"\"\"\n    create_database()\n    Session = sessionmaker(engine)\n    with Session() as session:\n        openai_completion_db = OpenAICallResponseTable(**recommender_response)\n        session.add(openai_call_responses)\n        session.commit()\n\n\ndef log_to_csv(recommender_response: dict[str, Any]):\n    \"\"\"Log the call response to a CSV file.\"\"\"\n    df = pd.DataFrame([recommender_response])\n    with open(\"log.csv\", \"w\") as f:\n        df.to_csv(f, index=False)\n\n\ndef log_to_logger(recommender_response: dict[str, Any]):\n    \"\"\"Log the call response to the logger.\"\"\"\n    logger.info(recommender_response)\n\n\nif __name__ == \"__main__\":\n    recommender = BookRecommender(topic=\"how to bake a cake\")\n    response = recommender.call()\n    recommender_response = recommender.dump() | response.dump()\n    log_to_database(recommender_response)\n    log_to_csv(recommender_response)\n    log_to_logger(recommender_response)\n</code></pre>"},{"location":"concepts/extracting_base_types/","title":"Extracting base types","text":"<p>Mirascope also makes it possible to extract base types without defining a <code>pydantic.BaseModel</code> with the same exact format for extraction:</p> <pre><code>from mirascope.openai import OpenAIExtractor\n\n\nclass BookRecommender(OpenAIExtractor[list[str]]):\n    extract_schema: Type[list[str]] = list[str]\n    prompt_template = \"Please recommend some science fiction books.\"\n\n\nbooks = BookRecommendation().extract()\nprint(books)\n#&gt; ['Dune', 'Neuromancer', \"Ender's Game\", \"The Hitchhiker's Guide to the Galaxy\", 'Foundation', 'Snow Crash']\n</code></pre> <p>We currently support: <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>list</code>, <code>set</code>, <code>tuple</code>, and <code>Enum</code>.</p> <p>We also support using <code>Union</code>, <code>Literal</code>, and <code>Annotated</code> </p> <p>Note</p> <p>If you\u2019re using <code>mypy</code> you\u2019ll need to add <code>#  type: ignore</code> due to how these types are handled differently by Python.</p>"},{"location":"concepts/extracting_base_types/#using-enum-or-literal-for-classification","title":"Using <code>Enum</code> or <code>Literal</code> for classification","text":"<p>One nice feature of extracting base types is that we can easily use <code>Enum</code> or <code>Literal</code> to define a set of labels that the model should use to classify the prompt. For example, let\u2019s classify whether or not some email text is spam:</p> <pre><code>from enum import Enum\n# from typing import Literal\n\nfrom mirascope.openai import OpenAIExtractor\n\n# Label = Literal[\"is spam\", \"is not spam\"]\n\n\nclass Label(Enum):\n    NOT_SPAM = \"not_spam\"\n    SPAM = \"spam\"\n\n\nclass NotSpam(OpenAIExtractor[Label]):\n    extract_schema: Type[Label] = Label\n    prompt_template = \"Your car insurance payment has been processed. Thank you for your business.\"\n\n\nclass Spam(OpenAIExtractor[Label]):\n    extract_schema: Type[Label] = Label\n    prompt_template = \"I can make you $1000 in just an hour. Interested?\"\n\n\n# assert NotSpam().extract() == \"is not spam\"\n# assert Spam().extract() == \"is spam\"\nassert NotSpam().extract() == Label.NOT_SPAM\nassert Spam().extract() == Label.SPAM\n</code></pre>"},{"location":"concepts/extracting_structured_information_using_llms/","title":"Extracting structured information with LLMs","text":"<p>Large Language Models (LLMs) are powerful at generating human-like text, but their outputs are inherently unstructured. Many real-world applications require structured data to function properly, such as extracting due dates, priorities, and task descriptions from user inputs for a task management application, or extracting tabular data from unstructured text sources for data analysis pipelines.</p> <p>Mirascope provides tools and techniques to address this challenge, allowing you to extract structured information from LLM outputs reliably.</p>"},{"location":"concepts/extracting_structured_information_using_llms/#challenges-in-extracting-structured-information","title":"Challenges in Extracting Structured Information","text":"<p>The key challenges in extracting structured information from LLMs include:</p> <ol> <li>Unstructured Outputs: LLMs are trained on vast amounts of unstructured text data, causing their outputs to be unstructured as well.</li> <li>Hallucinations and Inaccuracies: LLMs can sometimes generate factually incorrect information, complicating the extraction of accurate structured data.</li> </ol>"},{"location":"concepts/extracting_structured_information_using_llms/#mirascopes-approach","title":"Mirascope's Approach","text":"<p>Mirascope offers a convenient <code>extract</code> method on extractor classes to extract structured information from LLM outputs. This method leverages tools (function calling) to reliably extract the required structured data. While you can find more details in the following pages, let's consider a simple example where we want to extract task details like due date, priority, and description from a user's natural language input:</p> <pre><code>from typing import Literal\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass TaskDetails(BaseModel):\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    description: str\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract(TaskDetails)\nassert isinstance(task_details, TaskDetails)\nprint(TaskDetails)\n#&gt; due_date='next Friday' priority='high' description='Submit quarterly report'\n</code></pre> <p>As you can see, Mirascope makes this extremely simple. Under the hood, Mirascope uses the provided schema to extract the generated content and validate it (see Validation for more details).</p>"},{"location":"concepts/generating_content/","title":"Generating content","text":"<p>Now that you have your prompt, you can combine it with a model call to generate content. Mirascope provides high-level wrappers around common providers so you can focus on prompt engineering instead of learning the interface for providers. Our high-level wrappers are not required to use our prompts but simply provide convenience if you wish to use it.</p> <p>Note</p> <p>This doc uses OpenAI. See supported LLM providers for how to generate content with other model providers like Anthropic, Mistral, Cohere and more.</p>"},{"location":"concepts/generating_content/#openaicall","title":"OpenAICall","text":"<p><code>OpenAICall</code> extends <code>BasePrompt</code> and <code>BaseCall</code> to support interacting with the OpenAI API.</p>"},{"location":"concepts/generating_content/#call","title":"Call","text":"<p>You can initialize an <code>OpenAICall</code> instance and call the <code>call</code> method to generate an <code>OpenAICallResponse</code>:</p> <pre><code>import os\n\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nresponse = RecipeRecommender(ingredient=\"apples\").call()\nprint(response.content)  # prints the string content of the call\n</code></pre> <p>The <code>call_params</code> of the OpenAI client is tied to the call (and thereby the prompt). Refer to Engineering better prompts [Add link] for more information.</p>"},{"location":"concepts/generating_content/#async","title":"Async","text":"<p>If you are want concurrency, you can use the <code>async</code> function instead.</p> <pre><code>import asyncio\nimport os\n\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nasync def recommend_recipes():\n    \"\"\"Asynchronously calls the model using `OpenAICall` to generate a recipe.\"\"\"\n    return await RecipeRecommender(ingredient=\"apples\").call_async()\n\n\nprint(asyncio.run(recommend_recipes())) \n</code></pre>"},{"location":"concepts/generating_content/#callresponse","title":"CallResponse","text":"<p>The <code>call</code> method returns an <code>OpenAICallResponse</code> class instance, which is a simple wrapper around the <code>ChatCompletion</code> class in <code>openai</code> that extends <code>BaseCallResponse</code>. In fact, you can access everything from the original response as desired. The primary purpose of the class is to provide convenience.</p> <pre><code>from mirascope.openai.types import OpenAICallResponse\n\nresponse = OpenAICallResponse(...)\n\ncompletion.content     # original.choices[0].message.content\ncompletion.tool_calls  # original.choices[0].message.tool_calls\ncompletion.message     # original.choices[0].message\ncompletion.choice      # original.choices[0]\ncompletion.choices     # original.choices\nresponse.response      # ChatCompletion(...)\n</code></pre>"},{"location":"concepts/generating_content/#chain-of-thought-cot","title":"Chain of Thought (CoT)","text":"<p>Adding a chain of calls is as simple as writing a function:</p> <pre><code>import os\nfrom functools import cached_property\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass ChefSelector(OpenAICall):\n    prompt_template = \"Name a chef who is really good at cooking {food_type} food\"\n\n    food_type: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nclass RecipeRecommender(ChefSelector):\n    prompt_template = \"\"\"\n    SYSTEM:\n    Imagine that you are chef {chef}.\n    Your task is to recommend recipes that you, {chef}, would be excited to serve.\n\n    USER:\n    Recommend a {food_type} recipe using {ingredient}.\n    \"\"\"\n\n    ingredient: str\n\n    @cached_property\n    def chef(self) -&gt; str:\n        \"\"\"Uses `ChefSelector` to select the chef based on the food type.\"\"\"\n        return ChefSelector(food_type=self.food_type).call().content\n\n\nresponse = RecipeRecommender(food_type=\"japanese\", ingredient=\"apples\").call()\nprint(response.content)\n# &gt; Certainly! Here's a recipe for a delicious and refreshing Japanese Apple Salad: ...\n</code></pre>"},{"location":"concepts/philosophy/","title":"Mirascope\u2019s Philosophy","text":"<p>When we first started building with LLMs, we struggled to find the right tooling. Every library had it\u2019s own unique quirks, but they all shared one key aspect that never sat well with me \u2014 magic.</p> <p>One downstream effect of this documentation is that any Mirascope functionality you use is clearly explained right there where you\u2019re using it. What\u2019s happening \u201cunder-the-hood\u201d should be in plain sight.</p> <p>Don\u2019t get me wrong. Magic can be cool. Really cool sometimes. But it often gets in the way. Especially when LLMs are already magical enough.</p> <p>What we wanted was a set of building blocks so that we could build our own tools easily. We wanted the annoying little things to just work so we could focus on items with higher value-add. But we didn\u2019t want to buy-in to an all-or-nothing framework only to inevitably rip it all out when some assumptions they made were wrong for our use-case. Instead, we wanted convenience around the core modules of building with LLMs with full access to any nitty-gritty details as necessary.</p> <p>When done right, this isn\u2019t magic \u2014 but it feels like magic. This is the core of our philosophy.</p>"},{"location":"concepts/philosophy/#1-no-magic","title":"1. No Magic","text":"<p>This starts with editor support and clear, up-to-date documentation. I\u2019m talking about autocomplete with detailed docstrings. Lint errors that prevent nasty and annoying bugs. The things we\u2019ve come to expect from our development environments. We rely on these tools to engineer effectively and efficiently. If you rarely need to read our docs, we\u2019re doing our job right.</p> <p>Furthermore, the functionality itself should not obscure any of the underlying functionality supported by the LLM provider\u2019s API. Instead it should make using such functionality simple and seamless.</p>"},{"location":"concepts/philosophy/#2-convenient-not-convoluted","title":"2. Convenient, Not Convoluted","text":"<p>Abstractions are useful, but they can quickly become bloat. As systems become more complex, too many abstractions get in your way. They often make it difficult or impossible to do what you want. You have to work around the abstractions, which is often hacky.</p> <p>At Mirascope, we try our best to limit what we abstract. Building with Mirascope will feel convenient. It will feel like we take care of all the things you don\u2019t want to think about so you can focus on the meat of your problem \u2014 the stuff you\u2019re excited to build.</p>"},{"location":"concepts/philosophy/#3-modular-and-pythonic","title":"3. Modular and Pythonic","text":"<p>We also wanted to minimize what you need to learn to build with LLMs, and the things you do need to learn should be familiar and feel like writing the Python you\u2019re already used to writing.</p> <p>Everything \u201cAI\u201d is moving too quickly for \u201call-or-nothing\u201d frameworks. It should be easy to slot in whatever module we want \u2014 especially raw Python.</p> <p>We don\u2019t want a framework. We want a toolkit. We want building blocks.</p>"},{"location":"concepts/philosophy/#4-colocation","title":"4. Colocation","text":"<p>Colocation defines the boundaries of our modules. We try our best to limit our classes such they they contain and colocate anything that can impact the results of using that class. Everything that can impact the results should be versioned together. Things like the prompt template, temperature, model, and other call parameters. When extracting structured information, the schema to be extracted should be colocated with the call for extracting it. The prompt template for that extraction should be engineered to best extract that schema. It should all be versioned together.</p>"},{"location":"concepts/philosophy/#5-extensible","title":"5. Extensible","text":"<p>We\u2019re building a toolkit, not a framework. Mirascope hopes to make building your own AI tools as delightful as possible. This is similar in spirit to something like shadcn.</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/","title":"RAG (Retrieval-Augmented Generation)","text":"<p>Retrieval-Augmented Generation (RAG) at a high level is a technique used to pull in knowledge outside an LLM\u2019s training data to generate a response. This technique uses a vector database to store custom information such as company documents, databases, or other private information that is typically inaccessible in a vector format. When a user submits a query to the LLM, it first makes a semantic search in the vector database. The relevant data is then retrieved and included into the LLM's context giving more accurate results with reduced hallucination.</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#why-use-rag","title":"Why use RAG?","text":"<p>RAG plays a crucial role in addressing significant challenges associated with LLMs:</p> <ol> <li>Hallucinations: By providing the LLM with context and current information, RAG helps produce more accurate and relevant responses. This enhancement fosters user trust and reliability.</li> <li>Cost: Training new models with expanded knowledge is both time-consuming and financially demanding. RAG circumvents these issues by supplements the model with additional data without the need for retraining.</li> <li>Domain Knowledge: LLMs are trained on diverse data sets and lack the specificity required for certain tasks. RAG enables targeted knowledge making LLMs more adept at handling specialized requirements.</li> </ol>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#rag-in-mirascope","title":"RAG in Mirascope","text":"<p>A big focus on Mirascope RAG is not to reinvent how RAG is implemented but to speed up development by providing convenience. This is broken down into three main parts, <code>Chunkers</code> , <code>Embedders</code>, and <code>VectorStores</code> .</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#chunkers-splitters","title":"Chunkers (Splitters)","text":"<p>Chunkers are the first step in setting up a RAG flow. To put it simply, long text or other forms of documents are split into smaller chunks to be stored. These smaller chunks help semantic search find information quicker and more accurately. In Mirascope, every Chunker extends <code>BaseChunker</code> which only has one requirement, the chunk function:</p> <pre><code>import uuid\nfrom mirascope.rag.chunkers import BaseChunker\nfrom mirascope.rag.types import Document\n\nclass TextChunker(BaseChunker):\n    \"\"\"A text chunker that splits a text into chunks of a certain size and overlaps.\"\"\"\n\n    chunk_size: int\n    chunk_overlap: int\n\n    def chunk(self, text: str) -&gt; list[Document]:\n        chunks: list[Document] = []\n        start: int = 0\n        while start &lt; len(text):\n            end: int = min(start + self.chunk_size, len(text))\n            chunks.append(Document(text=text[start:end], id=str(uuid.uuid4())))\n            start += self.chunk_size - self.chunk_overlap\n        return chunks\n</code></pre> <p>This is a simple example. You can use pre-existing chunkers like <code>TextChunker</code> or create your own by extending <code>BaseChunker</code> and implementing your own chunk function.</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#embedders","title":"Embedders","text":"<p>The next step would be to take the text chunks and embed them into vectors.</p> <p>We currently only support OpenAI Embeddings but will soon support more.</p> <pre><code>import os\nfrom mirascope.openai import OpenAIEmbedder\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nembedder = OpenAIEmbedder()\nresponse = embedder.embed([\"your_message_to_embed\"])\n</code></pre> <p>Most of the time you will not need to call our Embedder classes directly, but you are free to extend our <code>BaseEmbedder</code> for more specific types of embeddings.</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#vectorstores","title":"VectorStores","text":"<p>The final step to put it all together is to connect to a VectorStore and start using RAG. In this example we will be using Chroma DB, with the same <code>TextChunker</code> and <code>OpenAIEmbedder</code> :</p> <pre><code># your_repo.stores.py\nfrom mirascope.chroma import ChromaSettings, ChromaVectorStore\nfrom mirascope.openai import OpenAIEmbedder\nfrom mirascope.rag import TextChunker\n\nclass MyStore(ChromaVectorStore):\n    embedder = OpenAIEmbedder()\n    chunker = TextChunker(chunk_size=1000, chunk_overlap=200)\n    index_name = \"my_index\"\n</code></pre> <p>Just like that, your RAG system is ready to be used. Note that we are settings class variables to snapshot this configuration for a particular index, so when you use this store across multiple applications, there is consistency.</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#add-documents","title":"Add Documents","text":"<p>Anytime you have new documents, you can add to your vectorstore with a few lines of code.</p> <pre><code>import os\nfrom your_repo.stores import MyStore\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfor file_name in os.listdir(\"YOUR_DIRECTORY\"):\n    with open(f\"YOUR_DIRECTORY/{file_name}\") as file:\n        data = file.read()\n        store = MyStore()\n        store.add(data)\n</code></pre>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#retrieve-documents","title":"Retrieve Documents","text":"<p>Combined with Mirascope Calls, you now have a RAG powered application.</p> <pre><code>import os\nfrom your_repo.stores import MyStore\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nclass QuestionAnswerer(OpenAICall):\n    prompt_template = \"\"\"\n    SYSTEM: \n    Answer the question based on the context.\n    {context}\n    USER: \n    {question}\n    \"\"\"\n\n    question: str\n\n    store: MyStore = MyStore()\n\n    @property\n    def context(self):\n        return self.store.retrieve(self.question).documents\n\nresponse = QuestionAnswerer(question=\"{YOUR_QUESTION_HERE}\").call()\nprint(response.content)\n</code></pre> <p>Note that the <code>context</code> property returns a list of document text strings, which will automatically get parsed into the system message with <code>\\n</code> separators. </p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#access-client-and-index","title":"Access Client and Index","text":"<p>You can access the VectorStore client and index by getting the private property <code>_client</code> and <code>_index</code> respectively. This is useful when you need to access VectorStore functionality such as <code>delete</code>.</p> <pre><code>from your_repo.stores import MyStore\n\nstore = MyStore()\nstore._client\nstore._index\n</code></pre>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#other-integrations","title":"Other Integrations","text":"<p>You can swap out your <code>Chunkers</code>, <code>Embedders</code>, and <code>VectorStores</code> by simply updating the imports.</p> <p>We've also designed Mirascope RAG so that it's easy to swap in other RAG tooling (e.g. Llama Index)</p> <p>Let us know who you would like us to integrate with next.</p>"},{"location":"concepts/rag_%28retrieval_augmented_generation%29/#roadmap","title":"Roadmap","text":"<ul> <li> Pinecone</li> <li> Astra DB</li> <li> Cohere Embeddings</li> <li> HuggingFace</li> </ul>"},{"location":"concepts/streaming_generated_content/","title":"Streaming generated content","text":"<p>Streaming generated content is similar to Generating Content so check that out if you haven\u2019t already.</p>"},{"location":"concepts/streaming_generated_content/#openaiprompt","title":"OpenAIPrompt","text":"<p>We will be using the same <code>OpenAICall</code> in Generating Content. Feel free to swap it out with a different provider.</p>"},{"location":"concepts/streaming_generated_content/#streaming","title":"Streaming","text":"<p>You can use the <code>stream</code> method to stream a response. All this is doing is setting <code>stream=True</code> and providing the <code>OpenAICallResponseChunk</code> convenience wrappers around the response chunks.</p> <pre><code>import os\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nstream = RecipeRecommender(ingredient=\"apples\").stream()\n\nfor chunk in stream:\n    print(chunk.content, end=\"\")\n</code></pre>"},{"location":"concepts/streaming_generated_content/#async","title":"Async","text":"<p>If you want concurrency, you can use the <code>stream_async</code> function instead.</p> <pre><code>import os\n\nfrom mirascope import OpenAIPrompt, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAIPrompt):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n\nasync def stream_recipe_recommendation():\n    \"\"\"Asynchronously streams the response for a call to the model using `OpenAICall`.\"\"\"\n    stream = RecipeRecommender(ingredient=\"apples\").async_stream()\n    async for chunk in astream:\n        print(chunk.content, end=\"\")\n\nasyncio.run(stream_recipe_recommendation())\n</code></pre>"},{"location":"concepts/streaming_generated_content/#openaicallresponsechunk","title":"OpenAICallResponseChunk","text":"<p>The <code>stream</code> method returns an <code>OpenAICallResponseChunk</code> instance, which is a convenience wrapper around the <code>ChatCompletionChunk</code> class in <code>openai</code></p> <pre><code>from mirascope.openai.types import OpenAIChatCompletionChunk\n\nchunk = OpenAICallResponseChunk(...)\n\nchunk.content  # original.choices[0].delta.content\nchunk.delta    # original.choices[0].delta\nchunk.choice   # original.choices[0]\nchunk.choices  # original.choices\nchunk.chunk    # ChatCompletionChunk(...)\n</code></pre>"},{"location":"concepts/streaming_tools_and_structured_outputs/","title":"Streaming Tools and Structured Outputs","text":"<p>When using tools (function calling) or extracting structured information, there are many instances in which you will want to stream the results. For example, consider making a call to an LLM that responds with multiple tool calls. Your system can have more real-time behavior if you can call each tool as it's returned instead of having to wait for all of them to be generated at once. Another example would be when returning structured information to a UI. Streaming the information enables real-time generative UI that can be generated as the fields are streamed.</p> <p>Note</p> <p>Currently streaming tools is only supported for OpenAI and Anthropic. We will aim to add support for other model providers when available in their APIs.</p>"},{"location":"concepts/streaming_tools_and_structured_outputs/#streaming-tools-function-calling","title":"Streaming Tools (Function Calling)","text":"<p>To stream tools, first call <code>stream</code> instead of <code>call</code> for an LLM call with tools. Then use the matching provider's tool stream class to stream the tools:</p> <pre><code>import os\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams, OpenAIToolStream\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\ndef print_book(title: str, author: str, description: str):\n    \"\"\"Prints the title and author of a book.\"\"\"\n    return f\"Title: {title}\\nAuthor: {author}\\nDescription: {description}\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend some books to read.\"\n\n    call_params = OpenAICallParams(tools=[print_book])\n\n\nstream = BookRecommender().stream()\ntool_stream = OpenAIToolStream.from_stream(stream)\nfor tool in tool_stream:\n    tool.fn(**tool.args)\n#&gt; Title: The Name of the Wind\\nAuthor: Patrick Rothfuss\\nDescription: ...\n#&gt; Title: Dune\\nAuthor: Frank Herbert\\nDescription: ...\n#&gt; ...\n</code></pre>"},{"location":"concepts/streaming_tools_and_structured_outputs/#streaming-partial-tools","title":"Streaming Partial Tools","text":"<p>Sometimes you may want to stream partial tools as well (i.e. the unfinished tool call with <code>None</code> for arguments that haven't yet been streamed). This can be useful for example when observing an agent's flow in real-time. You can simple set <code>allow_partial=True</code> to access this feature. In the following code example, we stream each partial tool and update a live console, printing each full tool call before moving on to the next:</p> <pre><code>import os\nimport time\n\nfrom rich.live import Live\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams, OpenAIToolStream\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\ndef print_book(title: str, author: str, description: str):\n    \"\"\"Prints the title and author of a book.\"\"\"\n    return f\"Title: {title}\\nAuthor: {author}\\nDescription: {description}\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend some books to read.\"\n\n    call_params = OpenAICallParams(tools=[print_book])\n\n\nstream = BookRecommender().stream()\ntool_stream = OpenAIToolStream.from_stream(stream, allow_partial=True)\n\nwith Live(\"\", refresh_per_second=15) as live:\n    partial_tools, index = [None], 0\n    previous_tool = None\n    for partial_tool in tool_stream:\n        if partial_tool is None:\n            index += 1\n            partial_tools.append(None)\n            continue\n        partial_tools[index] = partial_tool\n        live.update(\n            \"\\n-----------------------------\\n\".join(\n                [pt.fn(**pt.args) for pt in partial_tools]\n            )\n        )\n        time.sleep(0.1)\n</code></pre>"},{"location":"concepts/streaming_tools_and_structured_outputs/#streaming-pydantic-models","title":"Streaming Pydantic Models","text":"<p>You can also stream structured outputs when using an extractor. Simply call the <code>stream</code> function to stream partial outputs:</p> <pre><code>import os\nfrom typing import Literal, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass TaskDetails(BaseModel):\n    title: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n    due_date: str\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n\n    prompt_template = \"\"\"\n    Please extract the task details:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask_description = \"Submit quarterly report by next Friday. Task is high priority.\"\nstream = TaskExtractor(task=task_description).stream()\nfor partial_model in stream:\n    print(partial_model)\n#&gt; title='Submit quarterly report' priority=None due_date=None\n#&gt; title='Submit quarterly report' priority='high' due_date=None\n#&gt; title='Submit quarterly report' priority='high' due_date='next Friday'\n</code></pre>"},{"location":"concepts/supported_llm_providers/","title":"Supported LLM Providers","text":"<p>With new models dropping every week, it's important to be able to quickly test out different models. This can be an easy and powerful way to boost the performance of your application. Mirascope provides a unified interface that makes it fast and simple to swap between various providers.</p> <p>We currently support the following providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Mistral</li> <li>Cohere</li> <li>Groq</li> <li>Gemini</li> </ul> <p>This also means that we support any providers that use these APIs.</p> <p>Note</p> <p>If there\u2019s a provider you would like us to support that we don't yet support, please request the feature on our GitHub Issues page or contribute a PR yourself.</p>"},{"location":"concepts/supported_llm_providers/#examples","title":"Examples","text":"<p>For the majority of cases, switching providers when using Mirascope requires just 2-3 changes:</p> <ol> <li>Change the <code>mirascope.{provider}</code> import to the new provider</li> <li>Update your class to use the new provider</li> <li>Update any specific call params such as <code>model</code></li> </ol>"},{"location":"concepts/supported_llm_providers/#calls","title":"Calls","text":"OpenAIAnthropicMistralCohereGroqGemini <pre><code>from mirascope.openai import OpenAICall, OpenAICallParams\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = OpenAICallParams(model=\"gpt-4-turbo\")\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre> <pre><code>from mirascope.anthropic import AnthropicCall, AnthropicCallParams\n\n\nclass BookRecommender(AnthropicCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = AnthropicCallParams(model=\"claude-3-haiku-20240307\")\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre> <pre><code>from mirascope.mistral import MistralCall, MistralCallParams\n\n\nclass BookRecommender(MistralCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = MistralCallParams(model=\"mistral-large-latest\")\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre> <pre><code>from mirascope.cohere import CohereCall, CohereCallParams\n\n\nclass BookRecommender(CohereCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = CohereCallParams(model=\"command-r-plus\")\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre> <pre><code>from mirascope.groq import GroqCall, GroqCallParams\n\n\nclass BookRecommender(GroqCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = GroqCallParams(model=\"mixtral-8x7b-32768\")\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre> <pre><code>from mirascope.gemini import GeminiCall, GeminiCallParams\n\n\nclass BookRecommender(GeminiCall):\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro-latest\")\n\n\nresponse = BookRecommender(genre=\"fantasy\").call()\nprint(response.content)\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre>"},{"location":"concepts/supported_llm_providers/#extractors","title":"Extractors","text":"OpenAIAnthropicMistralCohereGroqGemini <pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel\n\nfrom mirascope.openai import OpenAIExtractor\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# &gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre> <pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel\n\nfrom mirascope.anthropic import AnthropicExtractor\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(AnthropicExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# &gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre> <pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel\n\nfrom mirascope.mistral import MistralExtractor\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(MistralExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# &gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre> <pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel\n\nfrom mirascope.cohere import CohereExtractor\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(CohereExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# &gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre> <pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel\n\nfrom mirascope.groq import GroqExtractor\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(GroqExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# &gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre> <pre><code>from typing import Literal, Type\n\nfrom pydantic import BaseModel\n\nfrom mirascope.gemini import GeminiExtractor\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\nclass TaskExtractor(GeminiExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n# &gt; description='Submit quarterly report' due_date='next Friday' priority='high'\n</code></pre> <p>Note</p> <p>If you are accessing features that are specific to a particular provider, switching can require a little more work. For example, Gemini has a different structure for messages from other providers, so if you're writing your own <code>messages</code> function, you'll need to make sure you update to conform to the new provider's messages structure. See the example section below for more details.</p>"},{"location":"concepts/supported_llm_providers/#settings-base_url-and-api_key","title":"Settings <code>base_url</code> and <code>api_key</code>","text":"<p>If you want to use a proxy, you can easily set the <code>base_url</code> class variable for any call. You can do the same to set the <code>api_key</code>. This is key for using providers such as Ollama, Anyscale, Together, and others that support the <code>OpenAI</code> API through a proxy.</p> <p>Here's an example using Ollama:</p> <pre><code>import os\n\nfrom mirascope.openai import OpenAICall, OpenAICallParams\n\n\nclass RecipeRecommender(OpenAICall):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    api_key = \"ollama\"\n    base_url = \"http://localhost:11434/v1\"\n    call_params = OpenAICallParams(model=\"mistral\")\n</code></pre>"},{"location":"concepts/supported_llm_providers/#more-detailed-walkthrough-of-swapping-providers-openai-gemini","title":"More detailed walkthrough of swapping providers (OpenAI -&gt; Gemini)","text":"<p>The generative-ai library and openai-python library are vastly different from each other, so swapping between them to attempt to gain better prompt responses is not worth the engineering effort and maintenance. </p> <p>This leads to people typically sticking with one provider even when providers release new features frequently. Take for example when Google announced Gemini 1.5, it would be very useful to implement prompts with the new context window. Thankfully, Mirascope makes this swap trivial.</p>"},{"location":"concepts/supported_llm_providers/#assuming-you-are-starting-with-openaicall","title":"Assuming you are starting with OpenAICall","text":"<pre><code>import os\nfrom mirascope import OpenAICall, OpenAICallParams\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass RecipeRecommender(OpenAICall):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"...\")\n\n\nresponse = RecipeRecommender(ingredient=\"apples\").call()\nprint(response.content)\n</code></pre> <ol> <li> <p>First install Mirascope\u2019s integration with gemini if you haven\u2019t already.</p> <pre><code>pip install mirascope[gemini]\n</code></pre> </li> <li> <p>Swap out OpenAI with Gemini:</p> <ol> <li>Replace <code>OpenAICall</code> and <code>OpenAICallParams</code> with <code>GeminiCall</code> and <code>GeminiCallParams</code> respectively </li> <li>Configure your Gemini API Key</li> <li>Update <code>GeminiCallParams</code> with new model and other attributes</li> </ol> </li> </ol> <pre><code>from google.generativeai import configure\nfrom mirasope.gemini import GeminiCall, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass RecipeRecommender(GeminiCall):\n    prompt_template = \"Recommend recipes that use {ingredient} as an ingredient\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n\n\nresponse = RecipeRecommender(ingredient=\"apples\").call()\nprint(response.content)\n</code></pre> <p>That\u2019s it for the basic example! Now you can evaluate the quality of your prompt with Gemini.</p>"},{"location":"concepts/supported_llm_providers/#something-a-bit-more-advanced","title":"Something a bit more advanced","text":"<p>What if you want to use a more complex message? The steps above are all the same except with one extra step.</p> <p>Consider this OpenAI example:</p> <pre><code>from mirascope import OpenAICall, OpenAICallParams\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass Recipe(OpenAICall):\n    ingredient: str\n\n    call_params = OpenAICallParams(model=\"gpt-3.5-turbo-0125\")\n\n    @property\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\"role\": \"system\", \"content\": \"You are the world's greatest chef.\"},\n            {\"role\": \"user\", \"content\": f\"Can you recommend some recipes that use {self.ingredient} as an ingredient?\"},\n        ]\n</code></pre> <p>The Gemini example will look like this:</p> <pre><code>from google.generativeai import configure\nfrom google.generativeai.types import ContentsType\nfrom mirasope.gemini import GeminiCall, GeminiCallParams\n\nconfigure(api_key=\"YOUR_GEMINI_API_KEY\")\n\n\nclass Recipe(GeminiCall):\n    \"\"\"A normal docstring\"\"\"\n\n    ingredient: str\n\n    call_params = GeminiCallParams(model=\"gemini-1.0-pro\")\n\n    @property\n    def messages(self) -&gt; ContentsType:\n        return [\n            {\"role\": \"user\", \"parts\": [\"You are the world's greatest chef.\"]},\n            {\"role\": \"model\", \"parts\": [\"I am the world's greatest chef.\"]},\n            {\"role\": \"user\", \"parts\": [f\"Can you recommend some recipes that use {self.ingredient} as an ingredient?\"]},\n        ]\n</code></pre> <p>Update the return type from <code>list[ChatCompletionMessageParam]</code> to <code>ContentsType</code> for the <code>messages</code> method. Gemini doesn\u2019t have a <code>system</code> role, so instead we need to simulate OpenAI\u2019s <code>system</code> message using a <code>user</code> \u2192 <code>model</code> pair. Refer to the providers documentation on how to format their messages array.</p>"},{"location":"concepts/tools_%28function_calling%29/","title":"Tools (Function Calling)","text":"<p>Large Language Models (LLMs) are incredibly powerful at generating human-like text, but their capabilities extend far beyond mere text generation. With the help of tools (often called function calling), LLMs can perform a wide range of tasks, from mathematical calculations to code execution and information retrieval.</p>"},{"location":"concepts/tools_%28function_calling%29/#what-are-tools","title":"What are Tools?","text":"<p>Tools, in the context of LLMs, are essentially functions or APIs that the model can call upon to perform specific tasks. These tools can range from simple arithmetic operations to complex web APIs or custom-built functions. By leveraging tools, LLMs can augment their capabilities and provide more accurate and useful outputs.</p>"},{"location":"concepts/tools_%28function_calling%29/#why-are-tools-important","title":"Why are Tools Important?","text":"<p>Traditionally, LLMs have been limited to generating text based solely on their training data and the provided prompt. While this approach can produce impressive results, it also has inherent limitations. Tools allow LLMs to break free from these constraints by accessing external data sources, performing calculations, and executing code, among other capabilities.</p> <p>Incorporating tools into LLM workflows opens up a wide range of possibilities, including:</p> <ol> <li>Improved Accuracy: By leveraging external data sources and APIs, LLMs can provide more accurate and up-to-date information, reducing the risk of hallucinations or factual errors.</li> <li>Enhanced Capabilities: Tools allow LLMs to perform tasks that would be challenging or impossible with text generation alone, such as mathematical computations, code execution, and data manipulation.</li> <li>Contextualized Responses: By incorporating external data and contextual information, LLMs can provide more relevant and personalized responses, tailored to the user's specific needs and context.</li> </ol>"},{"location":"concepts/tools_%28function_calling%29/#tools-in-mirascope","title":"Tools in Mirascope","text":"<p>Mirascope provides a clean and intuitive way to incorporate tools into your LLM workflows. The simplest form-factor we offer is to extract a single tool automatically generated from a function. We can then call that function with the extracted arguments:</p> <pre><code>from mirascope.openai import OpenAICall\n\n\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get's the weather for `location` and prints it.\n\n    Args:\n        location: The \"City, State\" or \"City, Country\" for which to get the weather.\n    \"\"\"\n    print(location)\n    if location == \"Tokyo, Japan\":\n        return f\"The weather in {location} is 72 degrees and sunny.\"\n    elif location == \"San Francisco, CA\":\n        return f\"The weather in {location} is 45 degrees and cloudy.\"\n    else:\n        return f\"I'm sorry, I don't have the weather for {location}.\"\n\n\nclass Forecast(OpenAICall):\n    \"\"\"What's the weather in Tokyo?\"\"\"\n\n\nweather_tool = Forecast().extract(get_weather)\nprint(weather_tool.fn(**weather_tool.args))\n#&gt; The weather in Tokyo, Japan is 72 degrees and sunny.\n</code></pre> <p>Note</p> <p>While it may not be clear from the above example, <code>tool.fn</code> is an extremely powerful simplification. When using multiple tools, having the function attached to the tool makes it immediately accessible and callable with a single line of code.</p> <p>In the following pages, we\u2019ll go into greater detail around how to define and use tools effectively.</p>"},{"location":"concepts/validation/","title":"Validation","text":"<p>When extracting structured information from LLMs, it\u2019s important that we validate the extracted information, especially the types. We want to make sure that if we\u2019re looking for an integer that we actual get an <code>int</code> back. One of the primary benefits of building on top of Pydantic is that it makes validation easy \u2014 in fact, we get validation on types out-of-the-box.</p> <p>We recommend you check out their thorough documentation for detailed information on everything you can do with their validators. This document will be brief and specifically related to LLM extraction to avoid duplication.</p>"},{"location":"concepts/validation/#validating-types","title":"Validating Types","text":"<p>When we extract information \u2014 for base types, <code>BaseModel</code>, or any of our tools \u2014 everything is powered by Pydantic. This means that we automatically get type validation and can handle it gracefully:</p> <pre><code>from typing import Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel, ValidationError\n\n\nclass Book(BaseModel):\n    title: str\n    price: float\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a book.\"\n\n\ntry:\n    book = BookRecommender().extract()\n    assert isinstance(book, Book)\n    print(book)\n    #&gt; title='The Alchemist' price=12.99\nexcept ValidationError as e:\n    print(e)\n  #&gt; 1 validation error for Book\n  #  price\n  #    Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='standard', input_type=str]\n  #      For further information visit https://errors.pydantic.dev/2.6/v/float_parsing\n</code></pre> <p>Now we can proceed with our extracted information knowing that it will behave as the expected type.</p>"},{"location":"concepts/validation/#custom-validation","title":"Custom Validation","text":"<p>It\u2019s often useful to write custom validation when working with LLMs so that we can automatically handle things that are difficult to hard-code. For example, consider determining whether the generated content adheres to your company\u2019s guidelines. It\u2019s a difficult task to determine this, but an LLM is well-suited to do the task well.</p> <p>We can use an LLM to make the determination by adding an <code>AfterValidator</code> to our extracted output:</p> <pre><code>from enum import Enum\nfrom typing import Annotated, Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\nclass Label(Enum):\n    HAPPY = \"happy story\"\n    SAD = \"sad story\"\n\n\nclass Sentiment(OpenAIExtractor[Label]):\n    extract_schema: Type[Label] = Label\n    prompt_template = \"Is the following happy or sad? {text}.\"\n\n    text: str\n\n\ndef validate_happy(story: str) -&gt; str:\n    \"\"\"Check if the content follows the guidelines.\"\"\"\n    label = Sentiment(text=story).extract()\n    assert label == Label.HAPPY, \"Story wasn't happy.\"\n    return story\n\n\nclass HappyStory(BaseModel):\n    story: Annotated[str, AfterValidator(validate_happy)]\n\n\nclass StoryTeller(OpenAIExtractor[HappyStory]):\n    extract_template: Type[HappyStory] = HappyStory\n    prompt_template = \"Please tell me a story that's really sad.\"\n\n\ntry:\n    story = StoryTeller().extract()\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for HappyStoryTool\n    #   story\n    #     Assertion failed, Story wasn't happy. [type=assertion_error, input_value=\"Once upon a time, there ...er every waking moment.\", input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n</code></pre>"},{"location":"concepts/writing_prompts/","title":"Writing prompts","text":""},{"location":"concepts/writing_prompts/#the-baseprompt-class","title":"The <code>BasePrompt</code> Class","text":"<p>The <code>prompt_template</code> class variable is the template used to generate the list of messages. The properties of the class are the template variables, which get formatted into the template during the creation of the list of messages.</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages())\n#&gt; [{\"role\": \"user\", \"content\": \"Can you recommend some books on coding?\"}]\n</code></pre> <p>The <code>messages</code> method parses the <code>prompt_template</code> into a list of messages. In this case, there's just a single user message.</p>"},{"location":"concepts/writing_prompts/#editor-support","title":"Editor Support","text":"<ul> <li> <p>Inline Errors</p> <p></p> </li> <li> <p>Autocomplete</p> <p></p> </li> </ul>"},{"location":"concepts/writing_prompts/#template-variables","title":"Template Variables","text":"<p>When you call <code>prompt.messages()</code> or <code>str(prompt)</code> the template will be formatted  using the fields and properties of the class that match the template variables. This means that you can define more complex properties through code using the built in python decorator <code>@property</code>. This is particularly useful when you want to inject template variables with custom formatting or template variables that depend on multiple attributes.</p> <pre><code>from mirascope import BasePrompt\n\nclass BasicAddition(BasePrompt):\n    prompt_template = \"\"\"\n    Can you solve this math problem for me?\n    {equation}\n    \"\"\"\n    first_number: float\n    second_number: float\n\n    @property\n    def equation(self) -&gt; str:\n        return f\"{self.first_number}+{self.second_number}=\"\n</code></pre> <p>If the type being returned is a list we will automatically format <code>list</code> and <code>list[list]</code> fields and properties with <code>\\n</code> and <code>\\n\\n</code> separators, respectively and stringify.</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    Can you recommend some books on the following topic and genre pairs?\n    {topics_x_genres}\n    \"\"\"\n\n    topics: list[str]\n    genres: list[str]\n\n    @property\n    def topics_x_genres(self) -&gt; list[str]:\n        \"\"\"Returns `topics` as a comma separated list.\"\"\"\n        return [\n            f\"Topic: {topic}, Genre: {genre}\"\n            for topic in self.topics\n            for genre in self.genre\n        ]\n\n\nprompt = BookRecommendationPrompt(\n    topics=[\"coding\", \"music\"], genres=[\"fiction\", \"fantasy\"]\n)\nprint(prompt)\n#&gt; Can you recommend some books on the following topic and genre pairs?\n#  Topic: coding, Genre: fiction\n#  Topic: coding, Genre: fantasy\n#  Topic: music, Genre: fiction\n#  Topic: music, Genre: fantasy\n</code></pre>"},{"location":"concepts/writing_prompts/#messages","title":"Messages","text":"<p>By default, the <code>BasePrompt</code> class treats the prompt template as a single user message. If you want to specify a list of messages instead, use the message keywords SYSTEM, USER, ASSISTANT, MODEL, or TOOL (depending on what's supported by your choice of provider):</p> <pre><code>from mirascope import BasePrompt\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nprint(prompt.messages())\n</code></pre> <pre><code>[{\"role\": \"system\", \"content\": \"You are the world's greatest librarian\"}, {\"role\": \"user\", \"content\": \"Can you recommend some books on coding?\"}]\n</code></pre> <p>Note</p> <p>This example is using Mirascope base `Message. If you are using a different provider, refer to the provider's documentation on their message roles.</p>"},{"location":"concepts/writing_prompts/#magic-is-optional","title":"Magic is optional","text":"<p>We understand that there are users that do not want to use template string magic. Mirascope allows the user to write the messages array manually, which has the added benefit of accessing functionality that is not yet supported by the template parser (such as Vision support).</p> <pre><code>from mirascope import BasePrompt\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    topic: str\n\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n                        },\n                    },\n                ],\n            },\n        ]\n</code></pre>"},{"location":"concepts/writing_prompts/#integrations-with-providers","title":"Integrations with Providers","text":"<p>The <code>BasePrompt</code> class should be used for providers that are not yet supported by Mirascope. Pass in the messages from the prompt into the LLM provider client messages array.</p> <pre><code>from mirascope import BasePrompt\nfrom some_llm_provider import LLMProvider\n\n\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n\n    USER:\n    Can you recommend some books on {topic}?\n    \"\"\"\n\n    topic: str\n\n\nprompt = BookRecommendationPrompt(topic=\"coding\")\nclient = LLMProvider(api_key=...)\nmessage = client.messages.create(\n    model=\"some-model\",\n    max_tokens=1000,\n    temperature=0.0,\n    stream=False,\n    messages=prompt.messages()\n)\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":"<p>Warning</p> <p>This page is under construction...for now check out our README and Concepts pages.</p>"},{"location":"integrations/client_wrappers/","title":"Client Wrappers","text":"<p>If you want to use Mirascope in conjunction with another library which implements a client wrapper (such as LangSmith), you can do so easily by setting the <code>wrapper</code> parameter within your call parameters. For example, setting this call parameter on an <code>OpenAICall</code> will internally wrap the <code>OpenAI</code> client within an <code>OpenAICall</code>, giving you access to both sets of functionalities. This will work for any of the providers we support.</p> <pre><code>from some_library import some_wrapper\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\",\n        wrapper=some_wrapper\n    )\n</code></pre> <p>Now, every call to <code>call</code>, <code>call_async</code>, <code>stream</code>, and <code>stream_async</code> will be executed on top of the wrapped <code>OpenAI</code> client.</p>"},{"location":"integrations/fastapi/","title":"FastAPI","text":"<p>Because we've build everything on top of Pydantic, we automatically integrate seamlessly with FastAPI. This was by design to reduce the overhead of writing out a separate schema for your endoint.</p> <pre><code>import os\nfrom typing import Type\n\nfrom fastapi import FastAPI\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\napp = FastAPI()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: Type[Book] = Book\n    prompt_template = \"Please recommend a {genre} book.\"\n\n    genre: str\n\n\n@app.post(\"/\")\ndef root(book_recommender: BookRecommender) -&gt; Book:\n    \"\"\"Generates a book based on provided `genre`.\"\"\"\n    return book_recommender.extract()\n</code></pre>"},{"location":"integrations/langchain/","title":"LangChain and LangSmith","text":""},{"location":"integrations/langchain/#using-mirascope-baseprompt-with-langchain","title":"Using Mirascope <code>BasePrompt</code> with LangChain","text":"<p>You may also want to use LangChain given it\u2019s tight integration with LangSmith. For us, one issue we had when we first started using LangChain was that their <code>invoke</code> function had no type-safety or lint help. This means that calling <code>invoke({\"foox\": \"foo\"})</code> was a difficult bug to catch. There\u2019s so much functionality in LangChain, and we wanted to make using it more pleasant.</p> <p>With Mirascope prompts, you can instantiate a <code>ChatPromptTemplate</code> from a Mirascope prompt template, and you can use the prompt\u2019s <code>model_dump</code> method so you don\u2019t have to worry about the invocation dictionary:</p> <pre><code>import os\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom mirascope import BasePrompt\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n\n\nclass JokePrompt(BasePrompt):\n    prompt_template = \"Tell me a short joke about {topic}\"\n\n    topic: str\n\n\njoke_prompt = JokePrompt(topic=\"ice cream\")\nprompt = ChatPromptTemplate.from_template(joke_prompt.template())\n# ^ instead of:\n# prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n\nmodel = ChatOpenAI(model=\"gpt-4\")\noutput_parser = StrOutputParser()\nchain = prompt | model | output_parser\n\njoke = chain.invoke(joke_prompt.model_dump())\n# ^ instead of:\n# joke = chain.invoke({\"topic\": \"ice cream\"})\nprint(joke)\n</code></pre>"},{"location":"integrations/langchain/#logging-a-langsmith-trace","title":"Logging a LangSmith trace","text":"<p>You can use client wrappers (as mentioned in client wrappers) to integrate Mirascope with LangSmith. When using a wrapper, you can generate content as you would with a normal <code>OpenAICall</code>:</p> <pre><code>import os\nfrom langsmith import wrappers\n\nfrom mirascope.openai import OpenAICall\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Can you recommend some books on {topic}?\"\n\n    topic: str\n\n    call_params = OpenAICallParams(\n        model=\"gpt-3.5-turbo\",\n        wrapper=wrappers.wrap_openai\n    )\n\nresponse = BookRecommender(topic=\"sci-fi\").call()\n</code></pre> <p>Now, if you log into LangSmith , you will be see your results have been traced. Of course, this integration works not just for <code>call</code>, but also for <code>stream</code> and <code>extract</code>.</p>"},{"location":"integrations/llama_index/","title":"Llama Index","text":"<p>Since Mirascope RAG is plug-and-play, you can easily use Llama Index for all of your RAG needs while still taking advantage of everything else Mirascope has to offer.</p> <pre><code>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom mirascope.anthropic import AnthropicCall\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader(\"./paul_graham_essays\").load_data()\nretriever = VectorStoreIndex.from_documents(documents).as_retriever()\n\n# Create Paul Graham Bot\nclass PaulGrahamBot(AnthropicCall):\n    prompt_template = \"\"\"\n    SYSTEM:\n    Your task is to respond to the user as though you are Paul Graham.\n\n    Here are some excerpts from Paul Graham's essays relevant to the user query.\n    Use them as a reference for how to respond.\n\n    &lt;excerpts&gt;\n    {excerpts}\n    &lt;/excepts&gt;\n    \"\"\"\n\n    query: str = \"\"\n\n    @property\n    def excerpts(self) -&gt; list[str]:\n        \"\"\"Retrieves excerpts from Paul Graham's essays relevant to `query`.\"\"\"\n        return [node.get_content() for node in retriever.retrieve(self.query)]\n\n\npg = PaulGrahamBot()\npg.query = input(\"User: \")\nresponse = pg.call()\nprint(response.content)\n</code></pre>"},{"location":"integrations/weights_and_biases/","title":"Weights &amp; Biases","text":"<p>If you want to seamlessly use Weights &amp; Biases\u2019 logging functionality, we\u2019ve got you covered</p>"},{"location":"integrations/weights_and_biases/#weave","title":"Weave","text":"<p>Mirascope seamlessly integrates with Weave with just a few lines of code. You can use it with any <code>BaseCall</code> or <code>BaseExtractor</code> extension such as <code>OpenAICall</code> or <code>AnthropicCall</code>. Simply add the <code>with_weave</code> decorator to your class and the <code>call</code>, <code>call_async</code>, <code>stream</code>, <code>stream_async</code>, <code>extract</code>, and <code>extract_async</code> methods will be automatically logged to the Weave project you initialize.</p> <p>The below examples show how to use the <code>with_weave</code> decorator to automatically log your runs to Weave. We've highlighted the lines that we've added to the original example to demonstrate how easy it is to use Weave with Mirascope.</p>"},{"location":"integrations/weights_and_biases/#call-example","title":"Call Example","text":"<pre><code>import weave\n\nfrom mirascope.openai import OpenAICall\nfrom mirascope.wandb import with_weave\n\nweave.init(\"my-project\")\n\n\n@with_weave\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Please recommend some {genre} books\"\n\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()  # this will automatically get logged with weave\nprint(response.content)\n</code></pre>"},{"location":"integrations/weights_and_biases/#extract-example","title":"Extract Example","text":"<pre><code>from typing import Literal, Type\n\nimport weave\nfrom pydantic import BaseModel\n\nfrom mirascope.openai import OpenAIExtractor\nfrom mirascope.wandb import with_weave\n\nweave.init(\"scratch-test\")\n\n\nclass TaskDetails(BaseModel):\n    description: str\n    due_date: str\n    priority: Literal[\"low\", \"normal\", \"high\"]\n\n\n@with_weave\nclass TaskExtractor(OpenAIExtractor[TaskDetails]):\n    extract_schema: Type[TaskDetails] = TaskDetails\n    prompt_template = \"\"\"\n    Extract the task details from the following task:\n    {task}\n    \"\"\"\n\n    task: str\n\n\ntask = \"Submit quarterly report by next Friday. Task is high priority.\"\ntask_details = TaskExtractor(task=task).extract()  # this will be logged automatically\nassert isinstance(task_details, TaskDetails)\nprint(task_details)\n</code></pre>"},{"location":"integrations/weights_and_biases/#trace","title":"Trace","text":"<p><code>WandbCallMixin</code> is a mixin with creation methods that internally call W&amp;B\u2019s <code>Trace()</code> function so you can easily log your runs. For standard responses, you can use <code>call_with_trace()</code>, and for extractions, you can use <code>WandbExtractorMixin</code>'s <code>extract_with_trace</code> method. These mixins are agnostic to the LLM provider, so you can use it with any <code>BaseCall</code> or <code>BaseExtractor</code> extension such as <code>OpenAICall</code> or <code>AnthropicCall</code>.</p>"},{"location":"integrations/weights_and_biases/#generating-content-with-a-wb-trace","title":"Generating Content with a W&amp;B Trace","text":"<p>The <code>call_with_trace()</code> function internally calls both <code>call()</code> and <code>wandb.Trace()</code> and is configured to properly log both successful completions and errors.</p> <p>Note that unlike a standard call, it requires the argument <code>span_type</code> to specify the type of <code>Trace</code> it initializes.  Once called, it will return a tuple of the call response and the created span <code>Trace</code>.</p> <pre><code>import os\nfrom mirascope.openai import OpenAICall, OpenAICallResponse\nfrom mirascope.wandb import WandbCallMixin\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Explainer(OpenAICall, WandbCallMixin[OpenAICallResponse]):\n    prompt_template = \"Tell me more about {topic} in detail.\"\n\n    topic: str\n\n\nexplainer = Explainer(span_type=\"llm\", topic=\"the Roman Empire\")\nresponse, span = explainer.call_with_trace()\nspan.log(name=\"my_trace\")\n</code></pre> <p>In addition, <code>call_with_trace</code> can take an argument  <code>parent</code> for chained calls, and the initialized <code>Trace</code> will be linked with its parent within W&amp;B logs.</p> <pre><code>import os\nfrom mirascope.openai import OpenAICall, OpenAICallResponse\nfrom mirascope.wandb import WandbCallMixin\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass Explainer(OpenAICall, WandbCallMixin[OpenAICallResponse]):\n    prompt_template = \"Tell me more about {topic} in detail.\"\n\n    topic: str\n\n\nclass Summarizer(OpenAICall, WandbCallMixin[OpenAICallResponse]):\n    prompt_template = \"Summarize the following: {text}\"\n\n    text: str\n\n\nexplainer = Explainer(span_type=\"llm\", topic=\"the Roman Empire\")\nresponse, explain_span = explainer.call_with_trace()\n\nsummarizer = Summarizer(span_type=\"llm\", text=explanation.content)\nresponse, _ = summarizer.call_with_trace(explain_span)\n\nexplain_span.log(name=\"my_trace\")\n</code></pre> <p>Since <code>WandbCallMixin</code> just adds a method to the call of your choice (e.g. <code>OpenAICall</code> as above), it will support function calling the same way you would a standard <code>OpenAICall</code>, as seen here</p>"},{"location":"integrations/weights_and_biases/#extracting-with-a-wb-trace","title":"Extracting with a W&amp;B Trace","text":"<p>When working with longer chains, it is often useful to use extractions so that data is passed along in a structured format. Just like <code>call_with_trace()</code> , you will need to pass in a <code>span_type</code> argument to the extractor and a <code>parent</code> to the extraction.</p> <pre><code>import os\nfrom typing import Type\n\nfrom mirascope.openai import OpenAIExtractor\nfrom mirascope.wandb import WandbExtractorMixin\nimport wandb\n\nwandb.login(key=\"YOUR_WANDB_API_KEY\")\nwandb.init(project=\"wandb_logged_chain\")\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nclass OceanCounter(OpenAIExtractor[int], WandbExtractorMixin[int]):\n    extract_schema: Type[int] = int\n    prompt_template = \"There are 7 oceans on earth.\"\n\n\nnum_oceans, span = OceanCounter(span_type=\"tool\").extract_with_trace()\n\nspan.log(name=\"mirascope_trace\")\n</code></pre>"}]}